[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Parcial Práctico de Series de Tiempo",
    "section": "",
    "text": "Introducción\nEn este parcial práctico se aborda el análisis de series de tiempo utilizando datos relacionados con las acciones del Bitcoin. El objetivo principal es aplicar diversos enfoques de modelado, tanto de Deep Learning como estadísticos, para pronosticar y comprender la dinámica de estas series temporales.\n\n\nModelos Implementados\nEn la parte de Deep Learning, se emplearán redes neuronales recurrentes (RNN), redes neuronales de memoria a largo plazo (LSTM) y perceptrones multicapa (MLP). Por otro lado, en la sección de modelado estadístico, se implementarán modelos de suavización exponencial, Holt-Winters y ARIMA.\nAdemás, se realizará un análisis exploratorio de los datos (EDA) para identificar patrones, tendencias y comportamientos relevantes en las acciones de Bitcoin, proporcionando una base sólida para la posterior implementación de los modelos.",
    "crumbs": [
      "Introducción"
    ]
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Análisis exploratorio de los datos",
    "section": "",
    "text": "En esta sección, se importan las librerías necesarias para el análisis exploratorio de los datos de las acciones de Bitcoin. Se utilizarán herramientas como Pandas y Numpy para el manejo de los datos, Plotly para la visualización, y algunos módulos de statsmodels para el análisis de series de tiempo y pruebas estadísticas.\n\nimport pandas as pd\nimport numpy as np\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\nfrom sklearn.impute import KNNImputer\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.tsa.stattools import acf, adfuller\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")",
    "crumbs": [
      "Análisis exploratorio de datos"
    ]
  },
  {
    "objectID": "eda.html#prueba-de-estacionariedad-de-ljung-box",
    "href": "eda.html#prueba-de-estacionariedad-de-ljung-box",
    "title": "Análisis exploratorio de los datos",
    "section": "Prueba de estacionariedad de Ljung-Box",
    "text": "Prueba de estacionariedad de Ljung-Box\n\nresult = acorr_ljungbox(btc_c['Price'], lags=[200, 500, 1000, 2000], return_df=True)\n\nif (result['lb_pvalue'] &lt; 0.05).any():\n    print(\"La serie no es estacionaria (rechazamos la hipótesis nula para algunos lags).\")\nelse:\n    print(\"La serie es estacionaria (no se rechaza la hipótesis nula para los lags evaluados).\")\n\nresult\n\nLa serie no es estacionaria (rechazamos la hipótesis nula para algunos lags).\n\n\n\n\n\n\n\n\n\nlb_stat\nlb_pvalue\n\n\n\n\n200\n6.937886e+05\n0.0\n\n\n500\n1.166338e+06\n0.0\n\n\n1000\n1.598385e+06\n0.0\n\n\n2000\n1.680488e+06\n0.0",
    "crumbs": [
      "Análisis exploratorio de datos"
    ]
  },
  {
    "objectID": "eda.html#prueba-de-estacionariedad-de-dickey-fuller",
    "href": "eda.html#prueba-de-estacionariedad-de-dickey-fuller",
    "title": "Análisis exploratorio de los datos",
    "section": "Prueba de estacionariedad de Dickey Fuller",
    "text": "Prueba de estacionariedad de Dickey Fuller\n\nresult = adfuller(btc_c['Price'])\n\nprint(f'Estadístico de prueba: {result[0]}')\nprint(f'Valor p: {result[1]}\\n')\n\nif result[1] &lt; 0.05:\n    print(\"La serie es estacionaria (rechazamos la hipótesis nula).\")\nelse:\n    print(\"La serie no es estacionaria (no se rechaza la hipótesis nula).\")\n\nEstadístico de prueba: -0.1643724104901296\nValor p: 0.9426372256091573\n\nLa serie no es estacionaria (no se rechaza la hipótesis nula).",
    "crumbs": [
      "Análisis exploratorio de datos"
    ]
  },
  {
    "objectID": "eda.html#transformaciones-para-obtener-una-serie-estacionaria",
    "href": "eda.html#transformaciones-para-obtener-una-serie-estacionaria",
    "title": "Análisis exploratorio de los datos",
    "section": "Transformaciones para obtener una serie estacionaria",
    "text": "Transformaciones para obtener una serie estacionaria\nPara obtener una serie estacionaria y facilitar el análisis y modelado de los precios de Bitcoin, se procederá a realizar una transformación basada en la diferenciación. La serie original ha mostrado indicios de no ser estacionaria, con un decaimiento lento en la autocorrelación, lo cual sugiere la presencia de tendencia. Por esta razón, se aplicará la diferenciación, que consiste en restar a cada valor su valor previo, con el objetivo de eliminar tendencias y estabilizar la media de la serie. Posteriormente, se evaluará la estacionariedad de la serie diferenciada mediante la prueba de Dickey-Fuller Aumentada (ADF), observando si el valor p es menor a 0.05, lo que indicaría que la serie ya es estacionaria.\n\nbtc_d = btc_c.copy()\nbtc_d['Price'] = btc_c['Price'].diff()\n\nbtc_d\n\n\n\n\n\n\n\n\nPrice\nOpen\nHigh\nLow\nVol.\nChange %\nWeek\nMonth\nYear\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n2010-07-18\nNaN\n0.0\n0.1\n0.1\n80.0\n0.00\n28\n7\n2010\n\n\n2010-07-19\n0.0\n0.1\n0.1\n0.1\n570.0\n0.00\n29\n7\n2010\n\n\n2010-07-20\n0.0\n0.1\n0.1\n0.1\n260.0\n0.00\n29\n7\n2010\n\n\n2010-07-21\n0.0\n0.1\n0.1\n0.1\n580.0\n0.00\n29\n7\n2010\n\n\n2010-07-22\n0.0\n0.1\n0.1\n0.1\n2160.0\n0.00\n29\n7\n2010\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2024-03-20\n5804.0\n62046.8\n68029.5\n60850.9\n133530.0\n9.35\n12\n3\n2024\n\n\n2024-03-21\n-2350.2\n67860.0\n68161.7\n64616.1\n75260.0\n-3.46\n12\n3\n2024\n\n\n2024-03-22\n-1718.3\n65501.5\n66633.3\n62328.3\n72430.0\n-2.62\n12\n3\n2024\n\n\n2024-03-23\n252.3\n63785.6\n65972.4\n63074.9\n35110.0\n0.40\n12\n3\n2024\n\n\n2024-03-24\n3174.1\n64036.5\n67587.8\n63812.9\n65590.0\n4.96\n12\n3\n2024\n\n\n\n\n4999 rows × 9 columns\n\n\n\n\nresult_diff = adfuller(btc_d['Price'].dropna())\nprint(f'Estadístico de prueba: {result_diff[0]}')\nprint(f'Valor p: {result_diff[1]}\\n')\n\nif result_diff[1] &lt; 0.05:\n    print(\"La serie diferenciada es estacionaria (rechazamos la hipótesis nula).\")\nelse:\n    print(\"La serie diferenciada no es estacionaria (no se rechaza la hipótesis nula).\")\n\nEstadístico de prueba: -9.886578088339688\nValor p: 3.6505273415768985e-17\n\nLa serie diferenciada es estacionaria (rechazamos la hipótesis nula).",
    "crumbs": [
      "Análisis exploratorio de datos"
    ]
  },
  {
    "objectID": "eda.html#prueba-de-estacionariedad-de-ljung-box-1",
    "href": "eda.html#prueba-de-estacionariedad-de-ljung-box-1",
    "title": "Análisis exploratorio de los datos",
    "section": "Prueba de estacionariedad de Ljung-Box",
    "text": "Prueba de estacionariedad de Ljung-Box\n\nresult = acorr_ljungbox(btc_ma['Price_adj'], lags=[24, 100, 200], return_df=True)\n\nif (result['lb_pvalue'] &lt; 0.05).any():\n    print(\"La serie no es estacionaria (rechazamos la hipótesis nula para algunos lags).\")\nelse:\n    print(\"La serie es estacionaria (no se rechaza la hipótesis nula para los lags evaluados).\")\n\nresult\n\nLa serie no es estacionaria (rechazamos la hipótesis nula para algunos lags).\n\n\n\n\n\n\n\n\n\nlb_stat\nlb_pvalue\n\n\n\n\n24\n409.071865\n1.028881e-71\n\n\n100\n836.880752\n1.008158e-116\n\n\n200\n1228.559253\n2.377668e-147",
    "crumbs": [
      "Análisis exploratorio de datos"
    ]
  },
  {
    "objectID": "eda.html#prueba-de-estacionariedad-de-dickey-fuller-1",
    "href": "eda.html#prueba-de-estacionariedad-de-dickey-fuller-1",
    "title": "Análisis exploratorio de los datos",
    "section": "Prueba de estacionariedad de Dickey Fuller",
    "text": "Prueba de estacionariedad de Dickey Fuller\n\nresult = adfuller(btc_ma['Price_adj'])\n\nprint(f'Estadístico de prueba: {result[0]}')\nprint(f'Valor p: {result[1]}\\n')\n\nif result[1] &lt; 0.05:\n    print(\"La serie es estacionaria (rechazamos la hipótesis nula).\")\nelse:\n    print(\"La serie no es estacionaria (no se rechaza la hipótesis nula).\")\n\nEstadístico de prueba: -21.156588565371955\nValor p: 0.0\n\nLa serie es estacionaria (rechazamos la hipótesis nula).\n\n\n\nlag_acf = acf(btc_ma['Price_adj'], nlags=100)\n\nfig = go.Figure()\n\nfig.add_trace(go.Bar(\n    x=list(range(len(lag_acf))),\n    y=lag_acf,\n    marker_color='blue'\n))\n\nfig.update_layout(\n    title='Autocorrelación',\n    xaxis_title='Lags',\n    yaxis_title='Autocorrelación',\n    margin={'b': 0, 'r': 30, 'l': 30, 't': 80},\n    plot_bgcolor='rgba(0, 0, 0, 0.0)',\n    paper_bgcolor='rgba(0, 0, 0, 0.0)',\n    font_color=\"white\",\n    hoverlabel=dict(\n        bgcolor=\"#222\"\n    ),\n    xaxis=dict(gridcolor='#222', tickfont=dict(color='white')),\n    yaxis=dict(gridcolor='#222', tickfont=dict(color='white'))\n)\n\nfig.show()",
    "crumbs": [
      "Análisis exploratorio de datos"
    ]
  },
  {
    "objectID": "deep.html",
    "href": "deep.html",
    "title": "Modelos de deep learning",
    "section": "",
    "text": "Para modelar la serie de tiempo con técnicas de Deep Learning, se requiere realizar una preparación cuidadosa de los datos y la configuración de modelos neuronales. En este caso, se utiliza la librería TensorFlow para implementar diferentes tipos de modelos, como Redes Neuronales Multicapa (MLP) y Redes Neuronales Recurrentes (LSTM, RNN), dado que estas arquitecturas son efectivas para capturar patrones no lineales y dependencias temporales en los datos.",
    "crumbs": [
      "Modelos de deep learning"
    ]
  },
  {
    "objectID": "deep.html#series-de-tiempo-del-precio",
    "href": "deep.html#series-de-tiempo-del-precio",
    "title": "Modelos de deep learning",
    "section": "Series de tiempo del precio",
    "text": "Series de tiempo del precio\n\nprice_7 = split_dataset(btc, 7, 'Price')\nprice_14 = split_dataset(btc, 14, 'Price')\nprice_21 = split_dataset(btc, 21, 'Price')\nprice_28 = split_dataset(btc, 28, 'Price')\n\nprice = [price_7, price_14, price_21, price_28]\n\nprint('τ = 7')\nprint(f'Tamaño del conjunto de entrenamiento τ = 7 en X: {price_7[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 7 en y: {price_7[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 7 en X: {price_7[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 7 en y: {price_7[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 7 en X: {price_7[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 7 en y: {price_7[5].shape}')\n\nprint('\\nτ = 14')\nprint(f'Tamaño del conjunto de entrenamiento τ = 14 en X: {price_14[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 14 en y: {price_14[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 14 en X: {price_14[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 14 en y: {price_14[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 14 en X: {price_14[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 14 en y: {price_14[5].shape}')\n\nprint('\\nτ = 21')\nprint(f'Tamaño del conjunto de entrenamiento τ = 21 en X: {price_21[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 21 en y: {price_21[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 21 en X: {price_21[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 21 en y: {price_21[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 21 en X: {price_21[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 21 en y: {price_21[5].shape}')\n\nprint('\\nτ = 28')\nprint(f'Tamaño del conjunto de entrenamiento τ = 28 en X: {price_28[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 28 en y: {price_28[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 28 en X: {price_28[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 28 en y: {price_28[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 28 en X: {price_28[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 28 en y: {price_28[5].shape}')\n\nτ = 7\nTamaño del conjunto de entrenamiento τ = 7 en X: (4971, 7)\nTamaño del conjunto de entrenamiento τ = 7 en y: (4971, 1)\nTamaño del conjunto de validación τ = 7 en X: (7, 7)\nTamaño del conjunto de validación τ = 7 en y: (7,)\nTamaño del conjunto de prueba τ = 7 en X: (7, 7)\nTamaño del conjunto de prueba τ = 7 en y: (7,)\n\nτ = 14\nTamaño del conjunto de entrenamiento τ = 14 en X: (4943, 14)\nTamaño del conjunto de entrenamiento τ = 14 en y: (4943, 1)\nTamaño del conjunto de validación τ = 14 en X: (14, 14)\nTamaño del conjunto de validación τ = 14 en y: (14,)\nTamaño del conjunto de prueba τ = 14 en X: (14, 14)\nTamaño del conjunto de prueba τ = 14 en y: (14,)\n\nτ = 21\nTamaño del conjunto de entrenamiento τ = 21 en X: (4915, 21)\nTamaño del conjunto de entrenamiento τ = 21 en y: (4915, 1)\nTamaño del conjunto de validación τ = 21 en X: (21, 21)\nTamaño del conjunto de validación τ = 21 en y: (21,)\nTamaño del conjunto de prueba τ = 21 en X: (21, 21)\nTamaño del conjunto de prueba τ = 21 en y: (21,)\n\nτ = 28\nTamaño del conjunto de entrenamiento τ = 28 en X: (4887, 28)\nTamaño del conjunto de entrenamiento τ = 28 en y: (4887, 1)\nTamaño del conjunto de validación τ = 28 en X: (28, 28)\nTamaño del conjunto de validación τ = 28 en y: (28,)\nTamaño del conjunto de prueba τ = 28 en X: (28, 28)\nTamaño del conjunto de prueba τ = 28 en y: (28,)",
    "crumbs": [
      "Modelos de deep learning"
    ]
  },
  {
    "objectID": "deep.html#series-de-tiempo-del-retorno-acumulado",
    "href": "deep.html#series-de-tiempo-del-retorno-acumulado",
    "title": "Modelos de deep learning",
    "section": "Series de tiempo del retorno acumulado",
    "text": "Series de tiempo del retorno acumulado\n\nat_7 = split_dataset(btc, 7, 'A_t')\nat_14 = split_dataset(btc, 14, 'A_t')\nat_21 = split_dataset(btc, 21, 'A_t')\nat_28 = split_dataset(btc, 28, 'A_t')\n\nat = [at_7, at_14, at_21, at_28]\n\nprint('τ = 7')\nprint(f'Tamaño del conjunto de entrenamiento τ = 7 en X: {at_7[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 7 en y: {at_7[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 7 en X: {at_7[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 7 en y: {at_7[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 7 en X: {at_7[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 7 en y: {at_7[5].shape}')\n\nprint('\\nτ = 14')\nprint(f'Tamaño del conjunto de entrenamiento τ = 14 en X: {at_14[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 14 en y: {at_14[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 14 en X: {at_14[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 14 en y: {at_14[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 14 en X: {at_14[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 14 en y: {at_14[5].shape}')\n\nprint('\\nτ = 21')\nprint(f'Tamaño del conjunto de entrenamiento τ = 21 en X: {at_21[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 21 en y: {at_21[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 21 en X: {at_21[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 21 en y: {at_21[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 21 en X: {at_21[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 21 en y: {at_21[5].shape}')\n\nprint('\\nτ = 28')\nprint(f'Tamaño del conjunto de entrenamiento τ = 28 en X: {at_28[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 28 en y: {at_28[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 28 en X: {at_28[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 28 en y: {at_28[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 28 en X: {at_28[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 28 en y: {at_28[5].shape}')\n\nτ = 7\nTamaño del conjunto de entrenamiento τ = 7 en X: (4971, 7)\nTamaño del conjunto de entrenamiento τ = 7 en y: (4971, 1)\nTamaño del conjunto de validación τ = 7 en X: (7, 7)\nTamaño del conjunto de validación τ = 7 en y: (7,)\nTamaño del conjunto de prueba τ = 7 en X: (7, 7)\nTamaño del conjunto de prueba τ = 7 en y: (7,)\n\nτ = 14\nTamaño del conjunto de entrenamiento τ = 14 en X: (4943, 14)\nTamaño del conjunto de entrenamiento τ = 14 en y: (4943, 1)\nTamaño del conjunto de validación τ = 14 en X: (14, 14)\nTamaño del conjunto de validación τ = 14 en y: (14,)\nTamaño del conjunto de prueba τ = 14 en X: (14, 14)\nTamaño del conjunto de prueba τ = 14 en y: (14,)\n\nτ = 21\nTamaño del conjunto de entrenamiento τ = 21 en X: (4915, 21)\nTamaño del conjunto de entrenamiento τ = 21 en y: (4915, 1)\nTamaño del conjunto de validación τ = 21 en X: (21, 21)\nTamaño del conjunto de validación τ = 21 en y: (21,)\nTamaño del conjunto de prueba τ = 21 en X: (21, 21)\nTamaño del conjunto de prueba τ = 21 en y: (21,)\n\nτ = 28\nTamaño del conjunto de entrenamiento τ = 28 en X: (4887, 28)\nTamaño del conjunto de entrenamiento τ = 28 en y: (4887, 1)\nTamaño del conjunto de validación τ = 28 en X: (28, 28)\nTamaño del conjunto de validación τ = 28 en y: (28,)\nTamaño del conjunto de prueba τ = 28 en X: (28, 28)\nTamaño del conjunto de prueba τ = 28 en y: (28,)",
    "crumbs": [
      "Modelos de deep learning"
    ]
  },
  {
    "objectID": "deep.html#series-de-tiempo-de-la-volatilidad",
    "href": "deep.html#series-de-tiempo-de-la-volatilidad",
    "title": "Modelos de deep learning",
    "section": "Series de tiempo de la volatilidad",
    "text": "Series de tiempo de la volatilidad\n\nSerie de tiempo de la volatidad para σ = 7\n\nvol_7 = split_dataset(btc, 7, 'σ_7')\nvol_14 = split_dataset(btc, 14, 'σ_7')\nvol_21 = split_dataset(btc, 21, 'σ_7')\nvol_28 = split_dataset(btc, 28, 'σ_7')\n\nvo_7 = [vol_7, vol_14, vol_21, vol_28]\n\nprint('τ = 7')\nprint(f'Tamaño del conjunto de entrenamiento τ = 7 en X: {vol_7[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 7 en y: {vol_7[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 7 en X: {vol_7[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 7 en y: {vol_7[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 7 en X: {vol_7[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 7 en y: {vol_7[5].shape}')\n\nprint('\\nτ = 14')\nprint(f'Tamaño del conjunto de entrenamiento τ = 14 en X: {vol_14[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 14 en y: {vol_14[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 14 en X: {vol_14[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 14 en y: {vol_14[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 14 en X: {vol_14[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 14 en y: {vol_14[5].shape}')\n\nprint('\\nτ = 21')\nprint(f'Tamaño del conjunto de entrenamiento τ = 21 en X: {vol_21[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 21 en y: {vol_21[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 21 en X: {vol_21[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 21 en y: {vol_21[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 21 en X: {vol_21[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 21 en y: {vol_21[5].shape}')\n\nprint('\\nτ = 28')\nprint(f'Tamaño del conjunto de entrenamiento τ = 28 en X: {vol_28[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 28 en y: {vol_28[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 28 en X: {vol_28[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 28 en y: {vol_28[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 28 en X: {vol_28[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 28 en y: {vol_28[5].shape}')\n\nτ = 7\nTamaño del conjunto de entrenamiento τ = 7 en X: (4971, 7)\nTamaño del conjunto de entrenamiento τ = 7 en y: (4971, 1)\nTamaño del conjunto de validación τ = 7 en X: (7, 7)\nTamaño del conjunto de validación τ = 7 en y: (7,)\nTamaño del conjunto de prueba τ = 7 en X: (7, 7)\nTamaño del conjunto de prueba τ = 7 en y: (7,)\n\nτ = 14\nTamaño del conjunto de entrenamiento τ = 14 en X: (4943, 14)\nTamaño del conjunto de entrenamiento τ = 14 en y: (4943, 1)\nTamaño del conjunto de validación τ = 14 en X: (14, 14)\nTamaño del conjunto de validación τ = 14 en y: (14,)\nTamaño del conjunto de prueba τ = 14 en X: (14, 14)\nTamaño del conjunto de prueba τ = 14 en y: (14,)\n\nτ = 21\nTamaño del conjunto de entrenamiento τ = 21 en X: (4915, 21)\nTamaño del conjunto de entrenamiento τ = 21 en y: (4915, 1)\nTamaño del conjunto de validación τ = 21 en X: (21, 21)\nTamaño del conjunto de validación τ = 21 en y: (21,)\nTamaño del conjunto de prueba τ = 21 en X: (21, 21)\nTamaño del conjunto de prueba τ = 21 en y: (21,)\n\nτ = 28\nTamaño del conjunto de entrenamiento τ = 28 en X: (4887, 28)\nTamaño del conjunto de entrenamiento τ = 28 en y: (4887, 1)\nTamaño del conjunto de validación τ = 28 en X: (28, 28)\nTamaño del conjunto de validación τ = 28 en y: (28,)\nTamaño del conjunto de prueba τ = 28 en X: (28, 28)\nTamaño del conjunto de prueba τ = 28 en y: (28,)\n\n\n\n\nSerie de tiempo de la volatidad para σ = 14\n\nvol_7 = split_dataset(btc, 7, 'σ_14')\nvol_14 = split_dataset(btc, 14, 'σ_14')\nvol_21 = split_dataset(btc, 21, 'σ_14')\nvol_28 = split_dataset(btc, 28, 'σ_14')\n\nvo_14 = [vol_7, vol_14, vol_21, vol_28]\n\nprint('τ = 7')\nprint(f'Tamaño del conjunto de entrenamiento τ = 7 en X: {vol_7[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 7 en y: {vol_7[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 7 en X: {vol_7[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 7 en y: {vol_7[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 7 en X: {vol_7[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 7 en y: {vol_7[5].shape}')\n\nprint('\\nτ = 14')\nprint(f'Tamaño del conjunto de entrenamiento τ = 14 en X: {vol_14[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 14 en y: {vol_14[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 14 en X: {vol_14[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 14 en y: {vol_14[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 14 en X: {vol_14[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 14 en y: {vol_14[5].shape}')\n\nprint('\\nτ = 21')\nprint(f'Tamaño del conjunto de entrenamiento τ = 21 en X: {vol_21[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 21 en y: {vol_21[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 21 en X: {vol_21[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 21 en y: {vol_21[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 21 en X: {vol_21[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 21 en y: {vol_21[5].shape}')\n\nprint('\\nτ = 28')\nprint(f'Tamaño del conjunto de entrenamiento τ = 28 en X: {vol_28[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 28 en y: {vol_28[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 28 en X: {vol_28[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 28 en y: {vol_28[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 28 en X: {vol_28[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 28 en y: {vol_28[5].shape}')\n\nτ = 7\nTamaño del conjunto de entrenamiento τ = 7 en X: (4971, 7)\nTamaño del conjunto de entrenamiento τ = 7 en y: (4971, 1)\nTamaño del conjunto de validación τ = 7 en X: (7, 7)\nTamaño del conjunto de validación τ = 7 en y: (7,)\nTamaño del conjunto de prueba τ = 7 en X: (7, 7)\nTamaño del conjunto de prueba τ = 7 en y: (7,)\n\nτ = 14\nTamaño del conjunto de entrenamiento τ = 14 en X: (4943, 14)\nTamaño del conjunto de entrenamiento τ = 14 en y: (4943, 1)\nTamaño del conjunto de validación τ = 14 en X: (14, 14)\nTamaño del conjunto de validación τ = 14 en y: (14,)\nTamaño del conjunto de prueba τ = 14 en X: (14, 14)\nTamaño del conjunto de prueba τ = 14 en y: (14,)\n\nτ = 21\nTamaño del conjunto de entrenamiento τ = 21 en X: (4915, 21)\nTamaño del conjunto de entrenamiento τ = 21 en y: (4915, 1)\nTamaño del conjunto de validación τ = 21 en X: (21, 21)\nTamaño del conjunto de validación τ = 21 en y: (21,)\nTamaño del conjunto de prueba τ = 21 en X: (21, 21)\nTamaño del conjunto de prueba τ = 21 en y: (21,)\n\nτ = 28\nTamaño del conjunto de entrenamiento τ = 28 en X: (4887, 28)\nTamaño del conjunto de entrenamiento τ = 28 en y: (4887, 1)\nTamaño del conjunto de validación τ = 28 en X: (28, 28)\nTamaño del conjunto de validación τ = 28 en y: (28,)\nTamaño del conjunto de prueba τ = 28 en X: (28, 28)\nTamaño del conjunto de prueba τ = 28 en y: (28,)\n\n\n\n\nSerie de tiempo de la volatidad para σ = 21\n\nvol_7 = split_dataset(btc, 7, 'σ_21')\nvol_14 = split_dataset(btc, 14, 'σ_21')\nvol_21 = split_dataset(btc, 21, 'σ_21')\nvol_28 = split_dataset(btc, 28, 'σ_21')\n\nvo_21 = [vol_7, vol_14, vol_21, vol_28]\n\nprint('τ = 7')\nprint(f'Tamaño del conjunto de entrenamiento τ = 7 en X: {vol_7[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 7 en y: {vol_7[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 7 en X: {vol_7[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 7 en y: {vol_7[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 7 en X: {vol_7[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 7 en y: {vol_7[5].shape}')\n\nprint('\\nτ = 14')\nprint(f'Tamaño del conjunto de entrenamiento τ = 14 en X: {vol_14[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 14 en y: {vol_14[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 14 en X: {vol_14[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 14 en y: {vol_14[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 14 en X: {vol_14[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 14 en y: {vol_14[5].shape}')\n\nprint('\\nτ = 21')\nprint(f'Tamaño del conjunto de entrenamiento τ = 21 en X: {vol_21[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 21 en y: {vol_21[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 21 en X: {vol_21[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 21 en y: {vol_21[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 21 en X: {vol_21[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 21 en y: {vol_21[5].shape}')\n\nprint('\\nτ = 28')\nprint(f'Tamaño del conjunto de entrenamiento τ = 28 en X: {vol_28[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 28 en y: {vol_28[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 28 en X: {vol_28[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 28 en y: {vol_28[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 28 en X: {vol_28[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 28 en y: {vol_28[5].shape}')\n\nτ = 7\nTamaño del conjunto de entrenamiento τ = 7 en X: (4971, 7)\nTamaño del conjunto de entrenamiento τ = 7 en y: (4971, 1)\nTamaño del conjunto de validación τ = 7 en X: (7, 7)\nTamaño del conjunto de validación τ = 7 en y: (7,)\nTamaño del conjunto de prueba τ = 7 en X: (7, 7)\nTamaño del conjunto de prueba τ = 7 en y: (7,)\n\nτ = 14\nTamaño del conjunto de entrenamiento τ = 14 en X: (4943, 14)\nTamaño del conjunto de entrenamiento τ = 14 en y: (4943, 1)\nTamaño del conjunto de validación τ = 14 en X: (14, 14)\nTamaño del conjunto de validación τ = 14 en y: (14,)\nTamaño del conjunto de prueba τ = 14 en X: (14, 14)\nTamaño del conjunto de prueba τ = 14 en y: (14,)\n\nτ = 21\nTamaño del conjunto de entrenamiento τ = 21 en X: (4915, 21)\nTamaño del conjunto de entrenamiento τ = 21 en y: (4915, 1)\nTamaño del conjunto de validación τ = 21 en X: (21, 21)\nTamaño del conjunto de validación τ = 21 en y: (21,)\nTamaño del conjunto de prueba τ = 21 en X: (21, 21)\nTamaño del conjunto de prueba τ = 21 en y: (21,)\n\nτ = 28\nTamaño del conjunto de entrenamiento τ = 28 en X: (4887, 28)\nTamaño del conjunto de entrenamiento τ = 28 en y: (4887, 1)\nTamaño del conjunto de validación τ = 28 en X: (28, 28)\nTamaño del conjunto de validación τ = 28 en y: (28,)\nTamaño del conjunto de prueba τ = 28 en X: (28, 28)\nTamaño del conjunto de prueba τ = 28 en y: (28,)\n\n\n\n\nSerie de tiempo de la volatidad para σ = 28\n\nvol_7 = split_dataset(btc, 7, 'σ_7')\nvol_14 = split_dataset(btc, 14, 'σ_7')\nvol_21 = split_dataset(btc, 21, 'σ_7')\nvol_28 = split_dataset(btc, 28, 'σ_7')\n\nvo_28 = [vol_7, vol_14, vol_21, vol_28]\n\nprint('τ = 7')\nprint(f'Tamaño del conjunto de entrenamiento τ = 7 en X: {vol_7[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 7 en y: {vol_7[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 7 en X: {vol_7[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 7 en y: {vol_7[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 7 en X: {vol_7[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 7 en y: {vol_7[5].shape}')\n\nprint('\\nτ = 14')\nprint(f'Tamaño del conjunto de entrenamiento τ = 14 en X: {vol_14[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 14 en y: {vol_14[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 14 en X: {vol_14[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 14 en y: {vol_14[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 14 en X: {vol_14[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 14 en y: {vol_14[5].shape}')\n\nprint('\\nτ = 21')\nprint(f'Tamaño del conjunto de entrenamiento τ = 21 en X: {vol_21[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 21 en y: {vol_21[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 21 en X: {vol_21[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 21 en y: {vol_21[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 21 en X: {vol_21[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 21 en y: {vol_21[5].shape}')\n\nprint('\\nτ = 28')\nprint(f'Tamaño del conjunto de entrenamiento τ = 28 en X: {vol_28[0].shape}')\nprint(f'Tamaño del conjunto de entrenamiento τ = 28 en y: {vol_28[1].shape}')\nprint(f'Tamaño del conjunto de validación τ = 28 en X: {vol_28[2].shape}')\nprint(f'Tamaño del conjunto de validación τ = 28 en y: {vol_28[3].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 28 en X: {vol_28[4].shape}')\nprint(f'Tamaño del conjunto de prueba τ = 28 en y: {vol_28[5].shape}')\n\nτ = 7\nTamaño del conjunto de entrenamiento τ = 7 en X: (4971, 7)\nTamaño del conjunto de entrenamiento τ = 7 en y: (4971, 1)\nTamaño del conjunto de validación τ = 7 en X: (7, 7)\nTamaño del conjunto de validación τ = 7 en y: (7,)\nTamaño del conjunto de prueba τ = 7 en X: (7, 7)\nTamaño del conjunto de prueba τ = 7 en y: (7,)\n\nτ = 14\nTamaño del conjunto de entrenamiento τ = 14 en X: (4943, 14)\nTamaño del conjunto de entrenamiento τ = 14 en y: (4943, 1)\nTamaño del conjunto de validación τ = 14 en X: (14, 14)\nTamaño del conjunto de validación τ = 14 en y: (14,)\nTamaño del conjunto de prueba τ = 14 en X: (14, 14)\nTamaño del conjunto de prueba τ = 14 en y: (14,)\n\nτ = 21\nTamaño del conjunto de entrenamiento τ = 21 en X: (4915, 21)\nTamaño del conjunto de entrenamiento τ = 21 en y: (4915, 1)\nTamaño del conjunto de validación τ = 21 en X: (21, 21)\nTamaño del conjunto de validación τ = 21 en y: (21,)\nTamaño del conjunto de prueba τ = 21 en X: (21, 21)\nTamaño del conjunto de prueba τ = 21 en y: (21,)\n\nτ = 28\nTamaño del conjunto de entrenamiento τ = 28 en X: (4887, 28)\nTamaño del conjunto de entrenamiento τ = 28 en y: (4887, 1)\nTamaño del conjunto de validación τ = 28 en X: (28, 28)\nTamaño del conjunto de validación τ = 28 en y: (28,)\nTamaño del conjunto de prueba τ = 28 en X: (28, 28)\nTamaño del conjunto de prueba τ = 28 en y: (28,)",
    "crumbs": [
      "Modelos de deep learning"
    ]
  },
  {
    "objectID": "deep.html#pliegues-del-precio",
    "href": "deep.html#pliegues-del-precio",
    "title": "Modelos de deep learning",
    "section": "Pliegues del precio",
    "text": "Pliegues del precio\n\ntau_v = [7, 14, 21, 28]\n\nfig_p = plot_folds(price, tau_v, 'Price')\nfig_p.show()\n\n\n\nEn esta visualización, se dividen los datos de precios en cuatro pliegues distintos, con cada pliegue utilizando una combinación específica de períodos de entrenamiento, validación y prueba. En cada pliegue, la sección morada representa los datos de entrenamiento, la sección azul claro corresponde a los datos de validación y la sección amarilla indica los datos de prueba. La estructura de estos pliegues asegura que todas las particiones del tiempo sean cubiertas, lo cual es esencial para evitar el sesgo temporal y evaluar correctamente la capacidad de generalización del modelo.",
    "crumbs": [
      "Modelos de deep learning"
    ]
  },
  {
    "objectID": "deep.html#pliegues-del-retorno-acumulado",
    "href": "deep.html#pliegues-del-retorno-acumulado",
    "title": "Modelos de deep learning",
    "section": "Pliegues del retorno acumulado",
    "text": "Pliegues del retorno acumulado\n\nfig_p = plot_folds(price, tau_v, 'A_j')\nfig_p.show()\n\n\n\nLos pliegues del retorno acumulado muestran una estructura similar a la de los precios, pero se enfocan en la serie temporal de retornos. Aquí, cada pliegue también cubre los períodos de entrenamiento, validación y prueba, lo cual es clave para evaluar cómo los modelos capturan la variabilidad y las tendencias de los retornos acumulados.",
    "crumbs": [
      "Modelos de deep learning"
    ]
  },
  {
    "objectID": "deep.html#pliegues-de-la-volatilidad",
    "href": "deep.html#pliegues-de-la-volatilidad",
    "title": "Modelos de deep learning",
    "section": "Pliegues de la volatilidad",
    "text": "Pliegues de la volatilidad\n\nσ = 7\n\nfig_p = plot_folds(price, tau_v, 'σ_7')\nfig_p.show()\n\n\n\n\n\nσ = 14\n\nfig_p = plot_folds(price, tau_v, 'σ_14')\nfig_p.show()\n\n\n\n\n\nσ = 21\n\nfig_p = plot_folds(price, tau_v, 'σ_21')\nfig_p.show()\n\n\n\n\n\nσ = 28\n\nfig_p = plot_folds(price, tau_v, 'σ_28')\nfig_p.show()\n\n\n\nEn las visualizaciones de pliegues para σ_7, σ_14, σ_21 y σ_28, se presentan las divisiones para distintas medidas de volatilidad con diferentes valores de τ. Cada pliegue sigue la misma estructura de entrenamiento, validación y prueba, lo cual permite examinar cómo los modelos responden a los cambios en la volatilidad a lo largo de distintos horizontes temporales.",
    "crumbs": [
      "Modelos de deep learning"
    ]
  },
  {
    "objectID": "deep.html#multilayer-perceptron-models-mlp",
    "href": "deep.html#multilayer-perceptron-models-mlp",
    "title": "Modelos de deep learning",
    "section": "Multilayer Perceptron Models (MLP)",
    "text": "Multilayer Perceptron Models (MLP)\n\nPredicciones para el precio\n\nτ = 7\n\ndef build_mlp(dim, learning_rate=0.001, activation='relu'):\n    model = Sequential()\n    model.add(Input(shape=(dim,)))\n    model.add(Dense(64, activation=activation))\n    model.add(Dense(32, activation=activation))\n    model.add(Dense(1, activation='linear'))\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=losses.MeanSquaredError())\n    return model\n\n\ndef get_grid(data, dim, var, tau, w = 1):\n    if var == 'volatilidad':\n        folder_path = f'best_grids/{var}/{var}_{w}_{tau}.pkl'\n    else:\n        folder_path = f'best_grids/{var}/{var}_{tau}.pkl'\n\n    if os.path.exists(folder_path):\n        print(\"Cargando resultados guardados...\")\n        with open(folder_path, 'rb') as file:\n            results = pickle.load(file)\n        return results\n\n    model = KerasRegressor(build_fn = build_mlp, verbose = 0, dim = dim)\n\n    param = {\n        'model__learning_rate': [0.001, 0.01, 0.1, 0.2],\n        'model__activation': ['relu', 'tanh', 'sigmoid'],\n        'epochs': [50, 100, 150]\n    }\n\n    grid = GridSearchCV(\n        estimator = model,\n        param_grid = param,\n        cv = 3,\n        verbose = 2\n    )\n\n    results = grid.fit(data[0], data[1])\n\n    with open(folder_path, 'wb') as file:\n        pickle.dump(results, file)\n\n    return results\n\n\ndim = price[0][0].shape[1]\n\nresults = get_grid(price[0], dim, 'price', 7)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: relu\nMejor número de epocas: 50\nMejor Longitud de paso μ (Learning Rate): 0.001\nMejor Puntuación: 0.9929642587499478\n\n\n\ndef load_models(type_, folder, w, n = 1):\n    if folder == 'volatilidad':\n        folder_path = f'models/{type_}/{folder}/{n}/{w}'\n    else:\n        folder_path = f'models/{type_}/{folder}/{w}'\n\n    models = []\n    for filename in sorted(os.listdir(folder_path)):\n        if filename.endswith('.keras'):\n            models.append(load_model(os.path.join(folder_path, filename)))\n    return models\n\ndef save_models(models, type_, folder, w, n = 1):\n    if folder == 'volatilidad':\n        folder_path = f'models/{type_}/{folder}/{n}/{w}'\n    else:\n        folder_path = f'models/{type_}/{folder}/{w}'\n\n    if not os.path.exists(folder_path):\n        os.makedirs(folder_path)\n    \n    for i, model in enumerate(models):\n        model.save(os.path.join(folder_path, f'MLP_{w}_{i}.keras'))\n\ndef build_model(type_, var, dim, neurons, dropouts, activation, learning_rate, w, n = 1):\n    models = load_models(type_, var, w, n)\n    if models:\n        print(f\"Modelos cargados desde la carpeta '{var}'.\")\n        return models\n\n    print(f\"No se encontraron modelos en la carpeta '{var}', construyendo nuevos modelos...\")\n\n    models = []\n\n    for neuron in neurons:\n        for dropout in dropouts:\n            model = Sequential()\n            model.add(Input(shape = (dim, )))\n            model.add(Dense(neuron, activation=activation))\n            model.add(Dense(neuron // 2, activation=activation))\n            model.add(Dense(neuron // 2, activation=activation))\n            model.add(Dropout(dropout))\n            model.add(Dense(1, activation='linear'))\n            model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n            \n            models.append(model)\n\n            print(f'Modelo con {(neuron, neuron // 2, neuron // 2, 1)} neuronas y dropout {dropout}:')\n            model.summary()\n\n    save_models(models, type_, var, w, n)\n    return models\n\n\nneurons = [32, 64, 128]\ndropouts = [0.2, 0.4, 0.6, 0.8]\n\nmodels = build_model('MLP', 'price', dim, neurons, dropouts, best_activation, best_mu, 7)\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'price'.\n\n\n\ndef best_model(var, models, X_val, y_val):\n    results = []\n\n    for i, model in enumerate(models):\n        pred = model.predict(X_val)\n\n        pred = pred.ravel()\n        y_val = y_val.ravel()\n\n        mape = mean_absolute_percentage_error(y_val, pred)\n        mae = mean_absolute_error(y_val, pred)\n        mse = mean_squared_error(y_val, pred)\n        rmse = np.sqrt(mse)\n        r2 = r2_score(y_val, pred)\n\n        #ljungbox = acorr_ljungbox(pred - y_val, lags = [10], return_df = True)['lb_pvalue'].iloc[0]\n        jb_stat, jb_p = jarque_bera(pred - y_val)\n\n        print(f'Modelo {i}: MSE = {mse:.4f}')\n\n        results.append({\n            \"Model Name\": f'MLP_{var}_{i}',\n            \"MAPE\": mape,\n            \"MAE\": mae,\n            \"RMSE\": rmse,\n            \"MSE\": mse,\n            \"R2\": r2,\n            #\"Ljung-Box P-Value\": ljungbox,\n            \"Jarque-Bera P-Value\": jb_p\n        })\n\n    results = pd.DataFrame(results)\n    return results\n\n\nmlp_p7_metrics = best_model('price', models, price[0][2], price[0][3])\n\nmlp_p7_metrics\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 61ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 61ms/step\nModelo 0: MSE = 0.0087\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 1: MSE = 0.0053\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 2: MSE = 0.0102\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 3: MSE = 0.0171\nWARNING:tensorflow:5 out of the last 5 calls to &lt;function TensorFlowTrainer.make_predict_function.&lt;locals&gt;.one_step_on_data_distributed at 0x16ab35790&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\nModelo 4: MSE = 0.0130\nWARNING:tensorflow:6 out of the last 6 calls to &lt;function TensorFlowTrainer.make_predict_function.&lt;locals&gt;.one_step_on_data_distributed at 0x16ab94430&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step\nModelo 5: MSE = 0.0211\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 6: MSE = 0.0103\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step\nModelo 7: MSE = 0.0135\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 8: MSE = 0.0118\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 9: MSE = 0.0015\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 10: MSE = 0.0074\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 11: MSE = 0.0246\n\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_price_0\n0.933592\n0.093359\n0.093359\n0.008716\n-4.525579e+31\n0.55747\n\n\n1\nMLP_price_1\n0.725742\n0.072574\n0.072574\n0.005267\n-2.734788e+31\n0.55747\n\n\n2\nMLP_price_2\n1.008198\n0.100820\n0.100820\n0.010165\n-5.277783e+31\n0.55747\n\n\n3\nMLP_price_3\n1.306148\n0.130615\n0.130615\n0.017060\n-8.858173e+31\nNaN\n\n\n4\nMLP_price_4\n1.139195\n0.113920\n0.113920\n0.012978\n-6.738385e+31\n0.55747\n\n\n5\nMLP_price_5\n1.454286\n0.145429\n0.145429\n0.021149\n-1.098144e+32\n0.55747\n\n\n6\nMLP_price_6\n1.012448\n0.101245\n0.101245\n0.010251\n-5.322367e+31\nNaN\n\n\n7\nMLP_price_7\n1.163491\n0.116349\n0.116349\n0.013537\n-7.028867e+31\nNaN\n\n\n8\nMLP_price_8\n1.085518\n0.108552\n0.108552\n0.011783\n-6.118338e+31\nNaN\n\n\n9\nMLP_price_9\n0.390892\n0.039089\n0.039089\n0.001528\n-7.933669e+30\nNaN\n\n\n10\nMLP_price_10\n0.858189\n0.085819\n0.085819\n0.007365\n-3.824063e+31\nNaN\n\n\n11\nMLP_price_11\n1.569040\n0.156904\n0.156904\n0.024619\n-1.278285e+32\n0.55747\n\n\n\n\n\n\n\n\n\nτ = 14\n\ndim = price[1][0].shape[1]\n\nresults = get_grid(price[1], dim, 'price', 14)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: relu\nMejor número de epocas: 100\nMejor Longitud de paso μ (Learning Rate): 0.001\nMejor Puntuación: 0.9923356886891406\n\n\n\nmodels = build_model('MLP', 'price', dim, neurons, dropouts, best_activation, best_mu, 14)\n\nmlp_p14_metrics = best_model('price', models, price[1][2], price[1][3])\nmlp_p14_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'price'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 0: MSE = 0.0119\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 1: MSE = 0.0219\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 2: MSE = 0.0118\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 3: MSE = 0.0146\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 4: MSE = 0.0254\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 5: MSE = 0.0116\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 6: MSE = 0.0107\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 7: MSE = 0.0159\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 8: MSE = 0.0129\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 9: MSE = 0.0043\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 10: MSE = 0.0153\n\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 11: MSE = 0.0122\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_price_0\n1.088821\n0.108882\n0.108882\n0.011855\n-6.155626e+31\n0.245879\n\n\n1\nMLP_price_1\n1.480355\n0.148035\n0.148035\n0.021914\n-1.137866e+32\n0.245879\n\n\n2\nMLP_price_2\n1.085182\n0.108518\n0.108518\n0.011776\n-6.114548e+31\n0.245879\n\n\n3\nMLP_price_3\n1.210023\n0.121002\n0.121002\n0.014642\n-7.602329e+31\n0.245879\n\n\n4\nMLP_price_4\n1.593198\n0.159320\n0.159320\n0.025383\n-1.317951e+32\n0.245879\n\n\n5\nMLP_price_5\n1.077287\n0.107729\n0.107729\n0.011605\n-6.025910e+31\nNaN\n\n\n6\nMLP_price_6\n1.032645\n0.103265\n0.103265\n0.010664\n-5.536839e+31\n0.245879\n\n\n7\nMLP_price_7\n1.262066\n0.126207\n0.126207\n0.015928\n-8.270341e+31\n0.245879\n\n\n8\nMLP_price_8\n1.136970\n0.113697\n0.113697\n0.012927\n-6.712084e+31\nNaN\n\n\n9\nMLP_price_9\n0.654840\n0.065484\n0.065484\n0.004288\n-2.226534e+31\nNaN\n\n\n10\nMLP_price_10\n1.237239\n0.123724\n0.123724\n0.015308\n-7.948164e+31\nNaN\n\n\n11\nMLP_price_11\n1.103586\n0.110359\n0.110359\n0.012179\n-6.323704e+31\n0.245879\n\n\n\n\n\n\n\n\n\nτ = 21\n\ndim = price[2][0].shape[1]\n\nresults = get_grid(price[2], dim, 'price', 21)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: relu\nMejor número de epocas: 100\nMejor Longitud de paso μ (Learning Rate): 0.001\nMejor Puntuación: 0.992061931285642\n\n\n\nmodels = build_model('MLP', 'price', dim, neurons, dropouts, best_activation, best_mu, 21)\n\nmlp_p21_metrics = best_model('price', models, price[2][2], price[2][3])\nmlp_p21_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'price'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 0: MSE = 0.0011\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 1: MSE = 0.0104\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 2: MSE = 0.0074\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 3: MSE = 0.0099\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 4: MSE = 0.0118\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 5: MSE = 0.0441\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 6: MSE = 0.0076\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 7: MSE = 0.0067\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 8: MSE = 0.0081\n\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 9: MSE = 0.0089\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 10: MSE = 0.0041\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 11: MSE = 0.0267\n\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_price_0\n0.324668\n0.032467\n0.032467\n0.001054\n-5.473179e+30\n0.011652\n\n\n1\nMLP_price_1\n1.021305\n0.102130\n0.102130\n0.010431\n-5.415893e+31\n0.011652\n\n\n2\nMLP_price_2\n0.858484\n0.085848\n0.085848\n0.007370\n-3.826697e+31\nNaN\n\n\n3\nMLP_price_3\n0.994687\n0.099469\n0.099469\n0.009894\n-5.137266e+31\n0.011652\n\n\n4\nMLP_price_4\n1.088038\n0.108804\n0.108804\n0.011838\n-6.146784e+31\n0.011652\n\n\n5\nMLP_price_5\n2.099725\n0.209973\n0.209973\n0.044088\n-2.289203e+32\nNaN\n\n\n6\nMLP_price_6\n0.871561\n0.087156\n0.087156\n0.007596\n-3.944169e+31\nNaN\n\n\n7\nMLP_price_7\n0.819830\n0.081983\n0.081983\n0.006721\n-3.489850e+31\nNaN\n\n\n8\nMLP_price_8\n0.902006\n0.090201\n0.090201\n0.008136\n-4.224529e+31\nNaN\n\n\n9\nMLP_price_9\n0.942669\n0.094267\n0.094267\n0.008886\n-4.614005e+31\nNaN\n\n\n10\nMLP_price_10\n0.641203\n0.064120\n0.064120\n0.004111\n-2.134768e+31\nNaN\n\n\n11\nMLP_price_11\n1.633952\n0.163395\n0.163395\n0.026698\n-1.386239e+32\nNaN\n\n\n\n\n\n\n\n\n\nτ = 28\n\ndim = price[3][0].shape[1]\n\nresults = get_grid(price[3], dim, 'price', 28)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: relu\nMejor número de epocas: 50\nMejor Longitud de paso μ (Learning Rate): 0.01\nMejor Puntuación: 0.9915164107311906\n\n\n\nmodels = build_model('MLP', 'price', dim, neurons, dropouts, best_activation, best_mu, 28)\n\nmlp_p28_metrics = best_model('price', models, price[3][2], price[3][3])\nmlp_p28_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'price'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 0: MSE = 0.0003\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 1: MSE = 0.0085\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 2: MSE = 0.0105\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 3: MSE = 0.0172\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 4: MSE = 0.0090\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 5: MSE = 0.0006\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 6: MSE = 0.0088\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 7: MSE = 0.0168\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 8: MSE = 0.0050\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 9: MSE = 0.0087\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 10: MSE = 0.0273\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 11: MSE = 0.0209\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_price_0\n0.171046\n0.017105\n0.018523\n0.000343\n-1.781498e+30\n4.947980e-03\n\n\n1\nMLP_price_1\n0.919785\n0.091978\n0.092095\n0.008481\n-4.403843e+31\n7.044676e-23\n\n\n2\nMLP_price_2\n1.021332\n0.102133\n0.102334\n0.010472\n-5.437525e+31\n1.268079e-02\n\n\n3\nMLP_price_3\n1.305967\n0.130597\n0.131146\n0.017199\n-8.930358e+31\n1.218378e-01\n\n\n4\nMLP_price_4\n0.950463\n0.095046\n0.095080\n0.009040\n-4.693975e+31\n8.858452e-07\n\n\n5\nMLP_price_5\n0.230914\n0.023091\n0.025364\n0.000643\n-3.340493e+30\n8.672279e-02\n\n\n6\nMLP_price_6\n0.933272\n0.093327\n0.093712\n0.008782\n-4.559847e+31\n6.673026e-02\n\n\n7\nMLP_price_7\n1.292144\n0.129214\n0.129778\n0.016842\n-8.745025e+31\n1.005144e-03\n\n\n8\nMLP_price_8\n0.701524\n0.070152\n0.070636\n0.004989\n-2.590696e+31\n2.241195e-01\n\n\n9\nMLP_price_9\n0.932640\n0.093264\n0.093422\n0.008728\n-4.531647e+31\n1.477324e-06\n\n\n10\nMLP_price_10\n1.650198\n0.165020\n0.165223\n0.027298\n-1.417418e+32\n3.693883e-18\n\n\n11\nMLP_price_11\n1.443498\n0.144350\n0.144478\n0.020874\n-1.083842e+32\n9.277897e-17\n\n\n\n\n\n\n\n\n\n\nPredicciones para el retorno acumulado\n\nτ = 7\n\ndim = at[0][0].shape[1]\n\nresults = get_grid(at[0], dim, 'retorno', 7)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: relu\nMejor número de epocas: 150\nMejor Longitud de paso μ (Learning Rate): 0.01\nMejor Puntuación: 0.9953678427084359\n\n\n\nmodels = build_model('MLP', 'retorno', dim, neurons, dropouts, best_activation, best_mu, 7)\n\nmlp_at7_metrics = best_model('at', models, at[0][2], at[0][3])\nmlp_at7_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'retorno'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 0: MSE = 13.5456\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 1: MSE = 6.5067\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 2: MSE = 16.8533\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 3: MSE = 31.8977\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 4: MSE = 32.0769\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 5: MSE = 28.4867\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 6: MSE = 13.2960\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 7: MSE = 22.6537\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 8: MSE = 9.9795\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 9: MSE = 13.0616\n\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 10: MSE = 5.4229\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 11: MSE = 19.5227\n\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_at_0\n0.851365\n3.680431\n3.680431\n13.545571\n0.0\n0.55747\n\n\n1\nMLP_at_1\n0.590060\n2.550815\n2.550815\n6.506657\n0.0\n0.55747\n\n\n2\nMLP_at_2\n0.949641\n4.105279\n4.105279\n16.853318\n0.0\n0.55747\n\n\n3\nMLP_at_3\n1.306462\n5.647805\n5.647805\n31.897697\n0.0\nNaN\n\n\n4\nMLP_at_4\n1.310126\n5.663645\n5.663645\n32.076874\n0.0\nNaN\n\n\n5\nMLP_at_5\n1.234634\n5.337297\n5.337297\n28.486740\n0.0\n0.55747\n\n\n6\nMLP_at_6\n0.843485\n3.646367\n3.646367\n13.295995\n0.0\nNaN\n\n\n7\nMLP_at_7\n1.100998\n4.759590\n4.759590\n22.653696\n0.0\nNaN\n\n\n8\nMLP_at_8\n0.730754\n3.159033\n3.159033\n9.979490\n0.0\nNaN\n\n\n9\nMLP_at_9\n0.836017\n3.614082\n3.614082\n13.061586\n0.0\nNaN\n\n\n10\nMLP_at_10\n0.538684\n2.328720\n2.328720\n5.422937\n0.0\nNaN\n\n\n11\nMLP_at_11\n1.022086\n4.418454\n4.418454\n19.522739\n0.0\n0.55747\n\n\n\n\n\n\n\n\n\nτ = 14\n\ndim = at[1][0].shape[1]\n\nresults = get_grid(at[1], dim, 'retorno', 14)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: relu\nMejor número de epocas: 100\nMejor Longitud de paso μ (Learning Rate): 0.001\nMejor Puntuación: 0.9947286045222513\n\n\n\nmodels = build_model('MLP', 'retorno', dim, neurons, dropouts, best_activation, best_mu, 14)\n\nmlp_at14_metrics = best_model('at', models, at[1][2], at[1][3])\nmlp_at14_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'retorno'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\nModelo 0: MSE = 18.0576\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\nModelo 1: MSE = 21.3061\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 2: MSE = 18.5075\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 3: MSE = 21.6683\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 4: MSE = 18.0936\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 5: MSE = 21.4362\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\nModelo 6: MSE = 4.8328\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 7: MSE = 19.3612\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 8: MSE = 24.7572\n\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 9: MSE = 13.9393\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 10: MSE = 25.1080\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 11: MSE = 23.0428\n\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_at_0\n0.982985\n4.249425\n4.249425\n18.057611\n-2.289074e+31\n0.245879\n\n\n1\nMLP_at_1\n1.067749\n4.615857\n4.615857\n21.306138\n-2.700874e+31\n0.245879\n\n\n2\nMLP_at_2\n0.995156\n4.302037\n4.302037\n18.507526\n-2.346108e+31\nNaN\n\n\n3\nMLP_at_3\n1.076787\n4.654926\n4.654926\n21.668332\n-2.746787e+31\n0.245879\n\n\n4\nMLP_at_4\n0.983966\n4.253663\n4.253663\n18.093648\n-2.293642e+31\nNaN\n\n\n5\nMLP_at_5\n1.071004\n4.629929\n4.629929\n21.436242\n-2.717366e+31\nNaN\n\n\n6\nMLP_at_6\n0.508532\n2.198373\n2.198373\n4.832846\n-6.126360e+30\nNaN\n\n\n7\nMLP_at_7\n1.017849\n4.400140\n4.400140\n19.361229\n-2.454327e+31\nNaN\n\n\n8\nMLP_at_8\n1.150979\n4.975658\n4.975658\n24.757176\n-3.138345e+31\nNaN\n\n\n9\nMLP_at_9\n0.863649\n3.733536\n3.733536\n13.939291\n-1.767015e+31\nNaN\n\n\n10\nMLP_at_10\n1.159105\n5.010787\n5.010787\n25.107990\n-3.182816e+31\n0.245879\n\n\n11\nMLP_at_11\n1.110414\n4.800294\n4.800294\n23.042820\n-2.921025e+31\nNaN\n\n\n\n\n\n\n\n\n\nτ = 21\n\ndim = at[2][0].shape[1]\n\nresults = get_grid(at[2], dim, 'retorno', 21)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: sigmoid\nMejor número de epocas: 100\nMejor Longitud de paso μ (Learning Rate): 0.001\nMejor Puntuación: 0.9944296065331537\n\n\n\nmodels = build_model('MLP', 'retorno', dim, neurons, dropouts, best_activation, best_mu, 21)\n\nmlp_at21_metrics = best_model('at', models, at[2][2], at[2][3])\nmlp_at21_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'retorno'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 0: MSE = 15.9112\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 1: MSE = 18.6403\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 2: MSE = 16.3590\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 3: MSE = 14.0434\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 4: MSE = 18.7071\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 5: MSE = 19.2759\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 6: MSE = 8.8351\n\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 7: MSE = 15.1769\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 8: MSE = 12.4313\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 9: MSE = 16.4611\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 10: MSE = 18.3412\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 11: MSE = 17.6120\n\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2116: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/scipy/stats/_stats_py.py:2117: RuntimeWarning:\n\nPrecision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_at_0\n0.922717\n3.988885\n3.988885\n15.911201\n-2.016984e+31\nNaN\n\n\n1\nMLP_at_1\n0.998718\n4.317438\n4.317438\n18.640267\n-2.362935e+31\nNaN\n\n\n2\nMLP_at_2\n0.935610\n4.044621\n4.044621\n16.358958\n-2.073744e+31\n0.011652\n\n\n3\nMLP_at_3\n0.866869\n3.747456\n3.747456\n14.043430\n-1.780216e+31\nNaN\n\n\n4\nMLP_at_4\n1.000508\n4.325176\n4.325176\n18.707149\n-2.371413e+31\nNaN\n\n\n5\nMLP_at_5\n1.015603\n4.390428\n4.390428\n19.275856\n-2.443505e+31\n0.011652\n\n\n6\nMLP_at_6\n0.687578\n2.972386\n2.972386\n8.835076\n-1.119979e+31\nNaN\n\n\n7\nMLP_at_7\n0.901173\n3.895749\n3.895749\n15.176864\n-1.923896e+31\nNaN\n\n\n8\nMLP_at_8\n0.815598\n3.525812\n3.525812\n12.431349\n-1.575861e+31\nNaN\n\n\n9\nMLP_at_9\n0.938525\n4.057223\n4.057223\n16.461059\n-2.086687e+31\nNaN\n\n\n10\nMLP_at_10\n0.990675\n4.282668\n4.282668\n18.341241\n-2.325029e+31\nNaN\n\n\n11\nMLP_at_11\n0.970782\n4.196668\n4.196668\n17.612022\n-2.232589e+31\n0.011652\n\n\n\n\n\n\n\n\n\nτ = 28\n\ndim = at[3][0].shape[1]\n\nresults = get_grid(at[3], dim, 'retorno', 28)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: relu\nMejor número de epocas: 50\nMejor Longitud de paso μ (Learning Rate): 0.001\nMejor Puntuación: 0.9926096353221231\n\n\n\nmodels = build_model('MLP', 'retorno', dim, neurons, dropouts, best_activation, best_mu, 28)\n\nmlp_at28_metrics = best_model('at', models, at[3][2], at[3][3])\nmlp_at28_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'retorno'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 0: MSE = 21.2353\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 1: MSE = 28.0062\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 2: MSE = 13.2884\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 3: MSE = 20.3033\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 4: MSE = 22.2990\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\nModelo 5: MSE = 19.4056\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 6: MSE = 11.5393\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 7: MSE = 17.3125\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 8: MSE = 19.5027\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 9: MSE = 27.7216\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 10: MSE = 16.9307\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\nModelo 11: MSE = 8.9896\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_at_0\n1.065973\n4.608177\n4.608178\n21.235300\n-2.691894e+31\n1.492363e-01\n\n\n1\nMLP_at_1\n1.224176\n5.292086\n5.292086\n28.006174\n-3.550204e+31\n1.041470e-01\n\n\n2\nMLP_at_2\n0.843245\n3.645330\n3.645330\n13.288431\n-1.684509e+31\n2.453968e-11\n\n\n3\nMLP_at_3\n1.042319\n4.505923\n4.505923\n20.303339\n-2.573754e+31\n6.852999e-02\n\n\n4\nMLP_at_4\n1.092344\n4.722177\n4.722178\n22.298961\n-2.826729e+31\n8.630661e-02\n\n\n5\nMLP_at_5\n1.019015\n4.405180\n4.405181\n19.405622\n-2.459955e+31\n2.404680e-04\n\n\n6\nMLP_at_6\n0.785789\n3.396948\n3.396948\n11.539258\n-1.462775e+31\n7.977298e-02\n\n\n7\nMLP_at_7\n0.962491\n4.160828\n4.160828\n17.312489\n-2.194619e+31\n2.762365e-03\n\n\n8\nMLP_at_8\n1.021561\n4.416188\n4.416188\n19.502714\n-2.472263e+31\n2.311245e-02\n\n\n9\nMLP_at_9\n1.217942\n5.265135\n5.265135\n27.721643\n-3.514136e+31\n4.282240e-02\n\n\n10\nMLP_at_10\n0.951819\n4.114693\n4.114694\n16.930703\n-2.146222e+31\n7.021213e-02\n\n\n11\nMLP_at_11\n0.693565\n2.998265\n2.998265\n8.989592\n-1.139566e+31\n1.668459e-10\n\n\n\n\n\n\n\n\n\n\nPredicciones para la volatilidad\n\nVolatilidad con ventana σ = 7\n\nτ = 7\n\ndim = vo_7[0][0].shape[1]\n\nresults = get_grid(vo_7[0], dim, 'volatilidad', 7, 7)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: tanh\nMejor número de epocas: 100\nMejor Longitud de paso μ (Learning Rate): 0.001\nMejor Puntuación: 0.8458686359787372\n\n\n\nmodels = build_model('MLP', 'volatilidad', dim, neurons, dropouts, best_activation, best_mu, 7, 7)\n\nmlp_vol7_7_metrics = best_model('vol7', models, vo_7[0][2], vo_7[0][3])\nmlp_vol7_7_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'volatilidad'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 0: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 1: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 2: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 3: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 4: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 5: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 6: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 7: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 8: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 9: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 10: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 11: MSE = 0.0000\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_vol7_0\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n1\nMLP_vol7_1\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n2\nMLP_vol7_2\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n3\nMLP_vol7_3\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n4\nMLP_vol7_4\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n5\nMLP_vol7_5\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n6\nMLP_vol7_6\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n7\nMLP_vol7_7\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n8\nMLP_vol7_8\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n9\nMLP_vol7_9\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n10\nMLP_vol7_10\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n11\nMLP_vol7_11\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n\n\n\n\n\n\n\nτ = 14\n\ndim = vo_7[1][0].shape[1]\n\nresults = get_grid(vo_7[1], dim, 'volatilidad', 14, 7)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: relu\nMejor número de epocas: 150\nMejor Longitud de paso μ (Learning Rate): 0.01\nMejor Puntuación: 0.8851995559100326\n\n\n\nmodels = build_model('MLP', 'volatilidad', dim, neurons, dropouts, best_activation, best_mu, 14, 7)\n\nmlp_vol7_14_metrics = best_model('vol7', models, vo_7[1][2], vo_7[1][3])\nmlp_vol7_14_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'volatilidad'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 0: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 1: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 2: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 3: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 4: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 5: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 6: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 7: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 8: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 9: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 10: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 11: MSE = 0.0000\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_vol7_0\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n1\nMLP_vol7_1\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n2\nMLP_vol7_2\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n3\nMLP_vol7_3\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n4\nMLP_vol7_4\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n5\nMLP_vol7_5\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n6\nMLP_vol7_6\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n7\nMLP_vol7_7\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n8\nMLP_vol7_8\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n9\nMLP_vol7_9\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n10\nMLP_vol7_10\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n11\nMLP_vol7_11\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n\n\n\n\n\n\n\nτ = 21\n\ndim = vo_7[2][0].shape[1]\n\nresults = get_grid(vo_7[2], dim, 'volatilidad', 21, 7)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: relu\nMejor número de epocas: 100\nMejor Longitud de paso μ (Learning Rate): 0.001\nMejor Puntuación: 0.8838780194852301\n\n\n\nmodels = build_model('MLP', 'volatilidad', dim, neurons, dropouts, best_activation, best_mu, 21, 7)\n\nmlp_vol7_21_metrics = best_model('vol7', models, vo_7[2][2], vo_7[2][3])\nmlp_vol7_21_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'volatilidad'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 0: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 1: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\nModelo 2: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\nModelo 3: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\nModelo 4: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\nModelo 5: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 6: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 7: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\nModelo 8: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\nModelo 9: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 10: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 11: MSE = 0.0000\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_vol7_0\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n1\nMLP_vol7_1\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n2\nMLP_vol7_2\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n3\nMLP_vol7_3\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n4\nMLP_vol7_4\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n5\nMLP_vol7_5\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n6\nMLP_vol7_6\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n7\nMLP_vol7_7\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n8\nMLP_vol7_8\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n9\nMLP_vol7_9\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n10\nMLP_vol7_10\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n11\nMLP_vol7_11\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n\n\n\n\n\n\n\nτ = 28\n\ndim = vo_7[3][0].shape[1]\n\nresults = get_grid(vo_7[3], dim, 'volatilidad', 28, 7)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: relu\nMejor número de epocas: 150\nMejor Longitud de paso μ (Learning Rate): 0.01\nMejor Puntuación: 0.8735585364167684\n\n\n\nmodels = build_model('MLP', 'volatilidad', dim, neurons, dropouts, best_activation, best_mu, 28, 7)\n\nmlp_vol7_28_metrics = best_model('vol7', models, vo_7[3][2], vo_7[3][3])\nmlp_vol7_28_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'volatilidad'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 0: MSE = 0.0451\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 1: MSE = 0.0034\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 2: MSE = 0.0024\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 3: MSE = 0.0102\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 4: MSE = 0.0114\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step\nModelo 5: MSE = 0.0085\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 6: MSE = 0.0019\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 7: MSE = 0.0038\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 8: MSE = 0.0072\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 9: MSE = 0.0286\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 10: MSE = 0.0759\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 11: MSE = 0.0087\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_vol7_0\n7.162994e+14\n0.159050\n0.212475\n0.045146\n0.0\n0.499207\n\n\n1\nMLP_vol7_1\n1.794322e+14\n0.039842\n0.057953\n0.003359\n0.0\n0.989639\n\n\n2\nMLP_vol7_2\n1.590308e+14\n0.035312\n0.049172\n0.002418\n0.0\n0.561823\n\n\n3\nMLP_vol7_3\n2.797398e+14\n0.062115\n0.100935\n0.010188\n0.0\n0.075613\n\n\n4\nMLP_vol7_4\n3.061127e+14\n0.067971\n0.106804\n0.011407\n0.0\n0.185153\n\n\n5\nMLP_vol7_5\n2.837046e+14\n0.062995\n0.092310\n0.008521\n0.0\n0.336134\n\n\n6\nMLP_vol7_6\n1.287562e+14\n0.028590\n0.043272\n0.001872\n0.0\n0.195448\n\n\n7\nMLP_vol7_7\n1.842945e+14\n0.040922\n0.061281\n0.003755\n0.0\n0.485559\n\n\n8\nMLP_vol7_8\n2.483087e+14\n0.055136\n0.084717\n0.007177\n0.0\n0.217519\n\n\n9\nMLP_vol7_9\n5.017698e+14\n0.111415\n0.169115\n0.028600\n0.0\n0.216930\n\n\n10\nMLP_vol7_10\n8.429797e+14\n0.187179\n0.275426\n0.075859\n0.0\n0.200165\n\n\n11\nMLP_vol7_11\n2.745355e+14\n0.060959\n0.093203\n0.008687\n0.0\n0.163997\n\n\n\n\n\n\n\n\n\n\nVolatilidad con ventana σ = 14\n\nτ = 7\n\ndim = vo_14[0][0].shape[1]\n\nresults = get_grid(vo_14[0], dim, 'volatilidad', 7, 14)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: tanh\nMejor número de epocas: 100\nMejor Longitud de paso μ (Learning Rate): 0.001\nMejor Puntuación: 0.9347361334808304\n\n\n\nmodels = build_model('MLP', 'volatilidad', dim, neurons, dropouts, best_activation, best_mu, 7, 14)\n\nmlp_vol14_7_metrics = best_model('vol14', models, vo_14[0][2], vo_14[0][3])\nmlp_vol14_7_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'volatilidad'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 0: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 1: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 2: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 3: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 4: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 5: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 6: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 7: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\nModelo 8: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 9: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 10: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 11: MSE = 0.0000\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_vol14_0\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n1\nMLP_vol14_1\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n2\nMLP_vol14_2\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n3\nMLP_vol14_3\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n4\nMLP_vol14_4\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n5\nMLP_vol14_5\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n6\nMLP_vol14_6\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n7\nMLP_vol14_7\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n8\nMLP_vol14_8\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n9\nMLP_vol14_9\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n10\nMLP_vol14_10\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n11\nMLP_vol14_11\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n\n\n\n\n\n\n\nτ = 14\n\ndim = vo_14[1][0].shape[1]\n\nresults = get_grid(vo_14[1], dim, 'volatilidad', 14, 14)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: tanh\nMejor número de epocas: 150\nMejor Longitud de paso μ (Learning Rate): 0.001\nMejor Puntuación: 0.9331265147277118\n\n\n\nmodels = build_model('MLP', 'volatilidad', dim, neurons, dropouts, best_activation, best_mu, 14, 14)\n\nmlp_vol14_14_metrics = best_model('vol14', models, vo_14[1][2], vo_14[1][3])\nmlp_vol14_14_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'volatilidad'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 0: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 1: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 2: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 3: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 4: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 5: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 6: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 7: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 8: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 9: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 10: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 11: MSE = 0.0000\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_vol14_0\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n1\nMLP_vol14_1\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n2\nMLP_vol14_2\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n3\nMLP_vol14_3\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n4\nMLP_vol14_4\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n5\nMLP_vol14_5\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n6\nMLP_vol14_6\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n7\nMLP_vol14_7\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n8\nMLP_vol14_8\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n9\nMLP_vol14_9\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n10\nMLP_vol14_10\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n11\nMLP_vol14_11\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n\n\n\n\n\n\n\nτ = 21\n\ndim = vo_14[2][0].shape[1]\n\nresults = get_grid(vo_14[2], dim, 'volatilidad', 21, 14)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: relu\nMejor número de epocas: 150\nMejor Longitud de paso μ (Learning Rate): 0.001\nMejor Puntuación: 0.9518923980994929\n\n\n\nmodels = build_model('MLP', 'volatilidad', dim, neurons, dropouts, best_activation, best_mu, 21, 14)\n\nmlp_vol14_21_metrics = best_model('vol14', models, vo_14[2][2], vo_14[2][3])\nmlp_vol14_21_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'volatilidad'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 0: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 1: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 2: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 3: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 4: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 5: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 6: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 7: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 8: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 9: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 10: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 11: MSE = 0.0000\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_vol14_0\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n1\nMLP_vol14_1\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n2\nMLP_vol14_2\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n3\nMLP_vol14_3\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n4\nMLP_vol14_4\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n5\nMLP_vol14_5\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n6\nMLP_vol14_6\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n7\nMLP_vol14_7\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n8\nMLP_vol14_8\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n9\nMLP_vol14_9\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n10\nMLP_vol14_10\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n11\nMLP_vol14_11\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n\n\n\n\n\n\n\nτ = 28\n\ndim = vo_14[3][0].shape[1]\n\nresults = get_grid(vo_14[3], dim, 'volatilidad', 28, 14)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: relu\nMejor número de epocas: 150\nMejor Longitud de paso μ (Learning Rate): 0.01\nMejor Puntuación: 0.9487999966795\n\n\n\nmodels = build_model('MLP', 'volatilidad', dim, neurons, dropouts, best_activation, best_mu, 28, 14)\n\nmlp_vol14_28_metrics = best_model('vol14', models, vo_14[3][2], vo_14[3][3])\nmlp_vol14_28_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'volatilidad'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 0: MSE = 0.0065\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 1: MSE = 0.0091\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 2: MSE = 0.0032\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 3: MSE = 0.0061\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 4: MSE = 0.1288\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 5: MSE = 0.0072\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 6: MSE = 0.0026\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 7: MSE = 0.0012\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 8: MSE = 0.0037\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 9: MSE = 0.0092\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 10: MSE = 0.0022\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 11: MSE = 0.0032\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_vol14_0\n2.896786e+14\n0.064322\n0.080682\n0.006510\n0.0\n0.610347\n\n\n1\nMLP_vol14_1\n3.629986e+14\n0.080602\n0.095384\n0.009098\n0.0\n0.378620\n\n\n2\nMLP_vol14_2\n2.233342e+14\n0.049590\n0.056374\n0.003178\n0.0\n0.370159\n\n\n3\nMLP_vol14_3\n2.897920e+14\n0.064347\n0.077794\n0.006052\n0.0\n0.579961\n\n\n4\nMLP_vol14_4\n1.360298e+15\n0.302047\n0.358896\n0.128806\n0.0\n0.200652\n\n\n5\nMLP_vol14_5\n2.998405e+14\n0.066578\n0.085084\n0.007239\n0.0\n0.563968\n\n\n6\nMLP_vol14_6\n1.894117e+14\n0.042058\n0.051380\n0.002640\n0.0\n0.701491\n\n\n7\nMLP_vol14_7\n1.301509e+14\n0.028899\n0.035027\n0.001227\n0.0\n0.621220\n\n\n8\nMLP_vol14_8\n2.004332e+14\n0.044505\n0.060973\n0.003718\n0.0\n0.255349\n\n\n9\nMLP_vol14_9\n3.788565e+14\n0.084123\n0.096175\n0.009250\n0.0\n0.387125\n\n\n10\nMLP_vol14_10\n1.767774e+14\n0.039252\n0.047189\n0.002227\n0.0\n0.617283\n\n\n11\nMLP_vol14_11\n2.147773e+14\n0.047690\n0.056558\n0.003199\n0.0\n0.483371\n\n\n\n\n\n\n\n\n\n\nVolatilidad con ventana σ = 21\n\nτ = 7\n\ndim = vo_21[0][0].shape[1]\n\nresults = get_grid(vo_21[0], dim, 'volatilidad', 7, 21)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: tanh\nMejor número de epocas: 100\nMejor Longitud de paso μ (Learning Rate): 0.001\nMejor Puntuación: 0.958933652212678\n\n\n\nmodels = build_model('MLP', 'volatilidad', dim, neurons, dropouts, best_activation, best_mu, 7, 21)\n\nmlp_vol21_7_metrics = best_model('vol21', models, vo_21[0][2], vo_21[0][3])\nmlp_vol21_7_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'volatilidad'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 0: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 1: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 2: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 3: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 4: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 5: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 6: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 7: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 8: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 9: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\nModelo 10: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 11: MSE = 0.0000\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_vol21_0\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n1\nMLP_vol21_1\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n2\nMLP_vol21_2\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n3\nMLP_vol21_3\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n4\nMLP_vol21_4\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n5\nMLP_vol21_5\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n6\nMLP_vol21_6\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n7\nMLP_vol21_7\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n8\nMLP_vol21_8\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n9\nMLP_vol21_9\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n10\nMLP_vol21_10\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n11\nMLP_vol21_11\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n\n\n\n\n\n\n\nτ = 14\n\ndim = vo_21[1][0].shape[1]\n\nresults = get_grid(vo_21[1], dim, 'volatilidad', 14, 21)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: relu\nMejor número de epocas: 100\nMejor Longitud de paso μ (Learning Rate): 0.01\nMejor Puntuación: 0.9590606369714157\n\n\n\nmodels = build_model('MLP', 'volatilidad', dim, neurons, dropouts, best_activation, best_mu, 14, 21)\n\nmlp_vol21_14_metrics = best_model('vol21', models, vo_21[1][2], vo_21[1][3])\nmlp_vol21_14_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'volatilidad'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 0: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 1: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 2: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 3: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 4: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 5: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 6: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 7: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 8: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 9: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 10: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 11: MSE = 0.0000\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_vol21_0\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n1\nMLP_vol21_1\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n2\nMLP_vol21_2\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n3\nMLP_vol21_3\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n4\nMLP_vol21_4\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n5\nMLP_vol21_5\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n6\nMLP_vol21_6\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n7\nMLP_vol21_7\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n8\nMLP_vol21_8\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n9\nMLP_vol21_9\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n10\nMLP_vol21_10\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n11\nMLP_vol21_11\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n\n\n\n\n\n\n\nτ = 21\n\ndim = vo_21[2][0].shape[1]\n\nresults = get_grid(vo_21[2], dim, 'volatilidad', 21, 21)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: tanh\nMejor número de epocas: 50\nMejor Longitud de paso μ (Learning Rate): 0.001\nMejor Puntuación: 0.9588696357579239\n\n\n\nmodels = build_model('MLP', 'volatilidad', dim, neurons, dropouts, best_activation, best_mu, 21, 21)\n\nmlp_vol21_21_metrics = best_model('vol21', models, vo_21[2][2], vo_21[2][3])\nmlp_vol21_21_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'volatilidad'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\nModelo 0: MSE = 0.4251\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 1: MSE = 0.1071\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 2: MSE = 1.7569\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 3: MSE = 0.2702\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 4: MSE = 0.0051\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 5: MSE = 0.0045\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 6: MSE = 0.9637\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 7: MSE = 0.4901\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 8: MSE = 1.6636\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 9: MSE = 0.5232\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 10: MSE = 0.0606\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 11: MSE = 0.1050\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_vol21_0\n2.936339e+15\n0.651998\n0.651998\n0.425102\n0.0\n2.045677e-05\n\n\n1\nMLP_vol21_1\n1.473618e+15\n0.327209\n0.327209\n0.107066\n0.0\n3.758186e-03\n\n\n2\nMLP_vol21_2\n5.969479e+15\n1.325491\n1.325491\n1.756925\n0.0\n4.163555e-04\n\n\n3\nMLP_vol21_3\n2.340928e+15\n0.519790\n0.519790\n0.270182\n0.0\n2.181144e-02\n\n\n4\nMLP_vol21_4\n3.220082e+14\n0.071500\n0.071500\n0.005112\n0.0\n3.754697e-02\n\n\n5\nMLP_vol21_5\n3.028158e+14\n0.067239\n0.067239\n0.004521\n0.0\n1.270918e-29\n\n\n6\nMLP_vol21_6\n4.421050e+15\n0.981670\n0.981670\n0.963676\n0.0\n7.620207e-10\n\n\n7\nMLP_vol21_7\n3.152691e+15\n0.700038\n0.700038\n0.490053\n0.0\n3.278896e-05\n\n\n8\nMLP_vol21_8\n5.808749e+15\n1.289801\n1.289801\n1.663587\n0.0\n3.886970e-07\n\n\n9\nMLP_vol21_9\n3.257432e+15\n0.723295\n0.723295\n0.523156\n0.0\n3.281390e-03\n\n\n10\nMLP_vol21_10\n1.108672e+15\n0.246175\n0.246175\n0.060602\n0.0\n5.361850e-26\n\n\n11\nMLP_vol21_11\n1.459581e+15\n0.324092\n0.324092\n0.105036\n0.0\n6.528045e-05\n\n\n\n\n\n\n\n\n\nτ = 28\n\ndim = vo_21[3][0].shape[1]\n\nresults = get_grid(vo_21[3], dim, 'volatilidad', 28, 21)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: relu\nMejor número de epocas: 100\nMejor Longitud de paso μ (Learning Rate): 0.001\nMejor Puntuación: 0.9707786364616338\n\n\n\nmodels = build_model('MLP', 'volatilidad', dim, neurons, dropouts, best_activation, best_mu, 28, 21)\n\nmlp_vol21_28_metrics = best_model('vol21', models, vo_21[3][2], vo_21[3][3])\nmlp_vol21_28_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'volatilidad'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 0: MSE = 0.0085\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 1: MSE = 0.0024\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 2: MSE = 0.0030\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 3: MSE = 0.0033\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 4: MSE = 0.0019\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 5: MSE = 0.0122\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 6: MSE = 0.0051\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 7: MSE = 0.0075\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 8: MSE = 0.0070\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 9: MSE = 0.0094\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 10: MSE = 0.0020\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 11: MSE = 0.0014\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_vol21_0\n1.564857e+14\n0.069459\n0.092173\n0.008496\n-3.865243\n0.008532\n\n\n1\nMLP_vol21_1\n1.839391e+14\n0.043247\n0.048635\n0.002365\n-0.354568\n0.378879\n\n\n2\nMLP_vol21_2\n5.063465e+13\n0.033515\n0.054505\n0.002971\n-0.701261\n0.002028\n\n\n3\nMLP_vol21_3\n8.273847e+13\n0.040933\n0.057785\n0.003339\n-0.912148\n0.004419\n\n\n4\nMLP_vol21_4\n2.666782e+13\n0.023944\n0.043379\n0.001882\n-0.077605\n0.002372\n\n\n5\nMLP_vol21_5\n1.488474e+14\n0.074913\n0.110356\n0.012178\n-5.974118\n0.111068\n\n\n6\nMLP_vol21_6\n9.634084e+13\n0.049596\n0.071310\n0.005085\n-1.912015\n0.002437\n\n\n7\nMLP_vol21_7\n3.345719e+14\n0.077327\n0.086720\n0.007520\n-3.306620\n0.603024\n\n\n8\nMLP_vol21_8\n1.294174e+14\n0.061066\n0.083875\n0.007035\n-3.028680\n0.005031\n\n\n9\nMLP_vol21_9\n1.295304e+14\n0.065887\n0.097198\n0.009448\n-4.410232\n0.046383\n\n\n10\nMLP_vol21_10\n1.456937e+14\n0.038858\n0.044239\n0.001957\n-0.120765\n0.297847\n\n\n11\nMLP_vol21_11\n1.097412e+14\n0.033493\n0.036828\n0.001356\n0.223314\n0.044301\n\n\n\n\n\n\n\n\n\n\nVolatilidad con ventana σ = 28\n\nτ = 7\n\ndim = vo_28[0][0].shape[1]\n\nresults = get_grid(vo_28[0], dim, 'volatilidad', 7, 28)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: tanh\nMejor número de epocas: 100\nMejor Longitud de paso μ (Learning Rate): 0.001\nMejor Puntuación: 0.8510466752155451\n\n\n\nmodels = build_model('MLP', 'volatilidad', dim, neurons, dropouts, best_activation, best_mu, 7, 28)\n\nmlp_vol28_7_metrics = best_model('vol28', models, vo_28[0][2], vo_28[0][3])\nmlp_vol28_7_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'volatilidad'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 0: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 1: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 2: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 3: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 4: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 5: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 6: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 7: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 8: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 9: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 10: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 11: MSE = 0.0000\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_vol28_0\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n1\nMLP_vol28_1\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n2\nMLP_vol28_2\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n3\nMLP_vol28_3\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n4\nMLP_vol28_4\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n5\nMLP_vol28_5\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n6\nMLP_vol28_6\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n7\nMLP_vol28_7\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n8\nMLP_vol28_8\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n9\nMLP_vol28_9\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n10\nMLP_vol28_10\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n11\nMLP_vol28_11\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n\n\n\n\n\n\n\nτ = 14\n\ndim = vo_28[1][0].shape[1]\n\nresults = get_grid(vo_28[1], dim, 'volatilidad', 14, 28)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: relu\nMejor número de epocas: 100\nMejor Longitud de paso μ (Learning Rate): 0.01\nMejor Puntuación: 0.8812891588138462\n\n\n\nmodels = build_model('MLP', 'volatilidad', dim, neurons, dropouts, best_activation, best_mu, 14, 28)\n\nmlp_vol28_14_metrics = best_model('vol28', models, vo_28[1][2], vo_28[1][3])\nmlp_vol28_14_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'volatilidad'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 0: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 1: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 2: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 3: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 4: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 5: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 6: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 7: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 8: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 9: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 10: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 11: MSE = 0.0000\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_vol28_0\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n1\nMLP_vol28_1\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n2\nMLP_vol28_2\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n3\nMLP_vol28_3\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n4\nMLP_vol28_4\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n5\nMLP_vol28_5\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n6\nMLP_vol28_6\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n7\nMLP_vol28_7\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n8\nMLP_vol28_8\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n9\nMLP_vol28_9\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n10\nMLP_vol28_10\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n11\nMLP_vol28_11\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n\n\n\n\n\n\n\nτ = 21\n\ndim = vo_28[2][0].shape[1]\n\nresults = get_grid(vo_28[2], dim, 'volatilidad', 21, 28)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: relu\nMejor número de epocas: 100\nMejor Longitud de paso μ (Learning Rate): 0.001\nMejor Puntuación: 0.8813218143824045\n\n\n\nmodels = build_model('MLP', 'volatilidad', dim, neurons, dropouts, best_activation, best_mu, 21, 28)\n\nmlp_vol28_21_metrics = best_model('vol28', models, vo_28[2][2], vo_28[2][3])\nmlp_vol28_21_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'volatilidad'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 0: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 1: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 2: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 3: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 4: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 5: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 6: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 7: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 8: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 9: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 10: MSE = 0.0000\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\nModelo 11: MSE = 0.0000\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_vol28_0\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n1\nMLP_vol28_1\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n2\nMLP_vol28_2\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n3\nMLP_vol28_3\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n4\nMLP_vol28_4\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n5\nMLP_vol28_5\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n6\nMLP_vol28_6\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n7\nMLP_vol28_7\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n8\nMLP_vol28_8\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n9\nMLP_vol28_9\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n10\nMLP_vol28_10\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n11\nMLP_vol28_11\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\n\n\n\n\n\n\n\n\n\nτ = 28\n\ndim = vo_28[3][0].shape[1]\n\nresults = get_grid(vo_28[3], dim, 'volatilidad', 28, 28)\n\nbest_params = results.best_params_\nbest_activation = results.best_params_['model__activation']\nbest_epochs = results.best_params_['epochs']\nbest_mu = results.best_params_['model__learning_rate']\nbest_score = results.best_score_\n\nprint(f\"Mejor función de activación: {best_activation}\")\nprint(f\"Mejor número de epocas: {best_epochs}\")\nprint(f\"Mejor Longitud de paso μ (Learning Rate): {best_mu}\")\nprint(f\"Mejor Puntuación: {best_score}\")\n\nCargando resultados guardados...\nMejor función de activación: relu\nMejor número de epocas: 100\nMejor Longitud de paso μ (Learning Rate): 0.01\nMejor Puntuación: 0.8743495907125691\n\n\n\nmodels = build_model('MLP', 'volatilidad', dim, neurons, dropouts, best_activation, best_mu, 28, 28)\n\nmlp_vol28_28_metrics = best_model('vol28', models, vo_28[3][2], vo_28[3][3])\nmlp_vol28_28_metrics\n\n/opt/anaconda3/envs/pt_ts/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning:\n\nSkipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n\n\n\nModelos cargados desde la carpeta 'volatilidad'.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 0: MSE = 0.0021\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 1: MSE = 0.0005\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 2: MSE = 0.0006\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 3: MSE = 0.0012\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\nModelo 4: MSE = 0.0017\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 5: MSE = 0.0017\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 6: MSE = 0.0007\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 7: MSE = 0.0004\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 8: MSE = 0.0003\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 9: MSE = 0.0057\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 10: MSE = 0.0027\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\nModelo 11: MSE = 0.0017\n\n\n\n\n\n\n\n\n\nModel Name\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\n\n\n\n\n0\nMLP_vol28_0\n1.473632e+14\n0.032721\n0.045414\n0.002062\n0.0\n0.284712\n\n\n1\nMLP_vol28_1\n6.236684e+13\n0.013848\n0.021517\n0.000463\n0.0\n0.103652\n\n\n2\nMLP_vol28_2\n7.109195e+13\n0.015786\n0.023828\n0.000568\n0.0\n0.233961\n\n\n3\nMLP_vol28_3\n1.109140e+14\n0.024628\n0.035262\n0.001243\n0.0\n0.214152\n\n\n4\nMLP_vol28_4\n1.314559e+14\n0.029189\n0.040948\n0.001677\n0.0\n0.248508\n\n\n5\nMLP_vol28_5\n1.293315e+14\n0.028717\n0.041125\n0.001691\n0.0\n0.180185\n\n\n6\nMLP_vol28_6\n6.711894e+13\n0.014903\n0.026100\n0.000681\n0.0\n0.008449\n\n\n7\nMLP_vol28_7\n6.612054e+13\n0.014682\n0.020149\n0.000406\n0.0\n0.416803\n\n\n8\nMLP_vol28_8\n4.960191e+13\n0.011014\n0.017467\n0.000305\n0.0\n0.047961\n\n\n9\nMLP_vol28_9\n2.180156e+14\n0.048409\n0.075491\n0.005699\n0.0\n0.111378\n\n\n10\nMLP_vol28_10\n1.742931e+14\n0.038701\n0.052252\n0.002730\n0.0\n0.267459\n\n\n11\nMLP_vol28_11\n1.311039e+14\n0.029111\n0.041407\n0.001715\n0.0\n0.247492\n\n\n\n\n\n\n\n\n\n\n\nSelección del mejor modelo MLP\n\nmetric_vars = [\n    'mlp_p7_metrics', 'mlp_p14_metrics', 'mlp_p21_metrics', 'mlp_p28_metrics',\n    'mlp_at7_metrics', 'mlp_at14_metrics', 'mlp_at21_metrics', 'mlp_at28_metrics',\n    'mlp_vol7_7_metrics', 'mlp_vol7_14_metrics', 'mlp_vol7_21_metrics', 'mlp_vol7_28_metrics',\n    'mlp_vol14_7_metrics', 'mlp_vol14_14_metrics', 'mlp_vol14_21_metrics', 'mlp_vol14_28_metrics',\n    'mlp_vol21_7_metrics', 'mlp_vol21_14_metrics', 'mlp_vol21_21_metrics', 'mlp_vol21_28_metrics',\n    'mlp_vol28_7_metrics', 'mlp_vol28_14_metrics', 'mlp_vol28_21_metrics', 'mlp_vol28_28_metrics'\n]\n\ndef get_best_metrics(metrics, s_by = 'MSE'):\n    return metrics.sort_values(by = s_by).iloc[0]\n\nsummary = pd.DataFrame()\n\nfor var in metric_vars:\n    metric = globals()[var]\n\n    best_metric = get_best_metrics(metric)\n    best_metric['Model'] = var\n\n    summary = pd.concat([summary, best_metric.to_frame().T], ignore_index = True)\n\nsummary = summary.drop(columns = ['Model Name'])\nsummary\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\nModel\n\n\n\n\n0\n0.390892\n0.039089\n0.039089\n0.001528\n-7933669470522589694504040660992.0\nNaN\nmlp_p7_metrics\n\n\n1\n0.65484\n0.065484\n0.065484\n0.004288\n-22265337444463706845134140211200.0\nNaN\nmlp_p14_metrics\n\n\n2\n0.324668\n0.032467\n0.032467\n0.001054\n-5473179021279250724616449032192.0\n0.011652\nmlp_p21_metrics\n\n\n3\n0.171046\n0.017105\n0.018523\n0.000343\n-1781497716922890013417599401984.0\n0.004948\nmlp_p28_metrics\n\n\n4\n0.538684\n2.32872\n2.32872\n5.422937\n0.0\nNaN\nmlp_at7_metrics\n\n\n5\n0.508532\n2.198373\n2.198373\n4.832846\n-6126360016078316546386823217152.0\nNaN\nmlp_at14_metrics\n\n\n6\n0.687578\n2.972386\n2.972386\n8.835076\n-11199789270786860826263298244608.0\nNaN\nmlp_at21_metrics\n\n\n7\n0.693565\n2.998265\n2.998265\n8.989592\n-11395661428177666513355814207488.0\n0.0\nmlp_at28_metrics\n\n\n8\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\nmlp_vol7_7_metrics\n\n\n9\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\nmlp_vol7_14_metrics\n\n\n10\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\nmlp_vol7_21_metrics\n\n\n11\n128756204624749.71875\n0.02859\n0.043272\n0.001872\n0.0\n0.195448\nmlp_vol7_28_metrics\n\n\n12\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\nmlp_vol14_7_metrics\n\n\n13\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\nmlp_vol14_14_metrics\n\n\n14\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\nmlp_vol14_21_metrics\n\n\n15\n130150873769106.28125\n0.028899\n0.035027\n0.001227\n0.0\n0.62122\nmlp_vol14_28_metrics\n\n\n16\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\nmlp_vol21_7_metrics\n\n\n17\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\nmlp_vol21_14_metrics\n\n\n18\n302815780810947.0625\n0.067239\n0.067239\n0.004521\n0.0\n0.0\nmlp_vol21_21_metrics\n\n\n19\n109741172775789.796875\n0.033493\n0.036828\n0.001356\n0.223314\n0.044301\nmlp_vol21_28_metrics\n\n\n20\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\nmlp_vol28_7_metrics\n\n\n21\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\nmlp_vol28_14_metrics\n\n\n22\n0.0\n0.0\n0.0\n0.0\n1.0\nNaN\nmlp_vol28_21_metrics\n\n\n23\n49601910482066.289062\n0.011014\n0.017467\n0.000305\n0.0\n0.047961\nmlp_vol28_28_metrics\n\n\n\n\n\n\n\n\nbest = [\n    'mlp_p28_metrics', 'mp_at7_metrics', 'mlp_vol7_28_metrics', \n    'mlp_vol14_28_metrics', 'mlp_vol21_28_metrics', 'mlp_vol28_28_metrics'\n]\n\nsummary = summary[summary['Model'].isin(best)]\nsummary\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nJarque-Bera P-Value\nModel\n\n\n\n\n3\n0.171046\n0.017105\n0.018523\n0.000343\n-1781497716922890013417599401984.0\n0.004948\nmlp_p28_metrics\n\n\n11\n128756204624749.71875\n0.02859\n0.043272\n0.001872\n0.0\n0.195448\nmlp_vol7_28_metrics\n\n\n15\n130150873769106.28125\n0.028899\n0.035027\n0.001227\n0.0\n0.62122\nmlp_vol14_28_metrics\n\n\n19\n109741172775789.796875\n0.033493\n0.036828\n0.001356\n0.223314\n0.044301\nmlp_vol21_28_metrics\n\n\n23\n49601910482066.289062\n0.011014\n0.017467\n0.000305\n0.0\n0.047961\nmlp_vol28_28_metrics",
    "crumbs": [
      "Modelos de deep learning"
    ]
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "Modelos estadísticos",
    "section": "",
    "text": "En este capítulo construiremos distintos modelos estadísticos estudiantes durante el módulo de series de tiempo para la predicción de distintas variables obtenidas a partir del conjunto de datos de las acciones del Bitcoin. Los modelos estadísticos que se construirán en este reporte serán:\nTal que para los modelos de suavización exponencial se construirán a mano y usando la librería de statsmodels.tsa.holterwinters para comparar resultados. Y, para los modelos ARIMA y GARCH construiremos utilizando la técnica de rolling al igual que sin aplicarla.",
    "crumbs": [
      "Modelos estadísticos"
    ]
  },
  {
    "objectID": "models.html#variable-del-precio",
    "href": "models.html#variable-del-precio",
    "title": "Modelos estadísticos",
    "section": "Variable del precio",
    "text": "Variable del precio\n\nprice7 = split_dataset(btc, 'Price', 7)\nprice14 = split_dataset(btc, 'Price', 14)\nprice21 = split_dataset(btc, 'Price', 21)\nprice28 = split_dataset(btc, 'Price', 28)\n\nTamaños de los conjuntos de datos para la variable price con τ = 7\nConjunto de entrenamiento: 4943\nConjunto de validación: 7\nConjunto de prueba: 7\n\nTamaños de los conjuntos de datos para la variable price con τ = 14\nConjunto de entrenamiento: 4943\nConjunto de validación: 14\nConjunto de prueba: 14\n\nTamaños de los conjuntos de datos para la variable price con τ = 21\nConjunto de entrenamiento: 4943\nConjunto de validación: 21\nConjunto de prueba: 21\n\nTamaños de los conjuntos de datos para la variable price con τ = 28\nConjunto de entrenamiento: 4943\nConjunto de validación: 28\nConjunto de prueba: 28",
    "crumbs": [
      "Modelos estadísticos"
    ]
  },
  {
    "objectID": "models.html#variable-price",
    "href": "models.html#variable-price",
    "title": "Modelos estadísticos",
    "section": "Variable Price",
    "text": "Variable Price\n\nprice7 = split_dataset(btc, 'Price', 7)\nprice14 = split_dataset(btc, 'Price', 14)\nprice21 = split_dataset(btc, 'Price', 21)\nprice28 = split_dataset(btc, 'Price', 28)\n\nTamaños de los conjuntos de datos para la variable price con τ = 7\nConjunto de entrenamiento: 4943\nConjunto de validación: 7\nConjunto de prueba: 7\n\nTamaños de los conjuntos de datos para la variable price con τ = 14\nConjunto de entrenamiento: 4943\nConjunto de validación: 14\nConjunto de prueba: 14\n\nTamaños de los conjuntos de datos para la variable price con τ = 21\nConjunto de entrenamiento: 4943\nConjunto de validación: 21\nConjunto de prueba: 21\n\nTamaños de los conjuntos de datos para la variable price con τ = 28\nConjunto de entrenamiento: 4943\nConjunto de validación: 28\nConjunto de prueba: 28",
    "crumbs": [
      "Modelos estadísticos"
    ]
  },
  {
    "objectID": "models.html#variable-a_t",
    "href": "models.html#variable-a_t",
    "title": "Modelos estadísticos",
    "section": "Variable \\(A_t\\)",
    "text": "Variable \\(A_t\\)\n\nA_t7 = split_dataset(btc, 'A_t', 7)\nA_t14 = split_dataset(btc, 'A_t', 14)\nA_t21 = split_dataset(btc, 'A_t', 21)\nA_t28 = split_dataset(btc, 'A_t', 28)\n\nTamaños de los conjuntos de datos para la variable a_t con τ = 7\nConjunto de entrenamiento: 4943\nConjunto de validación: 7\nConjunto de prueba: 7\n\nTamaños de los conjuntos de datos para la variable a_t con τ = 14\nConjunto de entrenamiento: 4943\nConjunto de validación: 14\nConjunto de prueba: 14\n\nTamaños de los conjuntos de datos para la variable a_t con τ = 21\nConjunto de entrenamiento: 4943\nConjunto de validación: 21\nConjunto de prueba: 21\n\nTamaños de los conjuntos de datos para la variable a_t con τ = 28\nConjunto de entrenamiento: 4943\nConjunto de validación: 28\nConjunto de prueba: 28",
    "crumbs": [
      "Modelos estadísticos"
    ]
  },
  {
    "objectID": "models.html#variable-sigma",
    "href": "models.html#variable-sigma",
    "title": "Modelos estadísticos",
    "section": "Variable \\(\\sigma\\)",
    "text": "Variable \\(\\sigma\\)\n\n\\(\\sigma\\) tal que \\(\\omega = 7\\)\n\nvol7_7 = split_dataset(btc, 'σ_7', 7)\nvol7_14 = split_dataset(btc, 'σ_7', 14)\nvol7_21 = split_dataset(btc, 'σ_7', 21)\nvol7_28 = split_dataset(btc, 'σ_7', 28)\n\nTamaños de los conjuntos de datos para la variable σ_7 con τ = 7\nConjunto de entrenamiento: 4943\nConjunto de validación: 7\nConjunto de prueba: 7\n\nTamaños de los conjuntos de datos para la variable σ_7 con τ = 14\nConjunto de entrenamiento: 4943\nConjunto de validación: 14\nConjunto de prueba: 14\n\nTamaños de los conjuntos de datos para la variable σ_7 con τ = 21\nConjunto de entrenamiento: 4943\nConjunto de validación: 21\nConjunto de prueba: 21\n\nTamaños de los conjuntos de datos para la variable σ_7 con τ = 28\nConjunto de entrenamiento: 4943\nConjunto de validación: 28\nConjunto de prueba: 28\n\n\n\n\n\n\\(\\sigma\\) tal que \\(\\omega = 14\\)\n\nvol14_7 = split_dataset(btc, 'σ_14', 7)\nvol14_14 = split_dataset(btc, 'σ_14', 14)\nvol14_21 = split_dataset(btc, 'σ_14', 21)\nvol14_28 = split_dataset(btc, 'σ_14', 28)\n\nTamaños de los conjuntos de datos para la variable σ_14 con τ = 7\nConjunto de entrenamiento: 4943\nConjunto de validación: 7\nConjunto de prueba: 7\n\nTamaños de los conjuntos de datos para la variable σ_14 con τ = 14\nConjunto de entrenamiento: 4943\nConjunto de validación: 14\nConjunto de prueba: 14\n\nTamaños de los conjuntos de datos para la variable σ_14 con τ = 21\nConjunto de entrenamiento: 4943\nConjunto de validación: 21\nConjunto de prueba: 21\n\nTamaños de los conjuntos de datos para la variable σ_14 con τ = 28\nConjunto de entrenamiento: 4943\nConjunto de validación: 28\nConjunto de prueba: 28\n\n\n\n\n\n\\(\\sigma\\) tal que \\(\\omega = 21\\)\n\nvol21_7 = split_dataset(btc, 'σ_21', 7)\nvol21_14 = split_dataset(btc, 'σ_21', 14)\nvol21_21 = split_dataset(btc, 'σ_21', 21)\nvol21_28 = split_dataset(btc, 'σ_21', 28)\n\nTamaños de los conjuntos de datos para la variable σ_21 con τ = 7\nConjunto de entrenamiento: 4943\nConjunto de validación: 7\nConjunto de prueba: 7\n\nTamaños de los conjuntos de datos para la variable σ_21 con τ = 14\nConjunto de entrenamiento: 4943\nConjunto de validación: 14\nConjunto de prueba: 14\n\nTamaños de los conjuntos de datos para la variable σ_21 con τ = 21\nConjunto de entrenamiento: 4943\nConjunto de validación: 21\nConjunto de prueba: 21\n\nTamaños de los conjuntos de datos para la variable σ_21 con τ = 28\nConjunto de entrenamiento: 4943\nConjunto de validación: 28\nConjunto de prueba: 28\n\n\n\n\n\n\\(\\sigma\\) tal que \\(\\omega = 28\\)\n\nvol28_7 = split_dataset(btc, 'σ_28', 7)\nvol28_14 = split_dataset(btc, 'σ_28', 14)\nvol28_21 = split_dataset(btc, 'σ_28', 21)\nvol28_28 = split_dataset(btc, 'σ_28', 28)\n\nTamaños de los conjuntos de datos para la variable σ_28 con τ = 7\nConjunto de entrenamiento: 4943\nConjunto de validación: 7\nConjunto de prueba: 7\n\nTamaños de los conjuntos de datos para la variable σ_28 con τ = 14\nConjunto de entrenamiento: 4943\nConjunto de validación: 14\nConjunto de prueba: 14\n\nTamaños de los conjuntos de datos para la variable σ_28 con τ = 21\nConjunto de entrenamiento: 4943\nConjunto de validación: 21\nConjunto de prueba: 21\n\nTamaños de los conjuntos de datos para la variable σ_28 con τ = 28\nConjunto de entrenamiento: 4943\nConjunto de validación: 28\nConjunto de prueba: 28",
    "crumbs": [
      "Modelos estadísticos"
    ]
  },
  {
    "objectID": "models.html#variable-retorno-acumulado",
    "href": "models.html#variable-retorno-acumulado",
    "title": "Modelos estadísticos",
    "section": "Variable retorno acumulado",
    "text": "Variable retorno acumulado\n\nA_t7 = split_dataset(btc, 'A_t', 7)\nA_t14 = split_dataset(btc, 'A_t', 14)\nA_t21 = split_dataset(btc, 'A_t', 21)\nA_t28 = split_dataset(btc, 'A_t', 28)\n\nTamaños de los conjuntos de datos para la variable a_t con τ = 7\nConjunto de entrenamiento: 4943\nConjunto de validación: 7\nConjunto de prueba: 7\n\nTamaños de los conjuntos de datos para la variable a_t con τ = 14\nConjunto de entrenamiento: 4943\nConjunto de validación: 14\nConjunto de prueba: 14\n\nTamaños de los conjuntos de datos para la variable a_t con τ = 21\nConjunto de entrenamiento: 4943\nConjunto de validación: 21\nConjunto de prueba: 21\n\nTamaños de los conjuntos de datos para la variable a_t con τ = 28\nConjunto de entrenamiento: 4943\nConjunto de validación: 28\nConjunto de prueba: 28",
    "crumbs": [
      "Modelos estadísticos"
    ]
  },
  {
    "objectID": "models.html#variable-volatilidad",
    "href": "models.html#variable-volatilidad",
    "title": "Modelos estadísticos",
    "section": "Variable volatilidad",
    "text": "Variable volatilidad\n\n\\(\\sigma\\) tal que \\(\\omega = 7\\)\n\nvol7_7 = split_dataset(btc, 'σ_7', 7)\nvol7_14 = split_dataset(btc, 'σ_7', 14)\nvol7_21 = split_dataset(btc, 'σ_7', 21)\nvol7_28 = split_dataset(btc, 'σ_7', 28)\n\nTamaños de los conjuntos de datos para la variable σ_7 con τ = 7\nConjunto de entrenamiento: 4943\nConjunto de validación: 7\nConjunto de prueba: 7\n\nTamaños de los conjuntos de datos para la variable σ_7 con τ = 14\nConjunto de entrenamiento: 4943\nConjunto de validación: 14\nConjunto de prueba: 14\n\nTamaños de los conjuntos de datos para la variable σ_7 con τ = 21\nConjunto de entrenamiento: 4943\nConjunto de validación: 21\nConjunto de prueba: 21\n\nTamaños de los conjuntos de datos para la variable σ_7 con τ = 28\nConjunto de entrenamiento: 4943\nConjunto de validación: 28\nConjunto de prueba: 28\n\n\n\n\n\n\\(\\sigma\\) tal que \\(\\omega = 14\\)\n\nvol14_7 = split_dataset(btc, 'σ_14', 7)\nvol14_14 = split_dataset(btc, 'σ_14', 14)\nvol14_21 = split_dataset(btc, 'σ_14', 21)\nvol14_28 = split_dataset(btc, 'σ_14', 28)\n\nTamaños de los conjuntos de datos para la variable σ_14 con τ = 7\nConjunto de entrenamiento: 4943\nConjunto de validación: 7\nConjunto de prueba: 7\n\nTamaños de los conjuntos de datos para la variable σ_14 con τ = 14\nConjunto de entrenamiento: 4943\nConjunto de validación: 14\nConjunto de prueba: 14\n\nTamaños de los conjuntos de datos para la variable σ_14 con τ = 21\nConjunto de entrenamiento: 4943\nConjunto de validación: 21\nConjunto de prueba: 21\n\nTamaños de los conjuntos de datos para la variable σ_14 con τ = 28\nConjunto de entrenamiento: 4943\nConjunto de validación: 28\nConjunto de prueba: 28\n\n\n\n\n\n\\(\\sigma\\) tal que \\(\\omega = 21\\)\n\nvol21_7 = split_dataset(btc, 'σ_21', 7)\nvol21_14 = split_dataset(btc, 'σ_21', 14)\nvol21_21 = split_dataset(btc, 'σ_21', 21)\nvol21_28 = split_dataset(btc, 'σ_21', 28)\n\nTamaños de los conjuntos de datos para la variable σ_21 con τ = 7\nConjunto de entrenamiento: 4943\nConjunto de validación: 7\nConjunto de prueba: 7\n\nTamaños de los conjuntos de datos para la variable σ_21 con τ = 14\nConjunto de entrenamiento: 4943\nConjunto de validación: 14\nConjunto de prueba: 14\n\nTamaños de los conjuntos de datos para la variable σ_21 con τ = 21\nConjunto de entrenamiento: 4943\nConjunto de validación: 21\nConjunto de prueba: 21\n\nTamaños de los conjuntos de datos para la variable σ_21 con τ = 28\nConjunto de entrenamiento: 4943\nConjunto de validación: 28\nConjunto de prueba: 28\n\n\n\n\n\n\\(\\sigma\\) tal que \\(\\omega = 28\\)\n\nvol28_7 = split_dataset(btc, 'σ_28', 7)\nvol28_14 = split_dataset(btc, 'σ_28', 14)\nvol28_21 = split_dataset(btc, 'σ_28', 21)\nvol28_28 = split_dataset(btc, 'σ_28', 28)\n\nTamaños de los conjuntos de datos para la variable σ_28 con τ = 7\nConjunto de entrenamiento: 4943\nConjunto de validación: 7\nConjunto de prueba: 7\n\nTamaños de los conjuntos de datos para la variable σ_28 con τ = 14\nConjunto de entrenamiento: 4943\nConjunto de validación: 14\nConjunto de prueba: 14\n\nTamaños de los conjuntos de datos para la variable σ_28 con τ = 21\nConjunto de entrenamiento: 4943\nConjunto de validación: 21\nConjunto de prueba: 21\n\nTamaños de los conjuntos de datos para la variable σ_28 con τ = 28\nConjunto de entrenamiento: 4943\nConjunto de validación: 28\nConjunto de prueba: 28",
    "crumbs": [
      "Modelos estadísticos"
    ]
  },
  {
    "objectID": "models.html#simple-exponential-smoothing",
    "href": "models.html#simple-exponential-smoothing",
    "title": "Modelos estadísticos",
    "section": "Simple Exponential Smoothing",
    "text": "Simple Exponential Smoothing\n\nCaso sin librería\n\n#def exp_smooth(data, param, start = None)",
    "crumbs": [
      "Modelos estadísticos"
    ]
  },
  {
    "objectID": "models.html#exponential-smoothing",
    "href": "models.html#exponential-smoothing",
    "title": "Modelos estadísticos",
    "section": "Exponential Smoothing",
    "text": "Exponential Smoothing\nSabemos que una suavización exponencial está dada por:\n\\[\n\\tilde{y} = \\lambda y_T + (1 - \\lambda) \\tilde{y}_{T - 1}\n\\]\ntal que \\(\\lambda\\) es nuestro parámetro a optimizar en este tipo de modelos. Además, note que \\(\\tilde{y}\\) es nuestra serie de tiempo suavizada de forma que \\(y_T = \\tilde{y}^{0}\\).\nPara obtener nuestras suavizaciones sin librería construyamos algunas funciones importantes para su construcción:\n\nclass ExponentialSmoothing:\n    def __init__(self, data: pd.Series, param: float, start: Optional[float] = None):\n        self.data = data\n        self.param = param\n        self.start = start if start is not None else data.iloc[0]\n\n    def smooth(self) -&gt; pd.Series:\n        y = self.data.copy()\n        y.iloc[0] = self.param * y.iloc[0] + (1 - self.param) * self.start\n\n        for i in range(1, len(y)):\n            y.iloc[i] = self.param * y.iloc[i] + (1 - self.param) * y.iloc[i - 1]\n\n        return y\n\n    def evaluate(self, smoothed_data: Optional[pd.Series] = None) -&gt; pd.DataFrame:\n        if smoothed_data is None:\n            smoothed_data = self.smooth()\n\n        T = len(self.data)\n        actual_values = self.data.values\n        predicted_values = pd.concat([pd.Series([self.data.iloc[0]]), smoothed_data.iloc[:-1]], ignore_index=True).values\n        error = actual_values - predicted_values\n        mean_actual = np.mean(actual_values)\n\n        SSE = sum(error**2)\n        MAPE = 100 * sum(abs(error / actual_values)) / T\n        MAE = np.mean(np.abs(error))\n        MSE = SSE / T\n        RMSE = np.sqrt(MSE)\n        R2 = 1 - (SSE / (np.sum((actual_values - mean_actual) ** 2)))\n\n\n        return pd.DataFrame({\n            'MAPE': [MAPE],\n            'MAE': [MAE],\n            'RMSE': [RMSE],\n            'MSE': [MSE],\n            'R2': [R2]\n        })\n\n    def optimize_param(self) -&gt; float:\n        smoothed_data = self.smooth()\n        return self.evaluate(smoothed_data)['MAE'].values[0]\n\n    def calculate_metrics(self, y: pd.Series, y_pred: pd.Series, metric_name: str, case: str) -&gt; pd.DataFrame:\n        residuals = y - y_pred\n        metrics_df = self.evaluate(y_pred)\n        metrics_df.index = metrics_df.index.map({0: f'{metric_name} {case}'})\n        \n        max_lag = min(10, len(residuals) - 1)\n        if max_lag &gt; 0:\n            ljung_box_pval = acorr_ljungbox(residuals, lags=[max_lag], return_df=True)['lb_pvalue'].iloc[0]\n        else:\n            ljung_box_pval = np.nan\n\n        jb_pvalue = jarque_bera(residuals)[1]\n        \n        metrics_df['Ljung-Box p-value'] = ljung_box_pval\n        metrics_df['Jarque-Bera p-value'] = jb_pvalue\n        return metrics_df, residuals\n\n    def plot_residuals(self, residuals: pd.Series, model_name: str):\n        fig, ax = plt.subplots(figsize=(8.6, 5))\n        ax.plot(residuals.index, residuals.values, color='#1f77b4', linestyle='-', marker='o', markersize=2)\n        ax.set_title(f'{model_name}', fontsize=14)\n        ax.set_xlabel('Índice', fontsize=12)\n        ax.set_ylabel('Residuo', fontsize=12)\n        ax.grid(True, linestyle='--', alpha=0.7)\n        plt.show()\n\n    def plot_diagnostics(self, residuals: pd.Series, model_name: str):\n        fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(7.6, 6))\n        sm.qqplot(residuals, line='s', ax=axes[0])\n        axes[0].set_title('QQ Plot de los Residuos')\n        sm.graphics.tsa.plot_pacf(residuals, lags=30, ax=axes[1])\n        axes[1].set_title('Autocorrelación Parcial de los Residuos')\n        plt.suptitle(f'{model_name}', fontsize=16)\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n        plt.show()\n\n    def residuals(self, model_name: str, metric_name: str, case: str, y: pd.Series, y_pred: pd.Series, show_info: bool = False) -&gt; pd.DataFrame:\n        metrics_df, residuals = self.calculate_metrics(y, y_pred, metric_name, case)\n\n        if show_info:\n            self.plot_residuals(residuals, model_name)\n            self.plot_diagnostics(residuals, model_name)\n\n            ljung_box_pval = metrics_df['Ljung-Box p-value'].iloc[0]\n            jb_pvalue = metrics_df['Jarque-Bera p-value'].iloc[0]\n\n            if ljung_box_pval &gt; 0.05:\n                print('No se rechaza H0: los residuales son independientes (no correlacionados).')\n            else:\n                print('Se rechaza H0: hay autocorrelación en los residuales.')\n\n            print(f'Jarque-Bera p-value: {jb_pvalue}')\n            if jb_pvalue &gt; 0.05:\n                print('No se rechaza H0: los residuales siguen una distribución normal.')\n            else:\n                print('Se rechaza H0: los residuales no siguen una distribución normal.')\n\n        return metrics_df\n\n    def plot_preds(self, y_train: pd.Series, y_val: pd.Series, y_test: pd.Series,\n                            y_pred_train: pd.Series, y_pred_val: pd.Series, y_pred_test: pd.Series):\n        fig = go.Figure()\n\n        fig.add_trace(go.Scatter(x=y_train[-250:].index, y=y_train[-250:], mode='lines', name='y_train', line=dict(color='#FDDBBB', dash = 'dash')))\n        fig.add_trace(go.Scatter(x=y_val.index, y=y_val, mode='lines', name='y_val', line=dict(color='#BFECFF', dash = 'dash')))\n        fig.add_trace(go.Scatter(x=y_test.index, y=y_test, mode='lines', name='y_test', line=dict(color='#C9E9D2', dash = 'dash')))\n\n        fig.add_trace(go.Scatter(x=y_pred_train[-250:].index, y=y_pred_train[-250:], mode='lines', name='y_pred_train',\n                                 line=dict(color='#CB9DF0', dash='dash')))\n        fig.add_trace(go.Scatter(x=y_pred_val.index, y=y_pred_val, mode='lines', name='y_pred_val',\n                                 line=dict(color='#FFCCEA', dash='dash')))\n        fig.add_trace(go.Scatter(x=y_pred_test.index, y=y_pred_test, mode='lines', name='y_pred_test',\n                                 line=dict(color='#EEEEEE', dash='dash')))\n\n        fig.update_layout(\n            title=\"Comparación de y v.s. y_pred\",\n            xaxis_title=\"\",\n            yaxis_title=\"\",\n            legend_title=\"Series\",\n            margin={'b': 0, 'r': 30, 'l': 30, 't': 80},\n            plot_bgcolor='rgba(0, 0, 0, 0.0)',\n            paper_bgcolor='rgba(0, 0, 0, 0.0)',\n            font_color=\"white\",\n            hoverlabel=dict(\n                bgcolor=\"#222\"\n            ),\n            xaxis=dict(gridcolor='#222', tickfont=dict(color='white')),\n            yaxis=dict(gridcolor='#222', tickfont=dict(color='white')),\n        )\n\n        return fig\n\nPara encontrar el mejor modelo y optimizar el hiperparámetro \\(\\lambda\\) evaluaremos nuestro modelo para \\(\\lambda\\) = 0.1, 0.2, 0.3, …, 1. De esta forma, creemos nuestro vector de posibles parámetros\n\nparams = np.arange(0.1, 1, 0.1)\n\n\nCaso simple sin librería\n\nPrice\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(price7[0], param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\n8.127488\n674.145803\n1543.999512\n2.383934e+06\n0.989635\n0.1\n\n\n1\n5.635438\n465.403502\n1099.627177\n1.209180e+06\n0.994743\n0.2\n\n\n2\n4.594736\n384.453951\n916.213559\n8.394473e+05\n0.996350\n0.3\n\n\n3\n4.014740\n338.103495\n814.197857\n6.629181e+05\n0.997118\n0.4\n\n\n4\n3.644007\n307.473577\n750.827255\n5.637416e+05\n0.997549\n0.5\n\n\n5\n3.396538\n286.395022\n709.679738\n5.036453e+05\n0.997810\n0.6\n\n\n6\n3.226568\n271.655603\n683.013517\n4.665075e+05\n0.997972\n0.7\n\n\n7\n3.112007\n262.536871\n666.757236\n4.445652e+05\n0.998067\n0.8\n\n\n8\n3.046369\n259.080426\n658.687453\n4.338692e+05\n0.998114\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(price7[0], best_param)\nprice7_se = model.smooth()\n\nmetrics_price7 = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'train τ = 7', price7[0], price7_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(price7[1], best_param)\nprice7_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(price7[2], best_param)\nprice7_se_test = model.smooth()\n\nmetrics_price7_val = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'val τ = 7', price7[1], price7_se_val)\nmetrics_price7_test = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'test τ = 7', price7[2], price7_se_test)\n\npd.concat([metrics_price7, metrics_price7_val, metrics_price7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\n3.046369\n259.080426\n658.687453\n4.338692e+05\n0.998114\n2.388399e-11\n0.000000\n\n\nMAE val τ = 7\n5.515992\n2595.563591\n3259.226089\n1.062255e+07\n-1.436019\n3.537907e-01\n0.713997\n\n\nMAE test τ = 7\n1.900521\n876.181038\n1056.085949\n1.115318e+06\n0.744230\n3.443839e-01\n0.877127\n\n\n\n\n\n\n\n\nfig = model.plot_preds(price7[0][-500:], price7[1], price7[2], price7_se, price7_se_val, price7_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(price14[0], best_param)\nprice14_se = model.smooth()\n\nmetrics_price14 = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'train τ = 14', price14[0], price14_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(price14[1], best_param)\nprice14_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(price14[2], best_param)\nprice14_se_test = model.smooth()\n\nmetrics_price14_val = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'val τ = 14', price14[1], price14_se_val)\nmetrics_price14_test = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'test τ = 14', price14[2], price14_se_test)\n\npd.concat([metrics_price14, metrics_price14_val, metrics_price14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\n3.046369\n259.080426\n658.687453\n4.338692e+05\n0.998114\n2.388399e-11\n0.000000\n\n\nMAE val τ = 14\n13.835246\n7146.479407\n7626.989891\n5.817097e+07\n-99.599588\n1.086528e-02\n0.536111\n\n\nMAE test τ = 14\n0.976014\n503.811603\n686.143049\n4.707923e+05\n0.185822\n2.466010e-01\n0.033318\n\n\n\n\n\n\n\n\nfig = model.plot_preds(price14[0][-500:], price14[1], price14[2], price14_se, price14_se_val, price14_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(price21[0], best_param)\nprice21_se = model.smooth()\n\nmetrics_price21 = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'train τ = 21', price21[0], price21_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(price21[1], best_param)\nprice21_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(price21[2], best_param)\nprice21_se_test = model.smooth()\n\nmetrics_price21_val = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'val τ = 21', price21[1], price21_se_val)\nmetrics_price21_test = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'test τ = 21', price21[2], price21_se_test)\n\npd.concat([metrics_price21, metrics_price21_val, metrics_price21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\n3.046369\n259.080426\n658.687453\n4.338692e+05\n0.998114\n2.388399e-11\n0.000000\n\n\nMAE val τ = 21\n21.587948\n13201.685005\n14045.488475\n1.972757e+08\n-3.394583\n2.492182e-02\n0.328854\n\n\nMAE test τ = 21\n2.479540\n1528.328258\n2218.995130\n4.923939e+06\n0.890313\n8.062632e-01\n0.378663\n\n\n\n\n\n\n\n\nfig = model.plot_preds(price21[0][-500:], price21[1], price21[2], price21_se, price21_se_val, price21_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(price28[0], best_param)\nprice28_se = model.smooth()\n\nmetrics_price28 = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'train τ = 28', price28[0], price28_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(price28[1], best_param)\nprice28_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(price28[2], best_param)\nprice28_se_test = model.smooth()\n\nmetrics_price28_val = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'val τ = 28', price28[1], price28_se_val)\nmetrics_price28_test = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'test τ = 28', price28[2], price28_se_test)\n\npd.concat([metrics_price28, metrics_price28_val, metrics_price28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\n3.046369\n259.080426\n658.687453\n4.338692e+05\n0.998114\n2.388399e-11\n0.000000\n\n\nMAE val τ = 28\n26.932500\n17886.266519\n18638.252370\n3.473845e+08\n-18.054225\n1.716122e-03\n0.185748\n\n\nMAE test τ = 28\n3.260694\n2141.568336\n2761.802083\n7.627551e+06\n0.581625\n6.889920e-01\n0.953362\n\n\n\n\n\n\n\n\nfig = model.plot_preds(price28[0][-500:], price28[1], price28[2], price28_se, price28_se_val, price28_se_test)\nfig.show()\n\n\n\n\nmetrics_price = pd.concat([\n    metrics_price7, metrics_price7_val, metrics_price7_test,\n    metrics_price14, metrics_price14_val, metrics_price14_test,\n    metrics_price21, metrics_price21_val, metrics_price21_test,\n    metrics_price28, metrics_price28_val, metrics_price28_test\n])\n\n\n\n\nRetorno acumulado\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(A_t7[0], param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\nNaN\n0.093896\n0.183250\n0.033581\n0.999102\n0.1\n\n\n1\nNaN\n0.061176\n0.127675\n0.016301\n0.999564\n0.2\n\n\n2\nNaN\n0.048750\n0.105511\n0.011132\n0.999702\n0.3\n\n\n3\nNaN\n0.042050\n0.093687\n0.008777\n0.999765\n0.4\n\n\n4\nNaN\n0.037878\n0.086676\n0.007513\n0.999799\n0.5\n\n\n5\nNaN\n0.035097\n0.082335\n0.006779\n0.999819\n0.6\n\n\n6\nNaN\n0.033219\n0.079635\n0.006342\n0.999830\n0.7\n\n\n7\nNaN\n0.031947\n0.078023\n0.006088\n0.999837\n0.8\n\n\n8\nNaN\n0.031174\n0.077196\n0.005959\n0.999841\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(A_t7[0], best_param)\nA_t7_se = model.smooth()\n\nmetrics_A_t7 = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'train τ = 7', A_t7[0], A_t7_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(A_t7[1], best_param)\nA_t7_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(A_t7[2], best_param)\nA_t7_se_test = model.smooth()\n\nmetrics_A_t7_val = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'val τ = 7', A_t7[1], A_t7_se_val)\nmetrics_A_t7_test = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'test τ = 7', A_t7[2], A_t7_se_test)\n\npd.concat([metrics_A_t7, metrics_A_t7_val, metrics_A_t7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\nNaN\n0.031174\n0.077196\n0.005959\n0.999841\n1.962549e-52\n0.000000\n\n\nMAE val τ = 7\n0.250985\n0.058543\n0.073220\n0.005361\n-1.466918\n3.598554e-01\n0.713362\n\n\nMAE test τ = 7\n0.083911\n0.019555\n0.023601\n0.000557\n0.743702\n3.236181e-01\n0.861572\n\n\n\n\n\n\n\n\nfig = model.plot_preds(A_t7[0][-500:], A_t7[1], A_t7[2], A_t7_se, A_t7_se_val, A_t7_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(A_t14[0], best_param)\nA_t14_se = model.smooth()\n\nmetrics_A_t14 = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'train τ = 14', A_t14[0], A_t14_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(A_t14[1], best_param)\nA_t14_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(A_t14[2], best_param)\nA_t14_se_test = model.smooth()\n\nmetrics_A_t14_val = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'val τ = 14', A_t14[1], A_t14_se_val)\nmetrics_A_t14_test = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'test τ = 14', A_t14[2], A_t14_se_test)\n\npd.concat([metrics_A_t14, metrics_A_t14_val, metrics_A_t14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\nNaN\n0.031174\n0.077196\n0.005959\n0.999841\n1.962549e-52\n0.000000\n\n\nMAE val τ = 14\n0.654477\n0.153282\n0.163826\n0.026839\n-115.084439\n1.399318e-02\n0.556588\n\n\nMAE test τ = 14\n0.042235\n0.009891\n0.013638\n0.000186\n0.195563\n2.670533e-01\n0.020649\n\n\n\n\n\n\n\n\nfig = model.plot_preds(A_t14[0][-500:], A_t14[1], A_t14[2], A_t14_se, A_t14_se_val, A_t14_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(A_t21[0], best_param)\nA_t21_se = model.smooth()\n\nmetrics_A_t21 = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'train τ = 21', A_t21[0], A_t21_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(A_t21[1], best_param)\nA_t21_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(A_t21[2], best_param)\nA_t21_se_test = model.smooth()\n\nmetrics_A_t21_val = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'val τ = 21', A_t21[1], A_t21_se_val)\nmetrics_A_t21_test = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'test τ = 21', A_t21[2], A_t21_se_test)\n\npd.concat([metrics_A_t21, metrics_A_t21_val, metrics_A_t21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\nNaN\n0.031174\n0.077196\n0.005959\n0.999841\n1.962549e-52\n0.000000\n\n\nMAE val τ = 21\n1.080485\n0.254931\n0.268152\n0.071905\n-4.091669\n1.539759e-02\n0.360388\n\n\nMAE test τ = 21\n0.108676\n0.025648\n0.037183\n0.001383\n0.902099\n8.409056e-01\n0.317597\n\n\n\n\n\n\n\n\nfig = model.plot_preds(A_t21[0][-500:], A_t21[1], A_t21[2], A_t21_se, A_t21_se_val, A_t21_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(A_t28[0], best_param)\nA_t28_se = model.smooth()\n\nmetrics_A_t28 = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'train τ = 28', A_t28[0], A_t28_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(A_t28[1], best_param)\nA_t28_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(A_t28[2], best_param)\nA_t28_se_test = model.smooth()\n\nmetrics_A_t28_val = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'val τ = 28', A_t28[1], A_t28_se_val)\nmetrics_A_t28_test = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'test τ = 28', A_t28[2], A_t28_se_test)\n\npd.concat([metrics_A_t28, metrics_A_t28_val, metrics_A_t28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\nNaN\n0.031174\n0.077196\n0.005959\n0.999841\n1.962549e-52\n0.000000\n\n\nMAE val τ = 28\n1.415508\n0.335268\n0.348769\n0.121640\n-23.813500\n1.094760e-03\n0.180574\n\n\nMAE test τ = 28\n0.139763\n0.033090\n0.043296\n0.001875\n0.617614\n7.767905e-01\n0.984256\n\n\n\n\n\n\n\n\nfig = model.plot_preds(A_t28[0][-500:], A_t28[1], A_t28[2], A_t28_se, A_t28_se_val, A_t28_se_test)\nfig.show()\n\n\n\n\nmetrics_A_t = pd.concat([\n    metrics_A_t7, metrics_A_t7_val, metrics_A_t7_test,\n    metrics_A_t14, metrics_A_t14_val, metrics_A_t14_test,\n    metrics_A_t21, metrics_A_t21_val, metrics_A_t21_test,\n    metrics_A_t28, metrics_A_t28_val, metrics_A_t28_test\n])\n\n\n\n\nVolatilidad \\(\\omega = 7\\)\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(vol7_7[0], param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\nNaN\n0.017460\n0.049634\n0.002464\n0.418100\n0.1\n\n\n1\nNaN\n0.014276\n0.042103\n0.001773\n0.581291\n0.2\n\n\n2\nNaN\n0.011940\n0.036801\n0.001354\n0.680098\n0.3\n\n\n3\nNaN\n0.010206\n0.033026\n0.001091\n0.742373\n0.4\n\n\n4\nNaN\n0.008933\n0.030342\n0.000921\n0.782540\n0.5\n\n\n5\nNaN\n0.007981\n0.028454\n0.000810\n0.808759\n0.6\n\n\n6\nNaN\n0.007244\n0.027155\n0.000737\n0.825820\n0.7\n\n\n7\nNaN\n0.006675\n0.026304\n0.000692\n0.836575\n0.8\n\n\n8\nNaN\n0.006238\n0.025808\n0.000666\n0.842670\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol7_7[0], best_param)\nvol7_7_se = model.smooth()\n\nmetrics_vol7_7 = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'train τ = 7', vol7_7[0], vol7_7_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol7_7[1], best_param)\nvol7_7_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol7_7[2], best_param)\nvol7_7_se_test = model.smooth()\n\nmetrics_vol7_7_val = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'val τ = 7', vol7_7[1], vol7_7_se_val)\nmetrics_vol7_7_test = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'test τ = 7', vol7_7[2], vol7_7_se_test)\n\npd.concat([metrics_vol7_7, metrics_vol7_7_val, metrics_vol7_7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\nNaN\n0.006238\n0.025808\n0.000666\n0.842670\n5.398223e-219\n0.000000\n\n\nMAE val τ = 7\n38.582404\n0.004440\n0.005734\n0.000033\n-1.529547\n7.971377e-01\n0.133831\n\n\nMAE test τ = 7\n16.090127\n0.002291\n0.002913\n0.000008\n0.347288\n6.168363e-01\n0.873767\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol7_7[0][-500:], vol7_7[1], vol7_7[2], vol7_7_se, vol7_7_se_val, vol7_7_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol7_14[0], best_param)\nvol7_14_se = model.smooth()\n\nmetrics_vol7_14 = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'train τ = 14', vol7_14[0], vol7_14_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol7_14[1], best_param)\nvol7_14_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol7_14[2], best_param)\nvol7_14_se_test = model.smooth()\n\nmetrics_vol7_14_val = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'val τ = 14', vol7_14[1], vol7_14_se_val)\nmetrics_vol7_14_test = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'test τ = 14', vol7_14[2], vol7_14_se_test)\n\npd.concat([metrics_vol7_14, metrics_vol7_14_val, metrics_vol7_14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\nNaN\n0.006238\n0.025808\n0.000666\n0.842670\n5.398223e-219\n0.000000\n\n\nMAE val τ = 14\n28.684157\n0.003924\n0.004494\n0.000020\n-0.240202\n4.915045e-01\n0.598503\n\n\nMAE test τ = 14\n15.542838\n0.001834\n0.002845\n0.000008\n0.503033\n7.350284e-01\n0.000542\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol7_14[0][-500:], vol7_14[1], vol7_14[2], vol7_14_se, vol7_14_se_val, vol7_14_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol7_21[0], best_param)\nvol7_21_se = model.smooth()\n\nmetrics_vol7_21 = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'train τ = 21', vol7_21[0], vol7_21_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol7_21[1], best_param)\nvol7_21_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol7_21[2], best_param)\nvol7_21_se_test = model.smooth()\n\nmetrics_vol7_21_val = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'val τ = 21', vol7_21[1], vol7_21_se_val)\nmetrics_vol7_21_test = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'test τ = 21', vol7_21[2], vol7_21_se_test)\n\npd.concat([metrics_vol7_21, metrics_vol7_21_val, metrics_vol7_21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\nNaN\n0.006238\n0.025808\n0.000666\n0.842670\n5.398223e-219\n0.000000\n\n\nMAE val τ = 21\n65.284736\n0.019478\n0.022630\n0.000512\n-1.216606\n8.974552e-01\n0.179385\n\n\nMAE test τ = 21\n15.627563\n0.003870\n0.005863\n0.000034\n0.851225\n8.721455e-01\n0.556941\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol7_21[0][-500:], vol7_21[1], vol7_21[2], vol7_21_se, vol7_21_se_val, vol7_21_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol7_28[0], best_param)\nvol7_28_se = model.smooth()\n\nmetrics_vol7_28 = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'train τ = 28', vol7_28[0], vol7_28_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol7_28[1], best_param)\nvol7_28_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol7_28[2], best_param)\nvol7_28_se_test = model.smooth()\n\nmetrics_vol7_28_val = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'val τ = 28', vol7_28[1], vol7_28_se_val)\nmetrics_vol7_28_test = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'test τ = 28', vol7_28[2], vol7_28_se_test)\n\npd.concat([metrics_vol7_28, metrics_vol7_28_val, metrics_vol7_28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\nNaN\n0.006238\n0.025808\n0.000666\n0.842670\n5.398223e-219\n0.000000\n\n\nMAE val τ = 28\n56.158278\n0.025475\n0.029637\n0.000878\n-4.649790\n7.826934e-01\n0.033274\n\n\nMAE test τ = 28\n15.058436\n0.005253\n0.007240\n0.000052\n0.662862\n1.952917e-02\n0.686641\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol7_28[0][-500:], vol7_28[1], vol7_28[2], vol7_28_se, vol7_28_se_val, vol7_28_se_test)\nfig.show()\n\n\n\n\nmetrics_vol7 = pd.concat([\n    metrics_vol7_7, metrics_vol7_7_val, metrics_vol7_7_test,\n    metrics_vol7_14, metrics_vol7_14_val, metrics_vol7_14_test,\n    metrics_vol7_21, metrics_vol7_21_val, metrics_vol7_21_test,\n    metrics_vol7_28, metrics_vol7_28_val, metrics_vol7_28_test\n])\n\n\n\n\nVolatilidad \\(\\omega = 14\\)\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(vol14_7[0], param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\nNaN\n0.013117\n0.040219\n0.001618\n0.590116\n0.1\n\n\n1\nNaN\n0.009063\n0.030808\n0.000949\n0.759490\n0.2\n\n\n2\nNaN\n0.006962\n0.025391\n0.000645\n0.836636\n0.3\n\n\n3\nNaN\n0.005679\n0.022119\n0.000489\n0.876023\n0.4\n\n\n4\nNaN\n0.004826\n0.020053\n0.000402\n0.898106\n0.5\n\n\n5\nNaN\n0.004227\n0.018704\n0.000350\n0.911352\n0.6\n\n\n6\nNaN\n0.003787\n0.017814\n0.000317\n0.919583\n0.7\n\n\n7\nNaN\n0.003459\n0.017243\n0.000297\n0.924663\n0.8\n\n\n8\nNaN\n0.003213\n0.016910\n0.000286\n0.927538\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol14_7[0], best_param)\nvol14_7_se = model.smooth()\n\nmetrics_vol14_7 = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'train τ = 7', vol14_7[0], vol14_7_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol14_7[1], best_param)\nvol14_7_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol14_7[2], best_param)\nvol14_7_se_test = model.smooth()\n\nmetrics_vol14_7_val = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'val τ = 7', vol14_7[1], vol14_7_se_val)\nmetrics_vol14_7_test = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'test τ = 7', vol14_7[2], vol14_7_se_test)\n\npd.concat([metrics_vol14_7, metrics_vol14_7_val, metrics_vol14_7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\nNaN\n0.003213\n0.016910\n2.859626e-04\n0.927538\n3.661844e-46\n0.000000\n\n\nMAE val τ = 7\n31.829856\n0.005188\n0.005659\n3.202489e-05\n-113.279909\n7.904339e-01\n0.082897\n\n\nMAE test τ = 7\n2.433294\n0.000401\n0.000611\n3.730931e-07\n-0.331372\n9.197661e-01\n0.916042\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol14_7[0][-500:], vol14_7[1], vol14_7[2], vol14_7_se, vol14_7_se_val, vol14_7_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol14_14[0], best_param)\nvol14_14_se = model.smooth()\n\nmetrics_vol14_14 = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'train τ = 14', vol14_14[0], vol14_14_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol14_14[1], best_param)\nvol14_14_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol14_14[2], best_param)\nvol14_14_se_test = model.smooth()\n\nmetrics_vol14_14_val = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'val τ = 14', vol14_14[1], vol14_14_se_val)\nmetrics_vol14_14_test = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'test τ = 14', vol14_14[2], vol14_14_se_test)\n\npd.concat([metrics_vol14_14, metrics_vol14_14_val, metrics_vol14_14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\nNaN\n0.003213\n0.016910\n2.859626e-04\n0.927538\n3.661844e-46\n0.000000\n\n\nMAE val τ = 14\n17.921353\n0.003009\n0.003957\n1.566023e-05\n-47.451388\n6.316147e-01\n0.000001\n\n\nMAE test τ = 14\n2.844051\n0.000477\n0.000673\n4.523776e-07\n-0.399617\n9.775644e-02\n0.193811\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol14_14[0][-500:], vol14_14[1], vol14_14[2], vol14_14_se, vol14_14_se_val, vol14_14_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol14_21[0], best_param)\nvol14_21_se = model.smooth()\n\nmetrics_vol14_21 = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'train τ = 21', vol14_21[0], vol14_21_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol14_21[1], best_param)\nvol14_21_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol14_21[2], best_param)\nvol14_21_se_test = model.smooth()\n\nmetrics_vol14_21_val = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'val τ = 21', vol14_21[1], vol14_21_se_val)\nmetrics_vol14_21_test = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'test τ = 21', vol14_21[2], vol14_21_se_test)\n\npd.concat([metrics_vol14_21, metrics_vol14_21_val, metrics_vol14_21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\nNaN\n0.003213\n0.016910\n0.000286\n0.927538\n3.661844e-46\n0.000000e+00\n\n\nMAE val τ = 21\n42.080250\n0.013563\n0.016111\n0.000260\n-1.492731\n7.237061e-01\n3.961840e-18\n\n\nMAE test τ = 21\n5.559147\n0.001618\n0.002897\n0.000008\n0.919391\n9.538744e-02\n6.903126e-04\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol14_21[0][-500:], vol14_21[1], vol14_21[2], vol14_21_se, vol14_21_se_val, vol14_21_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol14_28[0], best_param)\nvol14_28_se = model.smooth()\n\nmetrics_vol14_28 = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'train τ = 28', vol14_28[0], vol14_28_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol14_28[1], best_param)\nvol14_28_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol14_28[2], best_param)\nvol14_28_se_test = model.smooth()\n\nmetrics_vol14_28_val = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'val τ = 28', vol14_28[1], vol14_28_se_val)\nmetrics_vol14_28_test = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'test τ = 28', vol14_28[2], vol14_28_se_test)\n\npd.concat([metrics_vol14_28, metrics_vol14_28_val, metrics_vol14_28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\nNaN\n0.003213\n0.016910\n0.000286\n0.927538\n3.661844e-46\n0.000000e+00\n\n\nMAE val τ = 28\n48.974542\n0.019458\n0.021177\n0.000448\n-8.941659\n7.769108e-01\n7.033859e-33\n\n\nMAE test τ = 28\n6.124939\n0.002218\n0.003413\n0.000012\n0.741739\n8.302782e-02\n4.251965e-01\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol14_28[0][-500:], vol14_28[1], vol14_28[2], vol14_28_se, vol14_28_se_val, vol14_28_se_test)\nfig.show()\n\n\n\n\nmetrics_vol14 = pd.concat([\n    metrics_vol14_7, metrics_vol14_7_val, metrics_vol14_7_test,\n    metrics_vol14_14, metrics_vol14_14_val, metrics_vol14_14_test,\n    metrics_vol14_21, metrics_vol14_21_val, metrics_vol14_21_test,\n    metrics_vol14_28, metrics_vol14_28_val, metrics_vol14_28_test\n])\n\n\n\n\nVolatilidad \\(\\omega = 21\\)\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(vol21_7[0], param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\nNaN\n0.010916\n0.034353\n0.001180\n0.689858\n0.1\n\n\n1\nNaN\n0.006902\n0.024920\n0.000621\n0.836804\n0.2\n\n\n2\nNaN\n0.005065\n0.020224\n0.000409\n0.892515\n0.3\n\n\n3\nNaN\n0.004044\n0.017568\n0.000309\n0.918895\n0.4\n\n\n4\nNaN\n0.003409\n0.015927\n0.000254\n0.933338\n0.5\n\n\n5\nNaN\n0.002965\n0.014864\n0.000221\n0.941938\n0.6\n\n\n6\nNaN\n0.002642\n0.014166\n0.000201\n0.947262\n0.7\n\n\n7\nNaN\n0.002399\n0.013719\n0.000188\n0.950536\n0.8\n\n\n8\nNaN\n0.002215\n0.013461\n0.000181\n0.952378\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol21_7[0], best_param)\nvol21_7_se = model.smooth()\n\nmetrics_vol21_7 = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'train τ = 7', vol21_7[0], vol21_7_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol21_7[1], best_param)\nvol21_7_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol21_7[2], best_param)\nvol21_7_se_test = model.smooth()\n\nmetrics_vol21_7_val = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'val τ = 7', vol21_7[1], vol21_7_se_val)\nmetrics_vol21_7_test = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'test τ = 7', vol21_7[2], vol21_7_se_test)\n\npd.concat([metrics_vol21_7, metrics_vol21_7_val, metrics_vol21_7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\nNaN\n0.002215\n0.013461\n1.812115e-04\n0.952378\n3.427794e-46\n0.000000\n\n\nMAE val τ = 7\n20.487936\n0.004030\n0.005142\n2.643715e-05\n-77.322445\n9.767082e-01\n0.063760\n\n\nMAE test τ = 7\n2.902855\n0.000574\n0.000807\n6.517640e-07\n-0.930910\n4.601006e-01\n0.973017\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol21_7[0][-500:], vol21_7[1], vol21_7[2], vol21_7_se, vol21_7_se_val, vol21_7_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol21_14[0], best_param)\nvol21_14_se = model.smooth()\n\nmetrics_vol21_14 = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'train τ = 14', vol21_14[0], vol21_14_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol21_14[1], best_param)\nvol21_14_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol21_14[2], best_param)\nvol21_14_se_test = model.smooth()\n\nmetrics_vol21_14_val = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'val τ = 14', vol21_14[1], vol21_14_se_val)\nmetrics_vol21_14_test = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'test τ = 14', vol21_14[2], vol21_14_se_test)\n\npd.concat([metrics_vol21_14, metrics_vol21_14_val, metrics_vol21_14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\nNaN\n0.002215\n0.013461\n1.812115e-04\n0.952378\n3.427794e-46\n0.000000e+00\n\n\nMAE val τ = 14\n29.520160\n0.004966\n0.005738\n3.292506e-05\n-73.852531\n9.018475e-01\n6.075171e-08\n\n\nMAE test τ = 14\n2.824281\n0.000473\n0.000665\n4.424062e-07\n-0.005776\n3.931241e-01\n3.397258e-01\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol21_14[0][-500:], vol21_14[1], vol21_14[2], vol21_14_se, vol21_14_se_val, vol21_14_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol21_21[0], best_param)\nvol21_21_se = model.smooth()\n\nmetrics_vol21_21 = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'train τ = 21', vol21_21[0], vol21_21_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol21_21[1], best_param)\nvol21_21_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol21_21[2], best_param)\nvol21_21_se_test = model.smooth()\n\nmetrics_vol21_21_val = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'val τ = 21', vol21_21[1], vol21_21_se_val)\nmetrics_vol21_21_test = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'test τ = 21', vol21_21[2], vol21_21_se_test)\n\npd.concat([metrics_vol21_21, metrics_vol21_21_val, metrics_vol21_21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\nNaN\n0.002215\n0.013461\n0.000181\n0.952378\n3.427794e-46\n0.000000e+00\n\n\nMAE val τ = 21\n38.079290\n0.010021\n0.011686\n0.000137\n-1.198899\n4.569950e-01\n5.726914e-07\n\n\nMAE test τ = 21\n4.595185\n0.001179\n0.002118\n0.000004\n0.927733\n1.123216e-01\n5.485603e-04\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol21_21[0][-500:], vol21_21[1], vol21_21[2], vol21_21_se, vol21_21_se_val, vol21_21_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol21_28[0], best_param)\nvol21_28_se = model.smooth()\n\nmetrics_vol21_28 = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'train τ = 28', vol21_28[0], vol21_28_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol21_28[1], best_param)\nvol21_28_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol21_28[2], best_param)\nvol21_28_se_test = model.smooth()\n\nmetrics_vol21_28_val = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'val τ = 28', vol21_28[1], vol21_28_se_val)\nmetrics_vol21_28_test = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'test τ = 28', vol21_28[2], vol21_28_se_test)\n\npd.concat([metrics_vol21_28, metrics_vol21_28_val, metrics_vol21_28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\nNaN\n0.002215\n0.013461\n0.000181\n0.952378\n3.427794e-46\n0.000000e+00\n\n\nMAE val τ = 28\n42.779302\n0.016402\n0.018792\n0.000353\n-5.889967\n5.193451e-01\n5.208990e-17\n\n\nMAE test τ = 28\n3.613422\n0.001193\n0.002050\n0.000004\n0.917982\n4.577967e-01\n7.541113e-05\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol21_28[0][-500:], vol21_28[1], vol21_28[2], vol21_28_se, vol21_28_se_val, vol21_28_se_test)\nfig.show()\n\n\n\n\nmetrics_vol21 = pd.concat([\n    metrics_vol21_7, metrics_vol21_7_val, metrics_vol21_7_test,\n    metrics_vol21_14, metrics_vol21_14_val, metrics_vol21_14_test,\n    metrics_vol21_21, metrics_vol21_21_val, metrics_vol21_21_test,\n    metrics_vol21_28, metrics_vol21_28_val, metrics_vol21_28_test\n])\n\n\n\n\nVolatilidad \\(\\omega = 28\\)\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(vol28_7[0], param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\nNaN\n0.009313\n0.030121\n0.000907\n0.754956\n0.1\n\n\n1\nNaN\n0.005598\n0.021227\n0.000451\n0.878301\n0.2\n\n\n2\nNaN\n0.004076\n0.017158\n0.000294\n0.920488\n0.3\n\n\n3\nNaN\n0.003240\n0.014906\n0.000222\n0.939987\n0.4\n\n\n4\nNaN\n0.002708\n0.013524\n0.000183\n0.950601\n0.5\n\n\n5\nNaN\n0.002343\n0.012632\n0.000160\n0.956901\n0.6\n\n\n6\nNaN\n0.002078\n0.012050\n0.000145\n0.960784\n0.7\n\n\n7\nNaN\n0.001880\n0.011680\n0.000136\n0.963152\n0.8\n\n\n8\nNaN\n0.001729\n0.011472\n0.000132\n0.964457\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol28_7[0], best_param)\nvol28_7_se = model.smooth()\n\nmetrics_vol28_7 = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'train τ = 7', vol28_7[0], vol28_7_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol28_7[1], best_param)\nvol28_7_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol28_7[2], best_param)\nvol28_7_se_test = model.smooth()\n\nmetrics_vol28_7_val = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'val τ = 7', vol28_7[1], vol28_7_se_val)\nmetrics_vol28_7_test = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'test τ = 7', vol28_7[2], vol28_7_se_test)\n\npd.concat([metrics_vol28_7, metrics_vol28_7_val, metrics_vol28_7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\nNaN\n0.001729\n0.011472\n0.000132\n0.964457\n2.080088e-42\n0.000000\n\n\nMAE val τ = 7\n22.776284\n0.004897\n0.005409\n0.000029\n-9.049319\n4.853045e-01\n0.363816\n\n\nMAE test τ = 7\n4.049157\n0.000847\n0.001453\n0.000002\n0.274886\n9.865148e-01\n0.180878\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol28_7[0][-500:], vol28_7[1], vol28_7[2], vol28_7_se, vol28_7_se_val, vol28_7_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol28_14[0], best_param)\nvol28_14_se = model.smooth()\n\nmetrics_vol28_14 = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'train τ = 14', vol28_14[0], vol28_14_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol28_14[1], best_param)\nvol28_14_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol28_14[2], best_param)\nvol28_14_se_test = model.smooth()\n\nmetrics_vol28_14_val = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'val τ = 14', vol28_14[1], vol28_14_se_val)\nmetrics_vol28_14_test = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'test τ = 14', vol28_14[2], vol28_14_se_test)\n\npd.concat([metrics_vol28_14, metrics_vol28_14_val, metrics_vol28_14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\nNaN\n0.001729\n0.011472\n0.000132\n0.964457\n2.080088e-42\n0.000000\n\n\nMAE val τ = 14\n36.666901\n0.006637\n0.007027\n0.000049\n-12.781781\n3.199223e-01\n0.055452\n\n\nMAE test τ = 14\n3.334943\n0.000588\n0.001032\n0.000001\n0.702910\n1.836634e-01\n0.006907\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol28_14[0][-500:], vol28_14[1], vol28_14[2], vol28_14_se, vol28_14_se_val, vol28_14_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol28_21[0], best_param)\nvol28_21_se = model.smooth()\n\nmetrics_vol28_21 = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'train τ = 21', vol28_21[0], vol28_21_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol28_21[1], best_param)\nvol28_21_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol28_21[2], best_param)\nvol28_21_se_test = model.smooth()\n\nmetrics_vol28_21_val = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'val τ = 21', vol28_21[1], vol28_21_se_val)\nmetrics_vol28_21_test = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'test τ = 21', vol28_21[2], vol28_21_se_test)\n\npd.concat([metrics_vol28_21, metrics_vol28_21_val, metrics_vol28_21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\nNaN\n0.001729\n0.011472\n0.000132\n0.964457\n2.080088e-42\n0.000000\n\n\nMAE val τ = 21\n37.235382\n0.008127\n0.009179\n0.000084\n-1.115749\n1.548663e-01\n0.000681\n\n\nMAE test τ = 21\n4.143860\n0.000994\n0.001802\n0.000003\n0.918428\n1.884342e-01\n0.000639\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol28_21[0][-500:], vol28_21[1], vol28_21[2], vol28_21_se, vol28_21_se_val, vol28_21_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol28_28[0], best_param)\nvol28_28_se = model.smooth()\n\nmetrics_vol28_28 = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'train τ = 28', vol28_28[0], vol28_28_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol28_28[1], best_param)\nvol28_28_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol28_28[2], best_param)\nvol28_28_se_test = model.smooth()\n\nmetrics_vol28_28_val = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'val τ = 28', vol28_28[1], vol28_28_se_val)\nmetrics_vol28_28_test = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'test τ = 28', vol28_28[2], vol28_28_se_test)\n\npd.concat([metrics_vol28_28, metrics_vol28_28_val, metrics_vol28_28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\nNaN\n0.001729\n0.011472\n0.000132\n0.964457\n2.080088e-42\n0.000000\n\n\nMAE val τ = 28\n34.594749\n0.012416\n0.015259\n0.000233\n-3.315288\n9.806432e-03\n0.000266\n\n\nMAE test τ = 28\n3.738236\n0.001167\n0.001900\n0.000004\n0.933102\n1.810440e-01\n0.003661\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol28_28[0][-500:], vol28_28[1], vol28_28[2], vol28_28_se, vol28_28_se_val, vol28_28_se_test)\nfig.show()\n\n\n\n\nmetrics_vol28 = pd.concat([\n    metrics_vol28_7, metrics_vol28_7_val, metrics_vol28_7_test,\n    metrics_vol28_14, metrics_vol28_14_val, metrics_vol28_14_test,\n    metrics_vol28_21, metrics_vol28_21_val, metrics_vol28_21_test,\n    metrics_vol28_28, metrics_vol28_28_val, metrics_vol28_28_test\n])\n\n\n\n\n\nCaso doble sin librería\n\nPrice\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(price7_se, param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\n8.045131\n669.388624\n1529.357614\n2.338935e+06\n0.989828\n0.1\n\n\n1\n5.536273\n456.978816\n1078.011744\n1.162109e+06\n0.994946\n0.2\n\n\n2\n4.475046\n374.166563\n888.816295\n7.899944e+05\n0.996564\n0.3\n\n\n3\n3.877030\n326.568147\n781.476255\n6.107051e+05\n0.997344\n0.4\n\n\n4\n3.489181\n294.898951\n712.982225\n5.083437e+05\n0.997789\n0.5\n\n\n5\n3.219411\n271.905114\n666.763782\n4.445739e+05\n0.998067\n0.6\n\n\n6\n3.027411\n255.515647\n634.968296\n4.031847e+05\n0.998247\n0.7\n\n\n7\n2.890954\n243.801900\n613.422621\n3.762873e+05\n0.998364\n0.8\n\n\n8\n2.793634\n236.131931\n599.793220\n3.597519e+05\n0.998435\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(price7_se, best_param)\nprice7_se2 = model.smooth()\n\nmetrics_price7 = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'train τ = 7', price7_se, price7_se2, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(price7_se_val, best_param)\nprice7_se_val2 = model.smooth()\n\nmodel = ExponentialSmoothing(price7_se_test, best_param)\nprice7_se_test2 = model.smooth()\n\nmetrics_price7_val_ = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'val τ = 7', price7_se_val, price7_se_val2)\nmetrics_price7_test = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'test τ = 7', price7_se_test, price7_se_test2)\n\npd.concat([metrics_price7, metrics_price7_val, metrics_price7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\n2.793634\n236.131931\n599.793220\n3.597519e+05\n0.998435\n3.340594e-36\n0.000000\n\n\nMAE val τ = 7\n5.515992\n2595.563591\n3259.226089\n1.062255e+07\n-1.436019\n3.537907e-01\n0.713997\n\n\nMAE test τ = 7\n1.880853\n866.113474\n1032.913565\n1.066910e+06\n0.749018\n2.900096e-01\n0.914023\n\n\n\n\n\n\n\n\nfig = model.plot_preds(price7_se[-500:], price7_se_val, price7_se_test, price7_se2, price7_se_val2, price7_se_test2)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(price14_se, best_param)\nprice14_se2 = model.smooth()\n\nmetrics_price14 = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'train τ = 14', price14_se, price14_se2, show_info=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(price14_se_val, best_param)\nprice14_se_val2 = model.smooth()\n\nmodel = ExponentialSmoothing(price14_se_test, best_param)\nprice14_se_test2 = model.smooth()\n\nmetrics_price14_val = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'val τ = 14', price14_se_val, price14_se_val2)\nmetrics_price14_test = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'test τ = 14', price14_se_test, price14_se_test2)\n\npd.concat([metrics_price14, metrics_price14_val, metrics_price14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\n2.793634\n236.131931\n599.793220\n3.597519e+05\n0.998435\n3.340594e-36\n0.000000\n\n\nMAE val τ = 14\n13.879273\n7166.514862\n7633.896707\n5.827638e+07\n-105.162620\n4.163556e-03\n0.550624\n\n\nMAE test τ = 14\n0.897345\n462.661103\n618.859887\n3.829876e+05\n0.302308\n2.892078e-01\n0.066354\n\n\n\n\n\n\n\n\nfig = model.plot_preds(price14_se[-500:], price14_se_val, price14_se_test, price14_se2, price14_se_val2, price14_se_test2)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(price21_se, best_param)\nprice21_se2 = model.smooth()\nmetrics_price21 = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'train τ = 21', price21_se, price21_se2, show_info=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(price21_se_val, best_param)\nprice21_se_val2 = model.smooth()\n\nmodel = ExponentialSmoothing(price21_se_test, best_param)\nprice21_se_test2 = model.smooth()\n\nmetrics_price21_val = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'val τ = 21', price21_se_val, price21_se_val2)\nmetrics_price21_test = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'test τ = 21', price21_se_test, price21_se_test2)\n\npd.concat([metrics_price21, metrics_price21_val, metrics_price21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\n2.793634\n236.131931\n599.793220\n3.597519e+05\n0.998435\n3.340594e-36\n0.00000\n\n\nMAE val τ = 21\n21.552227\n13155.722231\n13983.888897\n1.955491e+08\n-3.416158\n3.472683e-03\n0.38113\n\n\nMAE test τ = 21\n2.214729\n1359.353803\n2014.121606\n4.056686e+06\n0.908386\n8.888633e-01\n0.20822\n\n\n\n\n\n\n\n\nfig = model.plot_preds(price21_se[-500:], price21_se_val, price21_se_test, price21_se2, price21_se_val2, price21_se_test2)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(price28_se, best_param)\nprice28_se2 = model.smooth()\nmetrics_price28 = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'train τ = 28', price28_se, price28_se2, show_info=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(price28_se_val, best_param)\nprice28_se_val2 = model.smooth()\n\nmodel = ExponentialSmoothing(price28_se_test, best_param)\nprice28_se_test2 = model.smooth()\n\nmetrics_price28_val = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'val τ = 28', price28_se_val, price28_se_val2)\nmetrics_price28_test = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'test τ = 28', price28_se_test, price28_se_test2)\n\npd.concat([metrics_price28, metrics_price28_val, metrics_price28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\n2.793634\n236.131931\n599.793220\n3.597519e+05\n0.998435\n3.340594e-36\n0.000000\n\n\nMAE val τ = 28\n26.935401\n17869.336011\n18604.732800\n3.461361e+08\n-18.263227\n7.163089e-05\n0.235948\n\n\nMAE test τ = 28\n2.826882\n1854.230239\n2444.620007\n5.976167e+06\n0.667413\n8.427913e-01\n0.936190\n\n\n\n\n\n\n\n\nfig = model.plot_preds(price28_se[-500:], price28_se_val, price28_se_test, price28_se2, price28_se_val2, price28_se_test2)\nfig.show()\n\n\n\n\nmetrics_price_d = pd.concat([\n    metrics_price7, metrics_price7_val, metrics_price7_test,\n    metrics_price14, metrics_price14_val, metrics_price14_test,\n    metrics_price21, metrics_price21_val, metrics_price21_test,\n    metrics_price28, metrics_price28_val, metrics_price28_test\n])\n\n\n\n\nRetorno acumulado\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(A_t7[0], param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\nNaN\n0.093896\n0.183250\n0.033581\n0.999102\n0.1\n\n\n1\nNaN\n0.061176\n0.127675\n0.016301\n0.999564\n0.2\n\n\n2\nNaN\n0.048750\n0.105511\n0.011132\n0.999702\n0.3\n\n\n3\nNaN\n0.042050\n0.093687\n0.008777\n0.999765\n0.4\n\n\n4\nNaN\n0.037878\n0.086676\n0.007513\n0.999799\n0.5\n\n\n5\nNaN\n0.035097\n0.082335\n0.006779\n0.999819\n0.6\n\n\n6\nNaN\n0.033219\n0.079635\n0.006342\n0.999830\n0.7\n\n\n7\nNaN\n0.031947\n0.078023\n0.006088\n0.999837\n0.8\n\n\n8\nNaN\n0.031174\n0.077196\n0.005959\n0.999841\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(A_t7_se, best_param)\nA_t7_se2 = model.smooth()\n\nmetrics_A_t7 = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 7', 'MAE', 'train τ = 7', A_t7_se, A_t7_se2, show_info=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(A_t7_se_val, best_param)\nA_t7_se_val2 = model.smooth()\n\nmodel = ExponentialSmoothing(A_t7_se_test, best_param)\nA_t7_se_test2 = model.smooth()\n\nmetrics_A_t7_val = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 7', 'MAE', 'val τ = 7', A_t7_se_val, A_t7_se_val2)\nmetrics_A_t7_test = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 7', 'MAE', 'test τ = 7', A_t7_se_test, A_t7_se_test2)\n\npd.concat([metrics_A_t7, metrics_A_t7_val, metrics_A_t7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\nNaN\n0.028736\n0.070272\n0.004938\n0.999868\n1.629383e-71\n0.000000\n\n\nMAE val τ = 7\n0.243336\n0.056755\n0.071366\n0.005093\n-1.400565\n2.949738e-01\n0.726934\n\n\nMAE test τ = 7\n0.083004\n0.019342\n0.023081\n0.000533\n0.748906\n2.762726e-01\n0.897414\n\n\n\n\n\n\n\n\nfig = model.plot_preds(A_t7_se[-500:], A_t7_se_val, A_t7_se_test, A_t7_se2, A_t7_se_val2, A_t7_se_test2)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(A_t14_se, best_param)\nA_t14_se2 = model.smooth()\n\nmetrics_A_t14 = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 14', 'MAE', 'train τ = 14', A_t14_se, A_t14_se2, show_info=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(A_t14_se_val, best_param)\nA_t14_se_val2 = model.smooth()\n\nmodel = ExponentialSmoothing(A_t14_se_test, best_param)\nA_t14_se_test2 = model.smooth()\n\nmetrics_A_t14_val = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 14', 'MAE', 'val τ = 14', A_t14_se_val, A_t14_se_val2)\nmetrics_A_t14_test = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 14', 'MAE', 'test τ = 14', A_t14_se_test, A_t14_se_test2)\n\npd.concat([metrics_A_t14, metrics_A_t14_val, metrics_A_t14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\nNaN\n0.028736\n0.070272\n0.004938\n0.999868\n1.629383e-71\n0.000000\n\n\nMAE val τ = 14\n0.656525\n0.153759\n0.164007\n0.026898\n-121.349253\n5.357212e-03\n0.566548\n\n\nMAE test τ = 14\n0.038828\n0.009093\n0.012303\n0.000151\n0.311457\n3.092052e-01\n0.044403\n\n\n\n\n\n\n\n\nfig = model.plot_preds(A_t14_se[-500:], A_t14_se_val, A_t14_se_test, A_t14_se2, A_t14_se_val2, A_t14_se_test2)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(A_t21_se, best_param)\nA_t21_se2 = model.smooth()\n\nmetrics_A_t21 = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 21', 'MAE', 'train τ = 21', A_t21_se, A_t21_se2, show_info=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(A_t21_se_val, best_param)\nA_t21_se_val2 = model.smooth()\nmodel = ExponentialSmoothing(A_t21_se_test, best_param)\nA_t21_se_test2 = model.smooth()\nmetrics_A_t21_val = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 21', 'MAE', 'val τ = 21', A_t21_se_val, A_t21_se_val2)\nmetrics_A_t21_test = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 21', 'MAE', 'test τ = 21', A_t21_se_test, A_t21_se_test2)\n\npd.concat([metrics_A_t21, metrics_A_t21_val, metrics_A_t21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\nNaN\n0.028736\n0.070272\n0.004938\n0.999868\n1.629383e-71\n0.000000\n\n\nMAE val τ = 21\n1.077922\n0.254305\n0.267282\n0.071440\n-4.117570\n1.966422e-03\n0.404310\n\n\nMAE test τ = 21\n0.097026\n0.022894\n0.034064\n0.001160\n0.916878\n8.745393e-01\n0.139848\n\n\n\n\n\n\n\n\nfig = model.plot_preds(A_t21_se[-500:], A_t21_se_val, A_t21_se_test, A_t21_se2, A_t21_se_val2, A_t21_se_test2)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(A_t28_se, best_param)\nA_t28_se2 = model.smooth()\nmetrics_A_t28 = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 28', 'MAE', 'train τ = 28', A_t28_se, A_t28_se2, show_info=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(A_t28_se_val, best_param)\nA_t28_se_val2 = model.smooth()\nmodel = ExponentialSmoothing(A_t28_se_test, best_param)\nA_t28_se_test2 = model.smooth()\nmetrics_A_t28_val = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 28', 'MAE', 'val τ = 28', A_t28_se_val, A_t28_se_val2)\nmetrics_A_t28_test = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 28', 'MAE', 'test τ = 28', A_t28_se_test, A_t28_se_test2)\n\npd.concat([metrics_A_t28, metrics_A_t28_val, metrics_A_t28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\nNaN\n0.028736\n0.070272\n0.004938\n0.999868\n1.629383e-71\n0.000000\n\n\nMAE val τ = 28\n1.414642\n0.335046\n0.348306\n0.121317\n-23.919565\n4.212720e-05\n0.219773\n\n\nMAE test τ = 28\n0.121057\n0.028659\n0.038435\n0.001477\n0.696563\n8.993733e-01\n0.975459\n\n\n\n\n\n\n\n\nfig = model.plot_preds(A_t28_se[-500:], A_t28_se_val, A_t28_se_test, A_t28_se2, A_t28_se_val2, A_t28_se_test2)\nfig.show()\n\n\n\n\nmetrics_A_t_d = pd.concat([\n    metrics_A_t7, metrics_A_t7_val, metrics_A_t7_test,\n    metrics_A_t14, metrics_A_t14_val, metrics_A_t14_test,\n    metrics_A_t21, metrics_A_t21_val, metrics_A_t21_test,\n    metrics_A_t28, metrics_A_t28_val, metrics_A_t28_test\n])\n\n\n\n\nVolatilidad \\(\\omega = 7\\)\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(vol7_7_se, param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\nNaN\n0.017234\n0.048916\n0.002393\n0.425776\n0.1\n\n\n1\nNaN\n0.014061\n0.041230\n0.001700\n0.592065\n0.2\n\n\n2\nNaN\n0.011750\n0.035770\n0.001279\n0.692948\n0.3\n\n\n3\nNaN\n0.010017\n0.031828\n0.001013\n0.756894\n0.4\n\n\n4\nNaN\n0.008745\n0.028969\n0.000839\n0.798603\n0.5\n\n\n5\nNaN\n0.007781\n0.026898\n0.000724\n0.826372\n0.6\n\n\n6\nNaN\n0.007036\n0.025408\n0.000646\n0.845080\n0.7\n\n\n7\nNaN\n0.006448\n0.024355\n0.000593\n0.857648\n0.8\n\n\n8\nNaN\n0.005980\n0.023647\n0.000559\n0.865810\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol7_7_se, best_param)\nvol7_7_se2 = model.smooth()\nmetrics_vol7_7 = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 7', 'MAE', 'train τ = 7', vol7_7_se, vol7_7_se2, show_info=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol7_7_se_val, best_param)\nvol7_7_se_val2 = model.smooth()\nmodel = ExponentialSmoothing(vol7_7_se_test, best_param)\nvol7_7_se_test2 = model.smooth()\nmetrics_vol7_7_val = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 7', 'MAE', 'val τ = 7', vol7_7_se_val, vol7_7_se_val2)\nmetrics_vol7_7_test = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 7', 'MAE', 'test τ = 7', vol7_7_se_test, vol7_7_se_test2)\n\npd.concat([metrics_vol7_7, metrics_vol7_7_val, metrics_vol7_7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\nNaN\n0.005980\n0.023647\n0.000559\n0.865810\n1.301410e-268\n0.000000\n\n\nMAE val τ = 7\n39.131154\n0.004442\n0.005800\n0.000034\n-1.681537\n7.546915e-01\n0.182257\n\n\nMAE test τ = 7\n14.913519\n0.002094\n0.002700\n0.000007\n0.418664\n5.304739e-01\n0.880066\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol7_7_se[-500:], vol7_7_se_val, vol7_7_se_test, vol7_7_se2, vol7_7_se_val2, vol7_7_se_test2)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol7_14_se, best_param)\nvol7_14_se2 = model.smooth()\nmetrics_vol7_14 = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 14', 'MAE', 'train τ = 14', vol7_14_se, vol7_14_se2, show_info=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol7_14_se_val, best_param)\nvol7_14_se_val2 = model.smooth()\nmodel = ExponentialSmoothing(vol7_14_se_test, best_param)\nvol7_14_se_test2 = model.smooth()\nmetrics_vol7_14_val = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 14', 'MAE', 'val τ = 14', vol7_14_se_val, vol7_14_se_val2)\nmetrics_vol7_14_test = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 14', 'MAE', 'test τ = 14', vol7_14_se_test, vol7_14_se_test2)\n\npd.concat([metrics_vol7_14, metrics_vol7_14_val, metrics_vol7_14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\nNaN\n0.005980\n0.023647\n0.000559\n0.865810\n1.301410e-268\n0.000000\n\n\nMAE val τ = 14\n27.809711\n0.003799\n0.004325\n0.000019\n-0.214463\n4.295016e-01\n0.652980\n\n\nMAE test τ = 14\n14.072095\n0.001731\n0.002604\n0.000007\n0.559776\n6.193442e-01\n0.000996\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol7_14_se[-500:], vol7_14_se_val, vol7_14_se_test, vol7_14_se2, vol7_14_se_val2, vol7_14_se_test2)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol7_21_se, best_param)\nvol7_21_se2 = model.smooth()\nmetrics_vol7_21 = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 21', 'MAE', 'train τ = 21', vol7_21_se, vol7_21_se2, show_info=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol7_21_se_val, best_param)\nvol7_21_se_val2 = model.smooth()\nmodel = ExponentialSmoothing(vol7_21_se_test, best_param)\nvol7_21_se_test2 = model.smooth()\nmetrics_vol7_21_val = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 21', 'MAE', 'val τ = 21', vol7_21_se_val, vol7_21_se_val2)\nmetrics_vol7_21_test = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 21', 'MAE', 'test τ = 21', vol7_21_se_test, vol7_21_se_test2)\n\npd.concat([metrics_vol7_21, metrics_vol7_21_val, metrics_vol7_21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\nNaN\n0.005980\n0.023647\n0.000559\n0.865810\n1.301410e-268\n0.000000\n\n\nMAE val τ = 21\n64.193928\n0.019320\n0.022442\n0.000504\n-1.211611\n8.771825e-01\n0.232819\n\n\nMAE test τ = 21\n14.464250\n0.003616\n0.005397\n0.000029\n0.872078\n7.713427e-01\n0.456959\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol7_21_se[-500:], vol7_21_se_val, vol7_21_se_test, vol7_21_se2, vol7_21_se_val2, vol7_21_se_test2)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol7_28_se, best_param)\nvol7_28_se2 = model.smooth()\nmetrics_vol7_28 = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 28', 'MAE', 'train τ = 28', vol7_28_se, vol7_28_se2, show_info=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol7_28_se_val, best_param)\nvol7_28_se_val2 = model.smooth()\nmodel = ExponentialSmoothing(vol7_28_se_test, best_param)\nvol7_28_se_test2 = model.smooth()\nmetrics_vol7_28_val = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 28', 'MAE', 'val τ = 28', vol7_28_se_val, vol7_28_se_val2)\nmetrics_vol7_28_test = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 28', 'MAE', 'test τ = 28', vol7_28_se_test, vol7_28_se_test2)\n\npd.concat([metrics_vol7_28, metrics_vol7_28_val, metrics_vol7_28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\nNaN\n0.005980\n0.023647\n0.000559\n0.865810\n1.301410e-268\n0.000000\n\n\nMAE val τ = 28\n56.137852\n0.025307\n0.029381\n0.000863\n-4.741277\n6.406643e-01\n0.055422\n\n\nMAE test τ = 28\n14.127762\n0.004961\n0.006745\n0.000045\n0.697439\n9.614044e-03\n0.631850\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol7_28_se[-500:], vol7_28_se_val, vol7_28_se_test, vol7_28_se2, vol7_28_se_val2, vol7_28_se_test2)\nfig.show()\n\n\n\n\nmetrics_vol7_d = pd.concat([\n    metrics_vol7_7, metrics_vol7_7_val, metrics_vol7_7_test,\n    metrics_vol7_14, metrics_vol7_14_val, metrics_vol7_14_test,\n    metrics_vol7_21, metrics_vol7_21_val, metrics_vol7_21_test,\n    metrics_vol7_28, metrics_vol7_28_val, metrics_vol7_28_test\n])\n\n\n\n\nVolatilidad \\(\\omega = 14\\)\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(vol14_7[0], param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\nNaN\n0.013117\n0.040219\n0.001618\n0.590116\n0.1\n\n\n1\nNaN\n0.009063\n0.030808\n0.000949\n0.759490\n0.2\n\n\n2\nNaN\n0.006962\n0.025391\n0.000645\n0.836636\n0.3\n\n\n3\nNaN\n0.005679\n0.022119\n0.000489\n0.876023\n0.4\n\n\n4\nNaN\n0.004826\n0.020053\n0.000402\n0.898106\n0.5\n\n\n5\nNaN\n0.004227\n0.018704\n0.000350\n0.911352\n0.6\n\n\n6\nNaN\n0.003787\n0.017814\n0.000317\n0.919583\n0.7\n\n\n7\nNaN\n0.003459\n0.017243\n0.000297\n0.924663\n0.8\n\n\n8\nNaN\n0.003213\n0.016910\n0.000286\n0.927538\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol14_7[0], best_param)\nvol14_7_se = model.smooth()\n\nmetrics_vol14_7 = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'train τ = 7', vol14_7[0], vol14_7_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol14_7[1], best_param)\nvol14_7_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol14_7[2], best_param)\nvol14_7_se_test = model.smooth()\n\nmetrics_vol14_7_val = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'val τ = 7', vol14_7[1], vol14_7_se_val)\nmetrics_vol14_7_test = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'test τ = 7', vol14_7[2], vol14_7_se_test)\n\npd.concat([metrics_vol14_7, metrics_vol14_7_val, metrics_vol14_7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\nNaN\n0.003213\n0.016910\n2.859626e-04\n0.927538\n3.661844e-46\n0.000000\n\n\nMAE val τ = 7\n31.829856\n0.005188\n0.005659\n3.202489e-05\n-113.279909\n7.904339e-01\n0.082897\n\n\nMAE test τ = 7\n2.433294\n0.000401\n0.000611\n3.730931e-07\n-0.331372\n9.197661e-01\n0.916042\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol14_7[0][-500:], vol14_7[1], vol14_7[2], vol14_7_se, vol14_7_se_val, vol14_7_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol14_14[0], best_param)\nvol14_14_se = model.smooth()\n\nmetrics_vol14_14 = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'train τ = 14', vol14_14[0], vol14_14_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol14_14[1], best_param)\nvol14_14_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol14_14[2], best_param)\nvol14_14_se_test = model.smooth()\n\nmetrics_vol14_14_val = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'val τ = 14', vol14_14[1], vol14_14_se_val)\nmetrics_vol14_14_test = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'test τ = 14', vol14_14[2], vol14_14_se_test)\n\npd.concat([metrics_vol14_14, metrics_vol14_14_val, metrics_vol14_14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\nNaN\n0.003213\n0.016910\n2.859626e-04\n0.927538\n3.661844e-46\n0.000000\n\n\nMAE val τ = 14\n17.921353\n0.003009\n0.003957\n1.566023e-05\n-47.451388\n6.316147e-01\n0.000001\n\n\nMAE test τ = 14\n2.844051\n0.000477\n0.000673\n4.523776e-07\n-0.399617\n9.775644e-02\n0.193811\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol14_14[0][-500:], vol14_14[1], vol14_14[2], vol14_14_se, vol14_14_se_val, vol14_14_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol14_21[0], best_param)\nvol14_21_se = model.smooth()\n\nmetrics_vol14_21 = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'train τ = 21', vol14_21[0], vol14_21_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol14_21[1], best_param)\nvol14_21_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol14_21[2], best_param)\nvol14_21_se_test = model.smooth()\n\nmetrics_vol14_21_val = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'val τ = 21', vol14_21[1], vol14_21_se_val)\nmetrics_vol14_21_test = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'test τ = 21', vol14_21[2], vol14_21_se_test)\n\npd.concat([metrics_vol14_21, metrics_vol14_21_val, metrics_vol14_21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\nNaN\n0.003213\n0.016910\n0.000286\n0.927538\n3.661844e-46\n0.000000e+00\n\n\nMAE val τ = 21\n42.080250\n0.013563\n0.016111\n0.000260\n-1.492731\n7.237061e-01\n3.961840e-18\n\n\nMAE test τ = 21\n5.559147\n0.001618\n0.002897\n0.000008\n0.919391\n9.538744e-02\n6.903126e-04\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol14_21[0][-500:], vol14_21[1], vol14_21[2], vol14_21_se, vol14_21_se_val, vol14_21_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol14_28[0], best_param)\nvol14_28_se = model.smooth()\n\nmetrics_vol14_28 = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'train τ = 28', vol14_28[0], vol14_28_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol14_28[1], best_param)\nvol14_28_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol14_28[2], best_param)\nvol14_28_se_test = model.smooth()\n\nmetrics_vol14_28_val = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'val τ = 28', vol14_28[1], vol14_28_se_val)\nmetrics_vol14_28_test = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'test τ = 28', vol14_28[2], vol14_28_se_test)\n\npd.concat([metrics_vol14_28, metrics_vol14_28_val, metrics_vol14_28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\nNaN\n0.003213\n0.016910\n0.000286\n0.927538\n3.661844e-46\n0.000000e+00\n\n\nMAE val τ = 28\n48.974542\n0.019458\n0.021177\n0.000448\n-8.941659\n7.769108e-01\n7.033859e-33\n\n\nMAE test τ = 28\n6.124939\n0.002218\n0.003413\n0.000012\n0.741739\n8.302782e-02\n4.251965e-01\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol14_28[0][-500:], vol14_28[1], vol14_28[2], vol14_28_se, vol14_28_se_val, vol14_28_se_test)\nfig.show()\n\n\n\n\nmetrics_vol14_d = pd.concat([\n    metrics_vol14_7, metrics_vol14_7_val, metrics_vol14_7_test,\n    metrics_vol14_14, metrics_vol14_14_val, metrics_vol14_14_test,\n    metrics_vol14_21, metrics_vol14_21_val, metrics_vol14_21_test,\n    metrics_vol14_28, metrics_vol14_28_val, metrics_vol14_28_test\n])\n\n\n\n\nVolatilidad \\(\\omega = 21\\)\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(vol21_7[0], param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\nNaN\n0.010916\n0.034353\n0.001180\n0.689858\n0.1\n\n\n1\nNaN\n0.006902\n0.024920\n0.000621\n0.836804\n0.2\n\n\n2\nNaN\n0.005065\n0.020224\n0.000409\n0.892515\n0.3\n\n\n3\nNaN\n0.004044\n0.017568\n0.000309\n0.918895\n0.4\n\n\n4\nNaN\n0.003409\n0.015927\n0.000254\n0.933338\n0.5\n\n\n5\nNaN\n0.002965\n0.014864\n0.000221\n0.941938\n0.6\n\n\n6\nNaN\n0.002642\n0.014166\n0.000201\n0.947262\n0.7\n\n\n7\nNaN\n0.002399\n0.013719\n0.000188\n0.950536\n0.8\n\n\n8\nNaN\n0.002215\n0.013461\n0.000181\n0.952378\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol21_7[0], best_param)\nvol21_7_se = model.smooth()\n\nmetrics_vol21_7 = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'train τ = 7', vol21_7[0], vol21_7_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol21_7[1], best_param)\nvol21_7_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol21_7[2], best_param)\nvol21_7_se_test = model.smooth()\n\nmetrics_vol21_7_val = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'val τ = 7', vol21_7[1], vol21_7_se_val)\nmetrics_vol21_7_test = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'test τ = 7', vol21_7[2], vol21_7_se_test)\n\npd.concat([metrics_vol21_7, metrics_vol21_7_val, metrics_vol21_7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\nNaN\n0.002215\n0.013461\n1.812115e-04\n0.952378\n3.427794e-46\n0.000000\n\n\nMAE val τ = 7\n20.487936\n0.004030\n0.005142\n2.643715e-05\n-77.322445\n9.767082e-01\n0.063760\n\n\nMAE test τ = 7\n2.902855\n0.000574\n0.000807\n6.517640e-07\n-0.930910\n4.601006e-01\n0.973017\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol21_7[0][-500:], vol21_7[1], vol21_7[2], vol21_7_se, vol21_7_se_val, vol21_7_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol21_14[0], best_param)\nvol21_14_se = model.smooth()\n\nmetrics_vol21_14 = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'train τ = 14', vol21_14[0], vol21_14_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol21_14[1], best_param)\nvol21_14_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol21_14[2], best_param)\nvol21_14_se_test = model.smooth()\n\nmetrics_vol21_14_val = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'val τ = 14', vol21_14[1], vol21_14_se_val)\nmetrics_vol21_14_test = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'test τ = 14', vol21_14[2], vol21_14_se_test)\n\npd.concat([metrics_vol21_14, metrics_vol21_14_val, metrics_vol21_14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\nNaN\n0.002215\n0.013461\n1.812115e-04\n0.952378\n3.427794e-46\n0.000000e+00\n\n\nMAE val τ = 14\n29.520160\n0.004966\n0.005738\n3.292506e-05\n-73.852531\n9.018475e-01\n6.075171e-08\n\n\nMAE test τ = 14\n2.824281\n0.000473\n0.000665\n4.424062e-07\n-0.005776\n3.931241e-01\n3.397258e-01\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol21_14[0][-500:], vol21_14[1], vol21_14[2], vol21_14_se, vol21_14_se_val, vol21_14_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol21_21[0], best_param)\nvol21_21_se = model.smooth()\n\nmetrics_vol21_21 = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'train τ = 21', vol21_21[0], vol21_21_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol21_21[1], best_param)\nvol21_21_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol21_21[2], best_param)\nvol21_21_se_test = model.smooth()\n\nmetrics_vol21_21_val = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'val τ = 21', vol21_21[1], vol21_21_se_val)\nmetrics_vol21_21_test = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'test τ = 21', vol21_21[2], vol21_21_se_test)\n\npd.concat([metrics_vol21_21, metrics_vol21_21_val, metrics_vol21_21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\nNaN\n0.002215\n0.013461\n0.000181\n0.952378\n3.427794e-46\n0.000000e+00\n\n\nMAE val τ = 21\n38.079290\n0.010021\n0.011686\n0.000137\n-1.198899\n4.569950e-01\n5.726914e-07\n\n\nMAE test τ = 21\n4.595185\n0.001179\n0.002118\n0.000004\n0.927733\n1.123216e-01\n5.485603e-04\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol21_21[0][-500:], vol21_21[1], vol21_21[2], vol21_21_se, vol21_21_se_val, vol21_21_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol21_28[0], best_param)\nvol21_28_se = model.smooth()\n\nmetrics_vol21_28 = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'train τ = 28', vol21_28[0], vol21_28_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol21_28[1], best_param)\nvol21_28_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol21_28[2], best_param)\nvol21_28_se_test = model.smooth()\n\nmetrics_vol21_28_val = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'val τ = 28', vol21_28[1], vol21_28_se_val)\nmetrics_vol21_28_test = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'test τ = 28', vol21_28[2], vol21_28_se_test)\n\npd.concat([metrics_vol21_28, metrics_vol21_28_val, metrics_vol21_28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\nNaN\n0.002215\n0.013461\n0.000181\n0.952378\n3.427794e-46\n0.000000e+00\n\n\nMAE val τ = 28\n42.779302\n0.016402\n0.018792\n0.000353\n-5.889967\n5.193451e-01\n5.208990e-17\n\n\nMAE test τ = 28\n3.613422\n0.001193\n0.002050\n0.000004\n0.917982\n4.577967e-01\n7.541113e-05\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol21_28[0][-500:], vol21_28[1], vol21_28[2], vol21_28_se, vol21_28_se_val, vol21_28_se_test)\nfig.show()\n\n\n\n\nmetrics_vol21_d = pd.concat([\n    metrics_vol21_7, metrics_vol21_7_val, metrics_vol21_7_test,\n    metrics_vol21_14, metrics_vol21_14_val, metrics_vol21_14_test,\n    metrics_vol21_21, metrics_vol21_21_val, metrics_vol21_21_test,\n    metrics_vol21_28, metrics_vol21_28_val, metrics_vol21_28_test\n])\n\n\n\n\nVolatilidad \\(\\omega = 28\\)\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(vol28_7[0], param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\nNaN\n0.009313\n0.030121\n0.000907\n0.754956\n0.1\n\n\n1\nNaN\n0.005598\n0.021227\n0.000451\n0.878301\n0.2\n\n\n2\nNaN\n0.004076\n0.017158\n0.000294\n0.920488\n0.3\n\n\n3\nNaN\n0.003240\n0.014906\n0.000222\n0.939987\n0.4\n\n\n4\nNaN\n0.002708\n0.013524\n0.000183\n0.950601\n0.5\n\n\n5\nNaN\n0.002343\n0.012632\n0.000160\n0.956901\n0.6\n\n\n6\nNaN\n0.002078\n0.012050\n0.000145\n0.960784\n0.7\n\n\n7\nNaN\n0.001880\n0.011680\n0.000136\n0.963152\n0.8\n\n\n8\nNaN\n0.001729\n0.011472\n0.000132\n0.964457\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol28_7[0], best_param)\nvol28_7_se = model.smooth()\n\nmetrics_vol28_7 = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'train τ = 7', vol28_7[0], vol28_7_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol28_7[1], best_param)\nvol28_7_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol28_7[2], best_param)\nvol28_7_se_test = model.smooth()\n\nmetrics_vol28_7_val = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'val τ = 7', vol28_7[1], vol28_7_se_val)\nmetrics_vol28_7_test = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'test τ = 7', vol28_7[2], vol28_7_se_test)\n\npd.concat([metrics_vol28_7, metrics_vol28_7_val, metrics_vol28_7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\nNaN\n0.001729\n0.011472\n0.000132\n0.964457\n2.080088e-42\n0.000000\n\n\nMAE val τ = 7\n22.776284\n0.004897\n0.005409\n0.000029\n-9.049319\n4.853045e-01\n0.363816\n\n\nMAE test τ = 7\n4.049157\n0.000847\n0.001453\n0.000002\n0.274886\n9.865148e-01\n0.180878\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol28_7[0][-500:], vol28_7[1], vol28_7[2], vol28_7_se, vol28_7_se_val, vol28_7_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol28_14[0], best_param)\nvol28_14_se = model.smooth()\n\nmetrics_vol28_14 = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'train τ = 14', vol28_14[0], vol28_14_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol28_14[1], best_param)\nvol28_14_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol28_14[2], best_param)\nvol28_14_se_test = model.smooth()\n\nmetrics_vol28_14_val = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'val τ = 14', vol28_14[1], vol28_14_se_val)\nmetrics_vol28_14_test = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'test τ = 14', vol28_14[2], vol28_14_se_test)\n\npd.concat([metrics_vol28_14, metrics_vol28_14_val, metrics_vol28_14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\nNaN\n0.001729\n0.011472\n0.000132\n0.964457\n2.080088e-42\n0.000000\n\n\nMAE val τ = 14\n36.666901\n0.006637\n0.007027\n0.000049\n-12.781781\n3.199223e-01\n0.055452\n\n\nMAE test τ = 14\n3.334943\n0.000588\n0.001032\n0.000001\n0.702910\n1.836634e-01\n0.006907\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol28_14[0][-500:], vol28_14[1], vol28_14[2], vol28_14_se, vol28_14_se_val, vol28_14_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol28_21[0], best_param)\nvol28_21_se = model.smooth()\n\nmetrics_vol28_21 = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'train τ = 21', vol28_21[0], vol28_21_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol28_21[1], best_param)\nvol28_21_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol28_21[2], best_param)\nvol28_21_se_test = model.smooth()\n\nmetrics_vol28_21_val = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'val τ = 21', vol28_21[1], vol28_21_se_val)\nmetrics_vol28_21_test = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'test τ = 21', vol28_21[2], vol28_21_se_test)\n\npd.concat([metrics_vol28_21, metrics_vol28_21_val, metrics_vol28_21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\nNaN\n0.001729\n0.011472\n0.000132\n0.964457\n2.080088e-42\n0.000000\n\n\nMAE val τ = 21\n37.235382\n0.008127\n0.009179\n0.000084\n-1.115749\n1.548663e-01\n0.000681\n\n\nMAE test τ = 21\n4.143860\n0.000994\n0.001802\n0.000003\n0.918428\n1.884342e-01\n0.000639\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol28_21[0][-500:], vol28_21[1], vol28_21[2], vol28_21_se, vol28_21_se_val, vol28_21_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol28_28[0], best_param)\nvol28_28_se = model.smooth()\n\nmetrics_vol28_28 = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'train τ = 28', vol28_28[0], vol28_28_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol28_28[1], best_param)\nvol28_28_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol28_28[2], best_param)\nvol28_28_se_test = model.smooth()\n\nmetrics_vol28_28_val = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'val τ = 28', vol28_28[1], vol28_28_se_val)\nmetrics_vol28_28_test = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'test τ = 28', vol28_28[2], vol28_28_se_test)\n\npd.concat([metrics_vol28_28, metrics_vol28_28_val, metrics_vol28_28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\nNaN\n0.001729\n0.011472\n0.000132\n0.964457\n2.080088e-42\n0.000000\n\n\nMAE val τ = 28\n34.594749\n0.012416\n0.015259\n0.000233\n-3.315288\n9.806432e-03\n0.000266\n\n\nMAE test τ = 28\n3.738236\n0.001167\n0.001900\n0.000004\n0.933102\n1.810440e-01\n0.003661\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol28_28[0][-500:], vol28_28[1], vol28_28[2], vol28_28_se, vol28_28_se_val, vol28_28_se_test)\nfig.show()\n\n\n\n\nmetrics_vol28_d = pd.concat([\n    metrics_vol28_7, metrics_vol28_7_val, metrics_vol28_7_test,\n    metrics_vol28_14, metrics_vol28_14_val, metrics_vol28_14_test,\n    metrics_vol28_21, metrics_vol28_21_val, metrics_vol28_21_test,\n    metrics_vol28_28, metrics_vol28_28_val, metrics_vol28_28_test\n])\n\n\n\n\n\nCaso simple usando statsmodels\nA continuación, usaremos la librería statsmodels para computar nuestra suavización exponencial mediante la función de HolterWinters.\n\nclass ExponentialSmoothing:\n    def __init__(self, data: pd.Series, trend: Optional[str] = None, seasonal: Optional[str] = None, seasonal_periods: Optional[int] = None, smoothing_level: Optional[float] = None):\n        self.data = data\n        self.trend = trend\n        self.seasonal = seasonal\n        self.seasonal_periods = seasonal_periods\n        self.smoothing_level = smoothing_level\n\n    def smooth(self) -&gt; pd.Series:\n        model = ExponentialSmoothingLib(self.data, trend=self.trend, seasonal=self.seasonal, seasonal_periods=self.seasonal_periods)\n        self.fitted_model = model.fit(smoothing_level=self.smoothing_level)\n        return self.fitted_model.fittedvalues\n\n    def evaluate(self, smoothed_data: Optional[pd.Series] = None) -&gt; pd.DataFrame:\n        if smoothed_data is None:\n            smoothed_data = self.smooth()\n\n        T = len(self.data)\n        actual_values = self.data.values\n        predicted_values = pd.concat([pd.Series([self.data.iloc[0]]), smoothed_data.iloc[:-1]], ignore_index=True).values\n        error = actual_values - predicted_values\n        mean_actual = np.mean(actual_values)\n\n        SSE = sum(error**2)\n        MAPE = 100 * sum(abs(error / actual_values)) / T\n        MAE = np.mean(np.abs(error))\n        MSE = SSE / T\n        RMSE = np.sqrt(MSE)\n        R2 = 1 - (SSE / (np.sum((actual_values - mean_actual) ** 2)))\n\n        return pd.DataFrame({\n            'MAPE': [MAPE],\n            'MAE': [MAE],\n            'RMSE': [RMSE],\n            'MSE': [MSE],\n            'R2': [R2]\n        })\n\n    def optimize_param(self) -&gt; float:\n        smoothed_data = self.smooth()\n        return self.evaluate(smoothed_data)['MAE'].values[0]\n\n    def calculate_metrics(self, y: pd.Series, y_pred: pd.Series, metric_name: str, case: str) -&gt; pd.DataFrame:\n        residuals = y - y_pred\n        metrics_df = self.evaluate(y_pred)\n        metrics_df.index = metrics_df.index.map({0: f'{metric_name} {case}'})\n\n        max_lag = min(10, len(residuals) - 1)\n        if max_lag &gt; 0:\n            ljung_box_pval = acorr_ljungbox(residuals, lags=[max_lag], return_df=True)['lb_pvalue'].iloc[0]\n        else:\n            ljung_box_pval = np.nan\n\n        jb_pvalue = jarque_bera(residuals)[1]\n\n        metrics_df['Ljung-Box p-value'] = ljung_box_pval\n        metrics_df['Jarque-Bera p-value'] = jb_pvalue\n        return metrics_df, residuals\n\n    def plot_residuals(self, residuals: pd.Series, model_name: str):\n        fig, ax = plt.subplots(figsize=(8.6, 5))\n        ax.plot(residuals.index, residuals.values, color='#1f77b4', linestyle='-', marker='o', markersize=2)\n        ax.set_title(f'{model_name}', fontsize=14)\n        ax.set_xlabel('Índice', fontsize=12)\n        ax.set_ylabel('Residuo', fontsize=12)\n        ax.grid(True, linestyle='--', alpha=0.7)\n        plt.show()\n\n    def plot_diagnostics(self, residuals: pd.Series, model_name: str):\n        fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(7.6, 6))\n        sm.qqplot(residuals, line='s', ax=axes[0])\n        axes[0].set_title('QQ Plot de los Residuos')\n        sm.graphics.tsa.plot_pacf(residuals, lags=30, ax=axes[1])\n        axes[1].set_title('Autocorrelación Parcial de los Residuos')\n        plt.suptitle(f'{model_name}', fontsize=16)\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n        plt.show()\n\n    def residuals(self, model_name: str, metric_name: str, case: str, y: pd.Series, y_pred: pd.Series, show_info: bool = False) -&gt; pd.DataFrame:\n        metrics_df, residuals = self.calculate_metrics(y, y_pred, metric_name, case)\n\n        if show_info:\n            self.plot_residuals(residuals, model_name)\n            self.plot_diagnostics(residuals, model_name)\n\n            ljung_box_pval = metrics_df['Ljung-Box p-value'].iloc[0]\n            jb_pvalue = metrics_df['Jarque-Bera p-value'].iloc[0]\n\n            if ljung_box_pval &gt; 0.05:\n                print('No se rechaza H0: los residuales son independientes (no correlacionados).')\n            else:\n                print('Se rechaza H0: hay autocorrelación en los residuales.')\n\n            print(f'Jarque-Bera p-value: {jb_pvalue}')\n            if jb_pvalue &gt; 0.05:\n                print('No se rechaza H0: los residuales siguen una distribución normal.')\n            else:\n                print('Se rechaza H0: los residuales no siguen una distribución normal.')\n\n        return metrics_df\n\n    def plot_preds(self, y_train: pd.Series, y_val: pd.Series, y_test: pd.Series,\n                            y_pred_train: pd.Series, y_pred_val: pd.Series, y_pred_test: pd.Series):\n        fig = go.Figure()\n\n        fig.add_trace(go.Scatter(x=y_train[-250:].index, y=y_train[-250:], mode='lines', name='y_train', line=dict(color='#FDDBBB', dash = 'dash')))\n        fig.add_trace(go.Scatter(x=y_val.index, y=y_val, mode='lines', name='y_val', line=dict(color='#BFECFF', dash = 'dash')))\n        fig.add_trace(go.Scatter(x=y_test.index, y=y_test, mode='lines', name='y_test', line=dict(color='#C9E9D2', dash = 'dash')))\n\n        fig.add_trace(go.Scatter(x=y_pred_train[-250:].index, y=y_pred_train[-250:], mode='lines', name='y_pred_train',\n                                 line=dict(color='#CB9DF0', dash='dash')))\n        fig.add_trace(go.Scatter(x=y_pred_val.index, y=y_pred_val, mode='lines', name='y_pred_val',\n                                 line=dict(color='#FFCCEA', dash='dash')))\n        fig.add_trace(go.Scatter(x=y_pred_test.index, y=y_pred_test, mode='lines', name='y_pred_test',\n                                 line=dict(color='#EEEEEE', dash='dash')))\n\n        fig.update_layout(\n            title=\"Comparación de y v.s. y_pred\",\n            xaxis_title=\"\",\n            yaxis_title=\"\",\n            legend_title=\"Series\",\n            margin={'b': 0, 'r': 30, 'l': 30, 't': 80},\n            plot_bgcolor='rgba(0, 0, 0, 0.0)',\n            paper_bgcolor='rgba(0, 0, 0, 0.0)',\n            font_color=\"white\",\n            hoverlabel=dict(\n                bgcolor=\"#222\"\n            ),\n            xaxis=dict(gridcolor='#222', tickfont=dict(color='white')),\n            yaxis=dict(gridcolor='#222', tickfont=dict(color='white')),\n        )\n\n        return fig\n\n\nPrice\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(price7[0], smoothing_level = param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\n8.876271\n737.275460\n1684.977617\n2.839150e+06\n0.987656\n0.1\n\n\n1\n6.609508\n544.141311\n1282.635726\n1.645154e+06\n0.992847\n0.2\n\n\n2\n5.705266\n474.744387\n1125.003019\n1.265632e+06\n0.994497\n0.3\n\n\n3\n5.224338\n437.207197\n1040.017149\n1.081636e+06\n0.995297\n0.4\n\n\n4\n4.928682\n413.858858\n988.328221\n9.767927e+05\n0.995753\n0.5\n\n\n5\n4.752038\n398.667162\n955.240544\n9.124845e+05\n0.996033\n0.6\n\n\n6\n4.637379\n387.862341\n933.984231\n8.723265e+05\n0.996207\n0.7\n\n\n7\n4.566312\n381.205526\n921.051463\n8.483358e+05\n0.996312\n0.8\n\n\n8\n4.525583\n377.548171\n914.523836\n8.363538e+05\n0.996364\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(price7[0], smoothing_level = best_param)\nprice7_se = model.smooth()\n\nmetrics_price7 = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'train τ = 7', price7[0], price7_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(price7[1], smoothing_level = best_param)\nprice7_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(price7[2], smoothing_level = best_param)\nprice7_se_test = model.smooth()\n\nmetrics_price7_val = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'val τ = 7', price7[1], price7_se_val)\nmetrics_price7_test = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'test τ = 7', price7[2], price7_se_test)\n\npd.concat([metrics_price7, metrics_price7_val, metrics_price7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\n4.525583\n377.548171\n914.523836\n8.363538e+05\n0.996364\n2.388399e-11\n0.000000\n\n\nMAE val τ = 7\n5.410749\n2551.131517\n3274.988349\n1.072555e+07\n-1.459638\n3.353613e-01\n0.723077\n\n\nMAE test τ = 7\n3.398894\n1579.028366\n1880.961863\n3.538018e+06\n0.188644\n3.429490e-01\n0.898806\n\n\n\n\n\n\n\n\nfig = model.plot_preds(price7[0][-500:], price7[1], price7[2], price7_se, price7_se_val, price7_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(price14[0], smoothing_level = best_param)\nprice14_se = model.smooth()\n\nmetrics_price14 = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'train τ = 14', price14[0], price14_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(price14[1], smoothing_level = best_param)\nprice14_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(price14[2], smoothing_level = best_param)\nprice14_se_test = model.smooth()\n\nmetrics_price14_val = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'val τ = 14', price14[1], price14_se_val)\nmetrics_price14_test = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'test τ = 14', price14[2], price14_se_test)\n\npd.concat([metrics_price14, metrics_price14_val, metrics_price14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\n4.525583\n377.548171\n914.523836\n8.363538e+05\n0.996364\n2.388399e-11\n0.000000\n\n\nMAE val τ = 14\n14.450054\n7462.100293\n7866.289479\n6.187851e+07\n-106.011317\n1.141840e-02\n0.539228\n\n\nMAE test τ = 14\n1.177869\n607.576676\n907.969720\n8.244090e+05\n-0.425715\n2.465928e-01\n0.033361\n\n\n\n\n\n\n\n\nfig = model.plot_preds(price14[0][-500:], price14[1], price14[2], price14_se, price14_se_val, price14_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(price21[0], smoothing_level = best_param)\nprice21_se = model.smooth()\n\nmetrics_price21 = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'train τ = 21', price21[0], price21_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(price21[1], smoothing_level = best_param)\nprice21_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(price21[2], smoothing_level = best_param)\nprice21_se_test = model.smooth()\n\nmetrics_price21_val = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'val τ = 21', price21[1], price21_se_val)\nmetrics_price21_test = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'test τ = 21', price21[2], price21_se_test)\n\npd.concat([metrics_price21, metrics_price21_val, metrics_price21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\n4.525583\n377.548171\n914.523836\n8.363538e+05\n0.996364\n2.388399e-11\n0.000000\n\n\nMAE val τ = 21\n22.210727\n13603.358996\n14522.647188\n2.109073e+08\n-3.698244\n2.561018e-02\n0.330107\n\n\nMAE test τ = 21\n3.319735\n2045.213849\n2992.335215\n8.954070e+06\n0.800536\n8.088324e-01\n0.379845\n\n\n\n\n\n\n\n\nfig = model.plot_preds(price21[0][-500:], price21[1], price21[2], price21_se, price21_se_val, price21_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(price28[0], smoothing_level = best_param)\nprice28_se = model.smooth()\n\nmetrics_price28 = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'train τ = 28', price28[0], price28_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(price28[1], smoothing_level = best_param)\nprice28_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(price28[2], smoothing_level = best_param)\nprice28_se_test = model.smooth()\n\nmetrics_price28_val = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'val τ = 28', price28[1], price28_se_val)\nmetrics_price28_test = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'test τ = 28', price28[2], price28_se_test)\n\npd.concat([metrics_price28, metrics_price28_val, metrics_price28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\n4.525583\n377.548171\n914.523836\n8.363538e+05\n0.996364\n2.388399e-11\n0.000000\n\n\nMAE val τ = 28\n27.350017\n18180.475472\n18998.294049\n3.609352e+08\n-18.797489\n1.791521e-03\n0.187340\n\n\nMAE test τ = 28\n3.870259\n2534.638645\n3252.079320\n1.057602e+07\n0.419900\n6.980119e-01\n0.957407\n\n\n\n\n\n\n\n\nfig = model.plot_preds(price28[0][-500:], price28[1], price28[2], price28_se, price28_se_val, price28_se_test)\nfig.show()\n\n\n\n\nmetrics_price = pd.concat([\n    metrics_price7, metrics_price7_val, metrics_price7_test,\n    metrics_price14, metrics_price14_val, metrics_price14_test,\n    metrics_price21, metrics_price21_val, metrics_price21_test,\n    metrics_price28, metrics_price28_val, metrics_price28_test\n])\n\n\n\n\nRetorno acumulado\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(A_t7[0], smoothing_level = param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\nNaN\n0.102749\n0.200028\n0.040011\n0.998930\n0.1\n\n\n1\nNaN\n0.072009\n0.148842\n0.022154\n0.999407\n0.2\n\n\n2\nNaN\n0.060886\n0.129237\n0.016702\n0.999553\n0.3\n\n\n3\nNaN\n0.055120\n0.119108\n0.014187\n0.999620\n0.4\n\n\n4\nNaN\n0.051761\n0.113368\n0.012852\n0.999656\n0.5\n\n\n5\nNaN\n0.049592\n0.110095\n0.012121\n0.999676\n0.6\n\n\n6\nNaN\n0.048146\n0.108364\n0.011743\n0.999686\n0.7\n\n\n7\nNaN\n0.047216\n0.107657\n0.011590\n0.999690\n0.8\n\n\n8\nNaN\n0.046706\n0.107660\n0.011591\n0.999690\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(A_t7[0], smoothing_level = best_param)\nA_t7_se = model.smooth()\n\nmetrics_A_t7 = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'train τ = 7', A_t7[0], A_t7_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(A_t7[1], smoothing_level = best_param)\nA_t7_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(A_t7[2], smoothing_level = best_param)\nA_t7_se_test = model.smooth()\n\nmetrics_A_t7_val = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'val τ = 7', A_t7[1], A_t7_se_val)\nmetrics_A_t7_test = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'test τ = 7', A_t7[2], A_t7_se_test)\n\npd.concat([metrics_A_t7, metrics_A_t7_val, metrics_A_t7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\nNaN\n0.046706\n0.107660\n0.011591\n0.999690\n1.962549e-52\n0.000000\n\n\nMAE val τ = 7\n0.246643\n0.057535\n0.073593\n0.005416\n-1.492079\n3.415244e-01\n0.723094\n\n\nMAE test τ = 7\n0.151569\n0.035334\n0.042044\n0.001768\n0.186598\n3.234844e-01\n0.883296\n\n\n\n\n\n\n\n\nfig = model.plot_preds(A_t7[0][-500:], A_t7[1], A_t7[2], A_t7_se, A_t7_se_val, A_t7_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(A_t14[0], smoothing_level = best_param)\nA_t14_se = model.smooth()\n\nmetrics_A_t14 = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'train τ = 14', A_t14[0], A_t14_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(A_t14[1], smoothing_level = best_param)\nA_t14_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(A_t14[2], smoothing_level = best_param)\nA_t14_se_test = model.smooth()\n\nmetrics_A_t14_val = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'val τ = 14', A_t14[1], A_t14_se_val)\nmetrics_A_t14_test = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'test τ = 14', A_t14[2], A_t14_se_test)\n\npd.concat([metrics_A_t14, metrics_A_t14_val, metrics_A_t14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\nNaN\n0.046706\n0.107660\n0.011591\n0.999690\n1.962549e-52\n0.000000\n\n\nMAE val τ = 14\n0.684678\n0.160353\n0.169150\n0.028612\n-122.751190\n1.482738e-02\n0.560407\n\n\nMAE test τ = 14\n0.051388\n0.012035\n0.018071\n0.000327\n-0.412516\n2.670484e-01\n0.020674\n\n\n\n\n\n\n\n\nfig = model.plot_preds(A_t14[0][-500:], A_t14[1], A_t14[2], A_t14_se, A_t14_se_val, A_t14_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(A_t21[0], smoothing_level = best_param)\nA_t21_se = model.smooth()\n\nmetrics_A_t21 = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'train τ = 21', A_t21[0], A_t21_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(A_t21[1], smoothing_level = best_param)\nA_t21_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(A_t21[2], smoothing_level = best_param)\nA_t21_se_test = model.smooth()\n\nmetrics_A_t21_val = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'val τ = 21', A_t21[1], A_t21_se_val)\nmetrics_A_t21_test = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'test τ = 21', A_t21[2], A_t21_se_test)\n\npd.concat([metrics_A_t21, metrics_A_t21_val, metrics_A_t21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\nNaN\n0.046706\n0.107660\n0.011591\n0.999690\n1.962549e-52\n0.000000\n\n\nMAE val τ = 21\n1.117020\n0.263572\n0.278311\n0.077457\n-4.484802\n1.595702e-02\n0.362303\n\n\nMAE test τ = 21\n0.150332\n0.035479\n0.052336\n0.002739\n0.806046\n8.434113e-01\n0.317602\n\n\n\n\n\n\n\n\nfig = model.plot_preds(A_t21[0][-500:], A_t21[1], A_t21[2], A_t21_se, A_t21_se_val, A_t21_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(A_t28[0], smoothing_level = best_param)\nA_t28_se = model.smooth()\n\nmetrics_A_t28 = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'train τ = 28', A_t28[0], A_t28_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(A_t28[1], smoothing_level = best_param)\nA_t28_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(A_t28[2], smoothing_level = best_param)\nA_t28_se_test = model.smooth()\n\nmetrics_A_t28_val = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'val τ = 28', A_t28[1], A_t28_se_val)\nmetrics_A_t28_test = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'test τ = 28', A_t28[2], A_t28_se_test)\n\npd.concat([metrics_A_t28, metrics_A_t28_val, metrics_A_t28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\nNaN\n0.046706\n0.107660\n0.011591\n0.999690\n1.962549e-52\n0.000000\n\n\nMAE val τ = 28\n1.442284\n0.341625\n0.356231\n0.126900\n-24.886559\n1.160388e-03\n0.182208\n\n\nMAE test τ = 28\n0.169122\n0.040034\n0.051840\n0.002687\n0.451802\n7.820676e-01\n0.978637\n\n\n\n\n\n\n\n\nfig = model.plot_preds(A_t28[0][-500:], A_t28[1], A_t28[2], A_t28_se, A_t28_se_val, A_t28_se_test)\nfig.show()\n\n\n\n\nmetrics_A_t = pd.concat([\n    metrics_A_t7, metrics_A_t7_val, metrics_A_t7_test,\n    metrics_A_t14, metrics_A_t14_val, metrics_A_t14_test,\n    metrics_A_t21, metrics_A_t21_val, metrics_A_t21_test,\n    metrics_A_t28, metrics_A_t28_val, metrics_A_t28_test\n])\n\n\n\n\nVolatilidad \\(\\omega = 7\\)\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(vol7_7[0], smoothing_level = param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\nNaN\n0.019004\n0.053951\n0.002911\n0.312482\n0.1\n\n\n1\nNaN\n0.016768\n0.049053\n0.002406\n0.431635\n0.2\n\n\n2\nNaN\n0.015059\n0.045384\n0.002060\n0.513477\n0.3\n\n\n3\nNaN\n0.013717\n0.042620\n0.001816\n0.570938\n0.4\n\n\n4\nNaN\n0.012701\n0.040561\n0.001645\n0.611401\n0.5\n\n\n5\nNaN\n0.011940\n0.039055\n0.001525\n0.639726\n0.6\n\n\n6\nNaN\n0.011336\n0.037981\n0.001443\n0.659258\n0.7\n\n\n7\nNaN\n0.010877\n0.037245\n0.001387\n0.672334\n0.8\n\n\n8\nNaN\n0.010532\n0.036776\n0.001352\n0.680547\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol7_7[0], smoothing_level = best_param)\nvol7_7_se = model.smooth()\n\nmetrics_vol7_7 = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'train τ = 7', vol7_7[0], vol7_7_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol7_7[1], smoothing_level = best_param)\nvol7_7_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol7_7[2], smoothing_level = best_param)\nvol7_7_se_test = model.smooth()\n\nmetrics_vol7_7_val = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'val τ = 7', vol7_7[1], vol7_7_se_val)\nmetrics_vol7_7_test = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'test τ = 7', vol7_7[2], vol7_7_se_test)\n\npd.concat([metrics_vol7_7, metrics_vol7_7_val, metrics_vol7_7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\nNaN\n0.010532\n0.036776\n0.001352\n0.680547\n5.398223e-219\n0.000000\n\n\nMAE val τ = 7\n37.628912\n0.004382\n0.005421\n0.000029\n-1.261051\n7.971377e-01\n0.133831\n\n\nMAE test τ = 7\n24.447876\n0.003538\n0.004207\n0.000018\n-0.361579\n6.168363e-01\n0.873767\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol7_7[0][-500:], vol7_7[1], vol7_7[2], vol7_7_se, vol7_7_se_val, vol7_7_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol7_14[0], smoothing_level = best_param)\nvol7_14_se = model.smooth()\n\nmetrics_vol7_14 = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'train τ = 14', vol7_14[0], vol7_14_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol7_14[1], smoothing_level = best_param)\nvol7_14_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol7_14[2], smoothing_level = best_param)\nvol7_14_se_test = model.smooth()\n\nmetrics_vol7_14_val = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'val τ = 14', vol7_14[1], vol7_14_se_val)\nmetrics_vol7_14_test = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'test τ = 14', vol7_14[2], vol7_14_se_test)\n\npd.concat([metrics_vol7_14, metrics_vol7_14_val, metrics_vol7_14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\nNaN\n0.010532\n0.036776\n0.001352\n0.680547\n5.398223e-219\n0.000000\n\n\nMAE val τ = 14\n18.763997\n0.002579\n0.003219\n0.000010\n0.363582\n5.166919e-01\n0.608130\n\n\nMAE test τ = 14\n25.542794\n0.002916\n0.004003\n0.000016\n0.015807\n7.364931e-01\n0.000651\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol7_14[0][-500:], vol7_14[1], vol7_14[2], vol7_14_se, vol7_14_se_val, vol7_14_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol7_21[0], smoothing_level = best_param)\nvol7_21_se = model.smooth()\n\nmetrics_vol7_21 = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'train τ = 21', vol7_21[0], vol7_21_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol7_21[1], smoothing_level = best_param)\nvol7_21_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol7_21[2], smoothing_level = best_param)\nvol7_21_se_test = model.smooth()\n\nmetrics_vol7_21_val = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'val τ = 21', vol7_21[1], vol7_21_se_val)\nmetrics_vol7_21_test = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'test τ = 21', vol7_21[2], vol7_21_se_test)\n\npd.concat([metrics_vol7_21, metrics_vol7_21_val, metrics_vol7_21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\nNaN\n0.010532\n0.036776\n0.001352\n0.680547\n5.398223e-219\n0.000000\n\n\nMAE val τ = 21\n67.284730\n0.019962\n0.023077\n0.000533\n-1.305010\n9.075412e-01\n0.185958\n\n\nMAE test τ = 21\n24.853723\n0.005817\n0.008511\n0.000072\n0.686491\n8.770575e-01\n0.560071\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol7_21[0][-500:], vol7_21[1], vol7_21[2], vol7_21_se, vol7_21_se_val, vol7_21_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol7_28[0], smoothing_level = best_param)\nvol7_28_se = model.smooth()\n\nmetrics_vol7_28 = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'train τ = 28', vol7_28[0], vol7_28_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol7_28[1], smoothing_level = best_param)\nvol7_28_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol7_28[2], smoothing_level = best_param)\nvol7_28_se_test = model.smooth()\n\nmetrics_vol7_28_val = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'val τ = 28', vol7_28[1], vol7_28_se_val)\nmetrics_vol7_28_test = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'test τ = 28', vol7_28[2], vol7_28_se_test)\n\npd.concat([metrics_vol7_28, metrics_vol7_28_val, metrics_vol7_28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\nNaN\n0.010532\n0.036776\n0.001352\n0.680547\n5.398223e-219\n0.000000\n\n\nMAE val τ = 28\n56.155201\n0.025201\n0.029054\n0.000844\n-4.429505\n7.972382e-01\n0.034458\n\n\nMAE test τ = 28\n23.450565\n0.007974\n0.011143\n0.000124\n0.201312\n1.851997e-02\n0.696147\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol7_28[0][-500:], vol7_28[1], vol7_28[2], vol7_28_se, vol7_28_se_val, vol7_28_se_test)\nfig.show()\n\n\n\n\nmetrics_vol7 = pd.concat([\n    metrics_vol7_7, metrics_vol7_7_val, metrics_vol7_7_test,\n    metrics_vol7_14, metrics_vol7_14_val, metrics_vol7_14_test,\n    metrics_vol7_21, metrics_vol7_21_val, metrics_vol7_21_test,\n    metrics_vol7_28, metrics_vol7_28_val, metrics_vol7_28_test\n])\n\n\n\n\nVolatilidad \\(\\omega = 14\\)\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(vol14_7[0], smoothing_level = param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\nNaN\n0.014361\n0.043906\n0.001928\n0.511511\n0.1\n\n\n1\nNaN\n0.010759\n0.036137\n0.001306\n0.669093\n0.2\n\n\n2\nNaN\n0.008880\n0.031524\n0.000994\n0.748183\n0.3\n\n\n3\nNaN\n0.007712\n0.028696\n0.000823\n0.791334\n0.4\n\n\n4\nNaN\n0.006944\n0.026903\n0.000724\n0.816595\n0.5\n\n\n5\nNaN\n0.006409\n0.025734\n0.000662\n0.832185\n0.6\n\n\n6\nNaN\n0.006021\n0.024964\n0.000623\n0.842086\n0.7\n\n\n7\nNaN\n0.005731\n0.024463\n0.000598\n0.848356\n0.8\n\n\n8\nNaN\n0.005515\n0.024158\n0.000584\n0.852115\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol14_7[0], smoothing_level = best_param)\nvol14_7_se = model.smooth()\n\nmetrics_vol14_7 = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'train τ = 7', vol14_7[0], vol14_7_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol14_7[1], smoothing_level = best_param)\nvol14_7_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol14_7[2], smoothing_level = best_param)\nvol14_7_se_test = model.smooth()\n\nmetrics_vol14_7_val = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'val τ = 7', vol14_7[1], vol14_7_se_val)\nmetrics_vol14_7_test = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'test τ = 7', vol14_7[2], vol14_7_se_test)\n\npd.concat([metrics_vol14_7, metrics_vol14_7_val, metrics_vol14_7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\nNaN\n0.005515\n0.024158\n5.836072e-04\n0.852115\n3.661844e-46\n0.000000\n\n\nMAE val τ = 7\n33.823503\n0.005510\n0.006004\n3.605256e-05\n-127.652529\n7.904339e-01\n0.082897\n\n\nMAE test τ = 7\n4.052057\n0.000671\n0.000833\n6.947101e-07\n-1.479053\n9.197661e-01\n0.916042\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol14_7[0][-500:], vol14_7[1], vol14_7[2], vol14_7_se, vol14_7_se_val, vol14_7_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol14_14[0], smoothing_level = best_param)\nvol14_14_se = model.smooth()\n\nmetrics_vol14_14 = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'train τ = 14', vol14_14[0], vol14_14_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol14_14[1], smoothing_level = best_param)\nvol14_14_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol14_14[2], smoothing_level = best_param)\nvol14_14_se_test = model.smooth()\n\nmetrics_vol14_14_val = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'val τ = 14', vol14_14[1], vol14_14_se_val)\nmetrics_vol14_14_test = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'test τ = 14', vol14_14[2], vol14_14_se_test)\n\npd.concat([metrics_vol14_14, metrics_vol14_14_val, metrics_vol14_14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\nNaN\n0.005515\n0.024158\n5.836072e-04\n0.852115\n3.661844e-46\n0.000000\n\n\nMAE val τ = 14\n20.689774\n0.003483\n0.004332\n1.877001e-05\n-57.072766\n6.315728e-01\n0.000001\n\n\nMAE test τ = 14\n3.756666\n0.000633\n0.000757\n5.729753e-07\n-0.772735\n1.073652e-01\n0.220719\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol14_14[0][-500:], vol14_14[1], vol14_14[2], vol14_14_se, vol14_14_se_val, vol14_14_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol14_21[0], smoothing_level = best_param)\nvol14_21_se = model.smooth()\n\nmetrics_vol14_21 = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'train τ = 21', vol14_21[0], vol14_21_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol14_21[1], smoothing_level = best_param)\nvol14_21_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol14_21[2], smoothing_level = best_param)\nvol14_21_se_test = model.smooth()\n\nmetrics_vol14_21_val = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'val τ = 21', vol14_21[1], vol14_21_se_val)\nmetrics_vol14_21_test = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'test τ = 21', vol14_21[2], vol14_21_se_test)\n\npd.concat([metrics_vol14_21, metrics_vol14_21_val, metrics_vol14_21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\nNaN\n0.005515\n0.024158\n0.000584\n0.852115\n3.661844e-46\n0.000000e+00\n\n\nMAE val τ = 21\n41.685487\n0.013470\n0.016199\n0.000262\n-1.520083\n7.230675e-01\n3.959695e-18\n\n\nMAE test τ = 21\n9.584982\n0.002913\n0.004743\n0.000022\n0.783916\n9.533840e-02\n6.906289e-04\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol14_21[0][-500:], vol14_21[1], vol14_21[2], vol14_21_se, vol14_21_se_val, vol14_21_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol14_28[0], smoothing_level = best_param)\nvol14_28_se = model.smooth()\n\nmetrics_vol14_28 = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'train τ = 28', vol14_28[0], vol14_28_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol14_28[1], smoothing_level = best_param)\nvol14_28_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol14_28[2], smoothing_level = best_param)\nvol14_28_se_test = model.smooth()\n\nmetrics_vol14_28_val = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'val τ = 28', vol14_28[1], vol14_28_se_val)\nmetrics_vol14_28_test = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'test τ = 28', vol14_28[2], vol14_28_se_test)\n\npd.concat([metrics_vol14_28, metrics_vol14_28_val, metrics_vol14_28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\nNaN\n0.005515\n0.024158\n0.000584\n0.852115\n3.661844e-46\n0.000000e+00\n\n\nMAE val τ = 28\n48.322368\n0.019203\n0.020935\n0.000438\n-8.715676\n7.769449e-01\n6.949045e-33\n\n\nMAE test τ = 28\n10.221859\n0.003779\n0.005210\n0.000027\n0.398179\n8.146092e-02\n4.263438e-01\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol14_28[0][-500:], vol14_28[1], vol14_28[2], vol14_28_se, vol14_28_se_val, vol14_28_se_test)\nfig.show()\n\n\n\n\nmetrics_vol14 = pd.concat([\n    metrics_vol14_7, metrics_vol14_7_val, metrics_vol14_7_test,\n    metrics_vol14_14, metrics_vol14_14_val, metrics_vol14_14_test,\n    metrics_vol14_21, metrics_vol14_21_val, metrics_vol14_21_test,\n    metrics_vol14_28, metrics_vol14_28_val, metrics_vol14_28_test\n])\n\n\n\n\nVolatilidad \\(\\omega = 21\\)\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(vol21_7[0], smoothing_level = param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\nNaN\n0.011977\n0.037543\n0.001409\n0.629595\n0.1\n\n\n1\nNaN\n0.008228\n0.029254\n0.000856\n0.775093\n0.2\n\n\n2\nNaN\n0.006499\n0.025110\n0.000630\n0.834308\n0.3\n\n\n3\nNaN\n0.005543\n0.022778\n0.000519\n0.863656\n0.4\n\n\n4\nNaN\n0.004951\n0.021346\n0.000456\n0.880255\n0.5\n\n\n5\nNaN\n0.004547\n0.020424\n0.000417\n0.890372\n0.6\n\n\n6\nNaN\n0.004251\n0.019822\n0.000393\n0.896747\n0.7\n\n\n7\nNaN\n0.004034\n0.019434\n0.000378\n0.900745\n0.8\n\n\n8\nNaN\n0.003870\n0.019202\n0.000369\n0.903106\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol21_7[0], smoothing_level = best_param)\nvol21_7_se = model.smooth()\n\nmetrics_vol21_7 = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'train τ = 7', vol21_7[0], vol21_7_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol21_7[1], smoothing_level = best_param)\nvol21_7_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol21_7[2], smoothing_level = best_param)\nvol21_7_se_test = model.smooth()\n\nmetrics_vol21_7_val = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'val τ = 7', vol21_7[1], vol21_7_se_val)\nmetrics_vol21_7_test = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'test τ = 7', vol21_7[2], vol21_7_se_test)\n\npd.concat([metrics_vol21_7, metrics_vol21_7_val, metrics_vol21_7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\nNaN\n0.003870\n0.019202\n3.687029e-04\n0.903106\n3.427794e-46\n0.000000\n\n\nMAE val τ = 7\n24.639399\n0.004875\n0.005659\n3.202053e-05\n-93.863726\n9.767082e-01\n0.063760\n\n\nMAE test τ = 7\n2.275079\n0.000456\n0.000654\n4.281579e-07\n-0.268456\n4.601006e-01\n0.973017\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol21_7[0][-500:], vol21_7[1], vol21_7[2], vol21_7_se, vol21_7_se_val, vol21_7_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol21_14[0], smoothing_level = best_param)\nvol21_14_se = model.smooth()\n\nmetrics_vol21_14 = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'train τ = 14', vol21_14[0], vol21_14_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol21_14[1], smoothing_level = best_param)\nvol21_14_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol21_14[2], smoothing_level = best_param)\nvol21_14_se_test = model.smooth()\n\nmetrics_vol21_14_val = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'val τ = 14', vol21_14[1], vol21_14_se_val)\nmetrics_vol21_14_test = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'test τ = 14', vol21_14[2], vol21_14_se_test)\n\npd.concat([metrics_vol21_14, metrics_vol21_14_val, metrics_vol21_14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\nNaN\n0.003870\n0.019202\n3.687029e-04\n0.903106\n3.427794e-46\n0.000000e+00\n\n\nMAE val τ = 14\n32.155220\n0.005409\n0.006234\n3.885917e-05\n-87.343266\n9.018262e-01\n6.074575e-08\n\n\nMAE test τ = 14\n4.408937\n0.000741\n0.000901\n8.110631e-07\n-0.843888\n3.785980e-01\n3.502899e-01\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol21_14[0][-500:], vol21_14[1], vol21_14[2], vol21_14_se, vol21_14_se_val, vol21_14_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol21_21[0], smoothing_level = best_param)\nvol21_21_se = model.smooth()\n\nmetrics_vol21_21 = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'train τ = 21', vol21_21[0], vol21_21_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol21_21[1], smoothing_level = best_param)\nvol21_21_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol21_21[2], smoothing_level = best_param)\nvol21_21_se_test = model.smooth()\n\nmetrics_vol21_21_val = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'val τ = 21', vol21_21[1], vol21_21_se_val)\nmetrics_vol21_21_test = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'test τ = 21', vol21_21[2], vol21_21_se_test)\n\npd.concat([metrics_vol21_21, metrics_vol21_21_val, metrics_vol21_21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\nNaN\n0.003870\n0.019202\n0.000369\n0.903106\n3.427794e-46\n0.000000e+00\n\n\nMAE val τ = 21\n39.490052\n0.010181\n0.011634\n0.000135\n-1.179671\n4.597669e-01\n5.724525e-07\n\n\nMAE test τ = 21\n7.883541\n0.002106\n0.003458\n0.000012\n0.807484\n1.146190e-01\n5.402006e-04\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol21_21[0][-500:], vol21_21[1], vol21_21[2], vol21_21_se, vol21_21_se_val, vol21_21_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol21_28[0], smoothing_level = best_param)\nvol21_28_se = model.smooth()\n\nmetrics_vol21_28 = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'train τ = 28', vol21_28[0], vol21_28_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol21_28[1], smoothing_level = best_param)\nvol21_28_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol21_28[2], smoothing_level = best_param)\nvol21_28_se_test = model.smooth()\n\nmetrics_vol21_28_val = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'val τ = 28', vol21_28[1], vol21_28_se_val)\nmetrics_vol21_28_test = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'test τ = 28', vol21_28[2], vol21_28_se_test)\n\npd.concat([metrics_vol21_28, metrics_vol21_28_val, metrics_vol21_28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\nNaN\n0.003870\n0.019202\n0.000369\n0.903106\n3.427794e-46\n0.000000e+00\n\n\nMAE val τ = 28\n41.624074\n0.016049\n0.018614\n0.000346\n-5.760398\n5.223368e-01\n5.210346e-17\n\n\nMAE test τ = 28\n6.174221\n0.002060\n0.003240\n0.000010\n0.795153\n4.854350e-01\n7.161414e-05\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol21_28[0][-500:], vol21_28[1], vol21_28[2], vol21_28_se, vol21_28_se_val, vol21_28_se_test)\nfig.show()\n\n\n\n\nmetrics_vol21 = pd.concat([\n    metrics_vol21_7, metrics_vol21_7_val, metrics_vol21_7_test,\n    metrics_vol21_14, metrics_vol21_14_val, metrics_vol21_14_test,\n    metrics_vol21_21, metrics_vol21_21_val, metrics_vol21_21_test,\n    metrics_vol21_28, metrics_vol21_28_val, metrics_vol21_28_test\n])\n\n\n\n\nVolatilidad \\(\\omega = 28\\)\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(vol28_7[0], smoothing_level = param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\nNaN\n0.010221\n0.032930\n0.001084\n0.707122\n0.1\n\n\n1\nNaN\n0.006674\n0.024917\n0.000621\n0.832307\n0.2\n\n\n2\nNaN\n0.005232\n0.021289\n0.000453\n0.877593\n0.3\n\n\n3\nNaN\n0.004451\n0.019303\n0.000373\n0.899359\n0.4\n\n\n4\nNaN\n0.003956\n0.018093\n0.000327\n0.911582\n0.5\n\n\n5\nNaN\n0.003620\n0.017316\n0.000300\n0.919013\n0.6\n\n\n6\nNaN\n0.003376\n0.016809\n0.000283\n0.923684\n0.7\n\n\n7\nNaN\n0.003193\n0.016485\n0.000272\n0.926600\n0.8\n\n\n8\nNaN\n0.003055\n0.016293\n0.000265\n0.928302\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol28_7[0], smoothing_level = best_param)\nvol28_7_se = model.smooth()\n\nmetrics_vol28_7 = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'train τ = 7', vol28_7[0], vol28_7_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol28_7[1], smoothing_level = best_param)\nvol28_7_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol28_7[2], smoothing_level = best_param)\nvol28_7_se_test = model.smooth()\n\nmetrics_vol28_7_val = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'val τ = 7', vol28_7[1], vol28_7_se_val)\nmetrics_vol28_7_test = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'test τ = 7', vol28_7[2], vol28_7_se_test)\n\npd.concat([metrics_vol28_7, metrics_vol28_7_val, metrics_vol28_7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\nNaN\n0.003055\n0.016293\n0.000265\n0.928302\n2.080088e-42\n0.000000\n\n\nMAE val τ = 7\n24.090786\n0.005197\n0.005680\n0.000032\n-10.081326\n4.853045e-01\n0.363816\n\n\nMAE test τ = 7\n6.550145\n0.001370\n0.001931\n0.000004\n-0.280476\n9.865148e-01\n0.180878\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol28_7[0][-500:], vol28_7[1], vol28_7[2], vol28_7_se, vol28_7_se_val, vol28_7_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol28_14[0], smoothing_level = best_param)\nvol28_14_se = model.smooth()\n\nmetrics_vol28_14 = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'train τ = 14', vol28_14[0], vol28_14_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol28_14[1], smoothing_level = best_param)\nvol28_14_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol28_14[2], smoothing_level = best_param)\nvol28_14_se_test = model.smooth()\n\nmetrics_vol28_14_val = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'val τ = 14', vol28_14[1], vol28_14_se_val)\nmetrics_vol28_14_test = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'test τ = 14', vol28_14[2], vol28_14_se_test)\n\npd.concat([metrics_vol28_14, metrics_vol28_14_val, metrics_vol28_14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\nNaN\n0.003055\n0.016293\n0.000265\n0.928302\n2.080088e-42\n0.000000\n\n\nMAE val τ = 14\n40.176470\n0.007236\n0.007645\n0.000058\n-15.313175\n3.171246e-01\n0.055724\n\n\nMAE test τ = 14\n5.134818\n0.000891\n0.001376\n0.000002\n0.471169\n1.825919e-01\n0.006916\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol28_14[0][-500:], vol28_14[1], vol28_14[2], vol28_14_se, vol28_14_se_val, vol28_14_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol28_21[0], smoothing_level = best_param)\nvol28_21_se = model.smooth()\n\nmetrics_vol28_21 = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'train τ = 21', vol28_21[0], vol28_21_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol28_21[1], smoothing_level = best_param)\nvol28_21_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol28_21[2], smoothing_level = best_param)\nvol28_21_se_test = model.smooth()\n\nmetrics_vol28_21_val = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'val τ = 21', vol28_21[1], vol28_21_se_val)\nmetrics_vol28_21_test = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'test τ = 21', vol28_21[2], vol28_21_se_test)\n\npd.concat([metrics_vol28_21, metrics_vol28_21_val, metrics_vol28_21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\nNaN\n0.003055\n0.016293\n0.000265\n0.928302\n2.080088e-42\n0.000000\n\n\nMAE val τ = 21\n37.953130\n0.008205\n0.009337\n0.000087\n-1.189503\n1.525188e-01\n0.000680\n\n\nMAE test τ = 21\n7.330286\n0.001812\n0.002927\n0.000009\n0.784885\n1.883750e-01\n0.000639\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol28_21[0][-500:], vol28_21[1], vol28_21[2], vol28_21_se, vol28_21_se_val, vol28_21_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol28_28[0], smoothing_level = best_param)\nvol28_28_se = model.smooth()\n\nmetrics_vol28_28 = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'train τ = 28', vol28_28[0], vol28_28_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol28_28[1], smoothing_level = best_param)\nvol28_28_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol28_28[2], smoothing_level = best_param)\nvol28_28_se_test = model.smooth()\n\nmetrics_vol28_28_val = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'val τ = 28', vol28_28[1], vol28_28_se_val)\nmetrics_vol28_28_test = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'test τ = 28', vol28_28[2], vol28_28_se_test)\n\npd.concat([metrics_vol28_28, metrics_vol28_28_val, metrics_vol28_28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\nNaN\n0.003055\n0.016293\n0.000265\n0.928302\n2.080088e-42\n0.000000\n\n\nMAE val τ = 28\n33.769083\n0.012103\n0.014920\n0.000223\n-3.125747\n9.645792e-03\n0.000266\n\n\nMAE test τ = 28\n6.719563\n0.002137\n0.003148\n0.000010\n0.816352\n2.064221e-01\n0.003490\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol28_28[0][-500:], vol28_28[1], vol28_28[2], vol28_28_se, vol28_28_se_val, vol28_28_se_test)\nfig.show()\n\n\n\n\nmetrics_vol28 = pd.concat([\n    metrics_vol28_7, metrics_vol28_7_val, metrics_vol28_7_test,\n    metrics_vol28_14, metrics_vol28_14_val, metrics_vol28_14_test,\n    metrics_vol28_21, metrics_vol28_21_val, metrics_vol28_21_test,\n    metrics_vol28_28, metrics_vol28_28_val, metrics_vol28_28_test\n])\n\n\n\n\n\nCaso doble usando statsmodels\n\nPrice\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(price7_se, smoothing_level = param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\n8.798789\n732.721036\n1671.422857\n2.793654e+06\n0.987841\n0.1\n\n\n1\n6.511403\n536.440785\n1263.717826\n1.596983e+06\n0.993049\n0.2\n\n\n2\n5.595620\n465.204441\n1102.082798\n1.214586e+06\n0.994714\n0.3\n\n\n3\n5.101244\n427.164987\n1013.637794\n1.027462e+06\n0.995528\n0.4\n\n\n4\n4.792659\n402.342712\n958.699794\n9.191053e+05\n0.996000\n0.5\n\n\n5\n4.592166\n385.640855\n922.378692\n8.507825e+05\n0.996297\n0.6\n\n\n6\n4.464190\n374.330200\n897.770556\n8.059920e+05\n0.996492\n0.7\n\n\n7\n4.376713\n365.854545\n881.258653\n7.766168e+05\n0.996620\n0.8\n\n\n8\n4.319775\n360.341886\n870.817024\n7.583223e+05\n0.996700\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(price7_se, smoothing_level = best_param)\nprice7_se2 = model.smooth()\n\nmetrics_price7 = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'train τ = 7', price7_se, price7_se2, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(price7_se_val, smoothing_level = best_param)\nprice7_se_val2 = model.smooth()\n\nmodel = ExponentialSmoothing(price7_se_test, smoothing_level = best_param)\nprice7_se_test2 = model.smooth()\n\nmetrics_price7_val_ = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'val τ = 7', price7_se_val, price7_se_val2)\nmetrics_price7_test = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'test τ = 7', price7_se_test, price7_se_test2)\n\npd.concat([metrics_price7, metrics_price7_val, metrics_price7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\n4.319775\n360.341886\n870.817024\n7.583223e+05\n0.996700\n3.353203e-36\n0.000000\n\n\nMAE val τ = 7\n5.410749\n2551.131517\n3274.988349\n1.072555e+07\n-1.459638\n3.353613e-01\n0.723077\n\n\nMAE test τ = 7\n2.931272\n1351.776008\n1768.748748\n3.128472e+06\n0.117423\n7.041111e-02\n0.808292\n\n\n\n\n\n\n\n\nfig = model.plot_preds(price7_se[-500:], price7_se_val, price7_se_test, price7_se2, price7_se_val2, price7_se_test2)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(price14_se, smoothing_level = best_param)\nprice14_se2 = model.smooth()\n\nmetrics_price14 = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'train τ = 14', price14_se, price14_se2, show_info=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(price14_se_val, smoothing_level = best_param)\nprice14_se_val2 = model.smooth()\n\nmodel = ExponentialSmoothing(price14_se_test, smoothing_level = best_param)\nprice14_se_test2 = model.smooth()\n\nmetrics_price14_val = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'val τ = 14', price14_se_val, price14_se_val2)\nmetrics_price14_test = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'test τ = 14', price14_se_test, price14_se_test2)\n\npd.concat([metrics_price14, metrics_price14_val, metrics_price14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\n4.319775\n360.341886\n870.817024\n7.583223e+05\n0.996700\n3.353203e-36\n0.000000\n\n\nMAE val τ = 14\n14.752890\n7599.196025\n7960.909495\n6.337608e+07\n-90.619982\n9.083214e-03\n0.415567\n\n\nMAE test τ = 14\n0.983599\n506.931940\n829.680695\n6.883701e+05\n0.004854\n2.575142e-01\n0.041523\n\n\n\n\n\n\n\n\nfig = model.plot_preds(price14_se[-500:], price14_se_val, price14_se_test, price14_se2, price14_se_val2, price14_se_test2)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(price21_se, smoothing_level = best_param)\nprice21_se2 = model.smooth()\nmetrics_price21 = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'train τ = 21', price21_se, price21_se2, show_info=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(price21_se_val, smoothing_level = best_param)\nprice21_se_val2 = model.smooth()\n\nmodel = ExponentialSmoothing(price21_se_test, smoothing_level = best_param)\nprice21_se_test2 = model.smooth()\n\nmetrics_price21_val = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'val τ = 21', price21_se_val, price21_se_val2)\nmetrics_price21_test = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'test τ = 21', price21_se_test, price21_se_test2)\n\npd.concat([metrics_price21, metrics_price21_val, metrics_price21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\n4.319775\n360.341886\n870.817024\n7.583223e+05\n0.996700\n3.353203e-36\n0.000000\n\n\nMAE val τ = 21\n21.798390\n13164.417955\n14118.107310\n1.993210e+08\n-3.695558\n4.399717e-03\n0.345503\n\n\nMAE test τ = 21\n3.186922\n1952.737279\n2873.376280\n8.256291e+06\n0.805500\n8.879869e-01\n0.199937\n\n\n\n\n\n\n\n\nfig = model.plot_preds(price21_se[-500:], price21_se_val, price21_se_test, price21_se2, price21_se_val2, price21_se_test2)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(price28_se, smoothing_level = best_param)\nprice28_se2 = model.smooth()\nmetrics_price28 = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'train τ = 28', price28_se, price28_se2, show_info=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(price28_se_val, smoothing_level = best_param)\nprice28_se_val2 = model.smooth()\n\nmodel = ExponentialSmoothing(price28_se_test, smoothing_level = best_param)\nprice28_se_test2 = model.smooth()\n\nmetrics_price28_val = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'val τ = 28', price28_se_val, price28_se_val2)\nmetrics_price28_test = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'test τ = 28', price28_se_test, price28_se_test2)\n\npd.concat([metrics_price28, metrics_price28_val, metrics_price28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\n4.319775\n360.341886\n870.817024\n7.583223e+05\n0.996700\n3.353203e-36\n0.000000\n\n\nMAE val τ = 28\n27.232540\n17991.502514\n18840.257822\n3.549553e+08\n-15.141019\n7.124967e-05\n0.213674\n\n\nMAE test τ = 28\n3.562825\n2327.965894\n2988.180680\n8.929224e+06\n0.593958\n8.436555e-01\n0.961865\n\n\n\n\n\n\n\n\nfig = model.plot_preds(price28_se[-500:], price28_se_val, price28_se_test, price28_se2, price28_se_val2, price28_se_test2)\nfig.show()\n\n\n\n\nmetrics_price_d = pd.concat([\n    metrics_price7, metrics_price7_val, metrics_price7_test,\n    metrics_price14, metrics_price14_val, metrics_price14_test,\n    metrics_price21, metrics_price21_val, metrics_price21_test,\n    metrics_price28, metrics_price28_val, metrics_price28_test\n])\n\n\n\n\nRetorno acumulado\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(A_t7[0], smoothing_level = param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\nNaN\n0.102749\n0.200028\n0.040011\n0.998930\n0.1\n\n\n1\nNaN\n0.072009\n0.148842\n0.022154\n0.999407\n0.2\n\n\n2\nNaN\n0.060886\n0.129237\n0.016702\n0.999553\n0.3\n\n\n3\nNaN\n0.055120\n0.119108\n0.014187\n0.999620\n0.4\n\n\n4\nNaN\n0.051761\n0.113368\n0.012852\n0.999656\n0.5\n\n\n5\nNaN\n0.049592\n0.110095\n0.012121\n0.999676\n0.6\n\n\n6\nNaN\n0.048146\n0.108364\n0.011743\n0.999686\n0.7\n\n\n7\nNaN\n0.047216\n0.107657\n0.011590\n0.999690\n0.8\n\n\n8\nNaN\n0.046706\n0.107660\n0.011591\n0.999690\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(A_t7_se, smoothing_level = best_param)\nA_t7_se2 = model.smooth()\n\nmetrics_A_t7 = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 7', 'MAE', 'train τ = 7', A_t7_se, A_t7_se2, show_info=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(A_t7_se_val, smoothing_level = best_param)\nA_t7_se_val2 = model.smooth()\n\nmodel = ExponentialSmoothing(A_t7_se_test, smoothing_level = best_param)\nA_t7_se_test2 = model.smooth()\n\nmetrics_A_t7_val = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 7', 'MAE', 'val τ = 7', A_t7_se_val, A_t7_se_val2)\nmetrics_A_t7_test = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 7', 'MAE', 'test τ = 7', A_t7_se_test, A_t7_se_test2)\n\npd.concat([metrics_A_t7, metrics_A_t7_val, metrics_A_t7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\nNaN\n0.044732\n0.101900\n0.010384\n0.999723\n1.630821e-71\n0.000000\n\n\nMAE val τ = 7\n0.178307\n0.041566\n0.058015\n0.003366\n-0.872815\n5.349222e-01\n0.846494\n\n\nMAE test τ = 7\n0.130835\n0.030491\n0.039711\n0.001577\n0.122497\n8.065773e-02\n0.797976\n\n\n\n\n\n\n\n\nfig = model.plot_preds(A_t7_se[-500:], A_t7_se_val, A_t7_se_test, A_t7_se2, A_t7_se_val2, A_t7_se_test2)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(A_t14_se, smoothing_level = best_param)\nA_t14_se2 = model.smooth()\n\nmetrics_A_t14 = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 14', 'MAE', 'train τ = 14', A_t14_se, A_t14_se2, show_info=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(A_t14_se_val, smoothing_level = best_param)\nA_t14_se_val2 = model.smooth()\n\nmodel = ExponentialSmoothing(A_t14_se_test, smoothing_level = best_param)\nA_t14_se_test2 = model.smooth()\n\nmetrics_A_t14_val = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 14', 'MAE', 'val τ = 14', A_t14_se_val, A_t14_se_val2)\nmetrics_A_t14_test = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 14', 'MAE', 'test τ = 14', A_t14_se_test, A_t14_se_test2)\n\npd.concat([metrics_A_t14, metrics_A_t14_val, metrics_A_t14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\nNaN\n0.044732\n0.101900\n0.010384\n0.999723\n1.630821e-71\n0.000000\n\n\nMAE val τ = 14\n0.699318\n0.163763\n0.171513\n0.029417\n-104.855285\n9.824910e-03\n0.443789\n\n\nMAE test τ = 14\n0.042901\n0.010047\n0.016528\n0.000273\n0.016989\n2.757463e-01\n0.027104\n\n\n\n\n\n\n\n\nfig = model.plot_preds(A_t14_se[-500:], A_t14_se_val, A_t14_se_test, A_t14_se2, A_t14_se_val2, A_t14_se_test2)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(A_t21_se, smoothing_level = best_param)\nA_t21_se2 = model.smooth()\n\nmetrics_A_t21 = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 21', 'MAE', 'train τ = 21', A_t21_se, A_t21_se2, show_info=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(A_t21_se_val, smoothing_level = best_param)\nA_t21_se_val2 = model.smooth()\nmodel = ExponentialSmoothing(A_t21_se_test, smoothing_level = best_param)\nA_t21_se_test2 = model.smooth()\nmetrics_A_t21_val = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 21', 'MAE', 'val τ = 21', A_t21_se_val, A_t21_se_val2)\nmetrics_A_t21_test = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 21', 'MAE', 'test τ = 21', A_t21_se_test, A_t21_se_test2)\n\npd.concat([metrics_A_t21, metrics_A_t21_val, metrics_A_t21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\nNaN\n0.044732\n0.101900\n0.010384\n0.999723\n1.630821e-71\n0.000000\n\n\nMAE val τ = 21\n1.092877\n0.257716\n0.272995\n0.074526\n-4.518534\n2.818119e-03\n0.371752\n\n\nMAE test τ = 21\n0.144241\n0.034033\n0.050542\n0.002555\n0.810844\n8.591690e-01\n0.133577\n\n\n\n\n\n\n\n\nfig = model.plot_preds(A_t21_se[-500:], A_t21_se_val, A_t21_se_test, A_t21_se2, A_t21_se_val2, A_t21_se_test2)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(A_t28_se, smoothing_level = best_param)\nA_t28_se2 = model.smooth()\nmetrics_A_t28 = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 28', 'MAE', 'train τ = 28', A_t28_se, A_t28_se2, show_info=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(A_t28_se_val, smoothing_level = best_param)\nA_t28_se_val2 = model.smooth()\nmodel = ExponentialSmoothing(A_t28_se_test, smoothing_level = best_param)\nA_t28_se_test2 = model.smooth()\nmetrics_A_t28_val = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 28', 'MAE', 'val τ = 28', A_t28_se_val, A_t28_se_val2)\nmetrics_A_t28_test = model.residuals('Simple Exponential Smoothing - Retorno Acumulado: τ = 28', 'MAE', 'test τ = 28', A_t28_se_test, A_t28_se_test2)\n\npd.concat([metrics_A_t28, metrics_A_t28_val, metrics_A_t28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\nNaN\n0.044732\n0.101900\n0.010384\n0.999723\n1.630821e-71\n0.000000\n\n\nMAE val τ = 28\n1.43171\n0.339018\n0.353954\n0.125284\n-19.569021\n3.910206e-05\n0.201270\n\n\nMAE test τ = 28\n0.15428\n0.036514\n0.047764\n0.002281\n0.625449\n8.902413e-01\n0.867971\n\n\n\n\n\n\n\n\nfig = model.plot_preds(A_t28_se[-500:], A_t28_se_val, A_t28_se_test, A_t28_se2, A_t28_se_val2, A_t28_se_test2)\nfig.show()\n\n\n\n\nmetrics_A_t_d = pd.concat([\n    metrics_A_t7, metrics_A_t7_val, metrics_A_t7_test,\n    metrics_A_t14, metrics_A_t14_val, metrics_A_t14_test,\n    metrics_A_t21, metrics_A_t21_val, metrics_A_t21_test,\n    metrics_A_t28, metrics_A_t28_val, metrics_A_t28_test\n])\n\n\n\n\nVolatilidad \\(\\omega = 7\\)\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(vol7_7_se, smoothing_level = param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\nNaN\n0.018776\n0.053273\n0.002838\n0.318988\n0.1\n\n\n1\nNaN\n0.016555\n0.048270\n0.002330\n0.440904\n0.2\n\n\n2\nNaN\n0.014857\n0.044502\n0.001980\n0.524765\n0.3\n\n\n3\nNaN\n0.013521\n0.041640\n0.001734\n0.583930\n0.4\n\n\n4\nNaN\n0.012507\n0.039478\n0.001559\n0.626010\n0.5\n\n\n5\nNaN\n0.011734\n0.037863\n0.001434\n0.655991\n0.6\n\n\n6\nNaN\n0.011121\n0.036672\n0.001345\n0.677285\n0.7\n\n\n7\nNaN\n0.010637\n0.035810\n0.001282\n0.692278\n0.8\n\n\n8\nNaN\n0.010263\n0.035204\n0.001239\n0.702614\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol7_7_se, smoothing_level = best_param)\nvol7_7_se2 = model.smooth()\nmetrics_vol7_7 = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 7', 'MAE', 'train τ = 7', vol7_7_se, vol7_7_se2, show_info=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol7_7_se_val, smoothing_level = best_param)\nvol7_7_se_val2 = model.smooth()\nmodel = ExponentialSmoothing(vol7_7_se_test, smoothing_level = best_param)\nvol7_7_se_test2 = model.smooth()\nmetrics_vol7_7_val = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 7', 'MAE', 'val τ = 7', vol7_7_se_val, vol7_7_se_val2)\nmetrics_vol7_7_test = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 7', 'MAE', 'test τ = 7', vol7_7_se_test, vol7_7_se_test2)\n\npd.concat([metrics_vol7_7, metrics_vol7_7_val, metrics_vol7_7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\nNaN\n0.010263\n0.035204\n0.001239\n0.702614\n1.302632e-268\n0.000000\n\n\nMAE val τ = 7\n56.610089\n0.005835\n0.006816\n0.000046\n-2.014865\n8.838192e-01\n0.191010\n\n\nMAE test τ = 7\n18.981655\n0.002789\n0.003703\n0.000014\n0.109953\n4.645415e-01\n0.632671\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol7_7_se[-500:], vol7_7_se_val, vol7_7_se_test, vol7_7_se2, vol7_7_se_val2, vol7_7_se_test2)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol7_14_se, smoothing_level = best_param)\nvol7_14_se2 = model.smooth()\nmetrics_vol7_14 = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 14', 'MAE', 'train τ = 14', vol7_14_se, vol7_14_se2, show_info=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol7_14_se_val, smoothing_level = best_param)\nvol7_14_se_val2 = model.smooth()\nmodel = ExponentialSmoothing(vol7_14_se_test, smoothing_level = best_param)\nvol7_14_se_test2 = model.smooth()\nmetrics_vol7_14_val = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 14', 'MAE', 'val τ = 14', vol7_14_se_val, vol7_14_se_val2)\nmetrics_vol7_14_test = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 14', 'MAE', 'test τ = 14', vol7_14_se_test, vol7_14_se_test2)\n\npd.concat([metrics_vol7_14, metrics_vol7_14_val, metrics_vol7_14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\nNaN\n0.010263\n0.035204\n0.001239\n0.702614\n1.302632e-268\n0.000000\n\n\nMAE val τ = 14\n17.239164\n0.002484\n0.003033\n0.000009\n0.372364\n7.279072e-01\n0.437233\n\n\nMAE test τ = 14\n23.176398\n0.002729\n0.003812\n0.000015\n0.008352\n6.429813e-01\n0.001017\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol7_14_se[-500:], vol7_14_se_val, vol7_14_se_test, vol7_14_se2, vol7_14_se_val2, vol7_14_se_test2)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol7_21_se, smoothing_level = best_param)\nvol7_21_se2 = model.smooth()\nmetrics_vol7_21 = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 21', 'MAE', 'train τ = 21', vol7_21_se, vol7_21_se2, show_info=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol7_21_se_val, smoothing_level = best_param)\nvol7_21_se_val2 = model.smooth()\nmodel = ExponentialSmoothing(vol7_21_se_test, smoothing_level = best_param)\nvol7_21_se_test2 = model.smooth()\nmetrics_vol7_21_val = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 21', 'MAE', 'val τ = 21', vol7_21_se_val, vol7_21_se_val2)\nmetrics_vol7_21_test = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 21', 'MAE', 'test τ = 21', vol7_21_se_test, vol7_21_se_test2)\n\npd.concat([metrics_vol7_21, metrics_vol7_21_val, metrics_vol7_21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\nNaN\n0.010263\n0.035204\n0.001239\n0.702614\n1.302632e-268\n0.000000\n\n\nMAE val τ = 21\n63.840747\n0.018600\n0.022139\n0.000490\n-1.157191\n8.973707e-01\n0.256545\n\n\nMAE test τ = 21\n23.988768\n0.005690\n0.008179\n0.000067\n0.705569\n7.157732e-01\n0.461761\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol7_21_se[-500:], vol7_21_se_val, vol7_21_se_test, vol7_21_se2, vol7_21_se_val2, vol7_21_se_test2)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol7_28_se, smoothing_level = best_param)\nvol7_28_se2 = model.smooth()\nmetrics_vol7_28 = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 28', 'MAE', 'train τ = 28', vol7_28_se, vol7_28_se2, show_info=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol7_28_se_val, smoothing_level = best_param)\nvol7_28_se_val2 = model.smooth()\nmodel = ExponentialSmoothing(vol7_28_se_test, smoothing_level = best_param)\nvol7_28_se_test2 = model.smooth()\nmetrics_vol7_28_val = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 28', 'MAE', 'val τ = 28', vol7_28_se_val, vol7_28_se_val2)\nmetrics_vol7_28_test = model.residuals('Simple Exponential Smoothing - Volatilidad: τ = 28', 'MAE', 'test τ = 28', vol7_28_se_test, vol7_28_se_test2)\n\npd.concat([metrics_vol7_28, metrics_vol7_28_val, metrics_vol7_28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\nNaN\n0.010263\n0.035204\n0.001239\n0.702614\n1.302632e-268\n0.000000\n\n\nMAE val τ = 28\n53.702341\n0.023434\n0.027306\n0.000746\n-4.002681\n6.629902e-01\n0.055569\n\n\nMAE test τ = 28\n22.227595\n0.007637\n0.010632\n0.000113\n0.241564\n1.135172e-02\n0.632942\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol7_28_se[-500:], vol7_28_se_val, vol7_28_se_test, vol7_28_se2, vol7_28_se_val2, vol7_28_se_test2)\nfig.show()\n\n\n\n\nmetrics_vol7_d = pd.concat([\n    metrics_vol7_7, metrics_vol7_7_val, metrics_vol7_7_test,\n    metrics_vol7_14, metrics_vol7_14_val, metrics_vol7_14_test,\n    metrics_vol7_21, metrics_vol7_21_val, metrics_vol7_21_test,\n    metrics_vol7_28, metrics_vol7_28_val, metrics_vol7_28_test\n])\n\n\n\n\nVolatilidad \\(\\omega = 14\\)\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(vol14_7[0], smoothing_level = param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\nNaN\n0.014361\n0.043906\n0.001928\n0.511511\n0.1\n\n\n1\nNaN\n0.010759\n0.036137\n0.001306\n0.669093\n0.2\n\n\n2\nNaN\n0.008880\n0.031524\n0.000994\n0.748183\n0.3\n\n\n3\nNaN\n0.007712\n0.028696\n0.000823\n0.791334\n0.4\n\n\n4\nNaN\n0.006944\n0.026903\n0.000724\n0.816595\n0.5\n\n\n5\nNaN\n0.006409\n0.025734\n0.000662\n0.832185\n0.6\n\n\n6\nNaN\n0.006021\n0.024964\n0.000623\n0.842086\n0.7\n\n\n7\nNaN\n0.005731\n0.024463\n0.000598\n0.848356\n0.8\n\n\n8\nNaN\n0.005515\n0.024158\n0.000584\n0.852115\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol14_7[0], smoothing_level = best_param)\nvol14_7_se = model.smooth()\n\nmetrics_vol14_7 = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'train τ = 7', vol14_7[0], vol14_7_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol14_7[1], smoothing_level = best_param)\nvol14_7_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol14_7[2], smoothing_level = best_param)\nvol14_7_se_test = model.smooth()\n\nmetrics_vol14_7_val = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'val τ = 7', vol14_7[1], vol14_7_se_val)\nmetrics_vol14_7_test = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'test τ = 7', vol14_7[2], vol14_7_se_test)\n\npd.concat([metrics_vol14_7, metrics_vol14_7_val, metrics_vol14_7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\nNaN\n0.005515\n0.024158\n5.836072e-04\n0.852115\n3.661844e-46\n0.000000\n\n\nMAE val τ = 7\n33.823503\n0.005510\n0.006004\n3.605256e-05\n-127.652529\n7.904339e-01\n0.082897\n\n\nMAE test τ = 7\n4.052057\n0.000671\n0.000833\n6.947101e-07\n-1.479053\n9.197661e-01\n0.916042\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol14_7[0][-500:], vol14_7[1], vol14_7[2], vol14_7_se, vol14_7_se_val, vol14_7_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol14_14[0], smoothing_level = best_param)\nvol14_14_se = model.smooth()\n\nmetrics_vol14_14 = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'train τ = 14', vol14_14[0], vol14_14_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol14_14[1], smoothing_level = best_param)\nvol14_14_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol14_14[2], smoothing_level = best_param)\nvol14_14_se_test = model.smooth()\n\nmetrics_vol14_14_val = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'val τ = 14', vol14_14[1], vol14_14_se_val)\nmetrics_vol14_14_test = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'test τ = 14', vol14_14[2], vol14_14_se_test)\n\npd.concat([metrics_vol14_14, metrics_vol14_14_val, metrics_vol14_14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\nNaN\n0.005515\n0.024158\n5.836072e-04\n0.852115\n3.661844e-46\n0.000000\n\n\nMAE val τ = 14\n20.689774\n0.003483\n0.004332\n1.877001e-05\n-57.072766\n6.315728e-01\n0.000001\n\n\nMAE test τ = 14\n3.756666\n0.000633\n0.000757\n5.729753e-07\n-0.772735\n1.073652e-01\n0.220719\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol14_14[0][-500:], vol14_14[1], vol14_14[2], vol14_14_se, vol14_14_se_val, vol14_14_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol14_21[0], smoothing_level = best_param)\nvol14_21_se = model.smooth()\n\nmetrics_vol14_21 = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'train τ = 21', vol14_21[0], vol14_21_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol14_21[1], smoothing_level = best_param)\nvol14_21_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol14_21[2], smoothing_level = best_param)\nvol14_21_se_test = model.smooth()\n\nmetrics_vol14_21_val = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'val τ = 21', vol14_21[1], vol14_21_se_val)\nmetrics_vol14_21_test = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'test τ = 21', vol14_21[2], vol14_21_se_test)\n\npd.concat([metrics_vol14_21, metrics_vol14_21_val, metrics_vol14_21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\nNaN\n0.005515\n0.024158\n0.000584\n0.852115\n3.661844e-46\n0.000000e+00\n\n\nMAE val τ = 21\n41.685487\n0.013470\n0.016199\n0.000262\n-1.520083\n7.230675e-01\n3.959695e-18\n\n\nMAE test τ = 21\n9.584982\n0.002913\n0.004743\n0.000022\n0.783916\n9.533840e-02\n6.906289e-04\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol14_21[0][-500:], vol14_21[1], vol14_21[2], vol14_21_se, vol14_21_se_val, vol14_21_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol14_28[0], smoothing_level = best_param)\nvol14_28_se = model.smooth()\n\nmetrics_vol14_28 = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'train τ = 28', vol14_28[0], vol14_28_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol14_28[1], smoothing_level = best_param)\nvol14_28_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol14_28[2], smoothing_level = best_param)\nvol14_28_se_test = model.smooth()\n\nmetrics_vol14_28_val = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'val τ = 28', vol14_28[1], vol14_28_se_val)\nmetrics_vol14_28_test = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'test τ = 28', vol14_28[2], vol14_28_se_test)\n\npd.concat([metrics_vol14_28, metrics_vol14_28_val, metrics_vol14_28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\nNaN\n0.005515\n0.024158\n0.000584\n0.852115\n3.661844e-46\n0.000000e+00\n\n\nMAE val τ = 28\n48.322368\n0.019203\n0.020935\n0.000438\n-8.715676\n7.769449e-01\n6.949045e-33\n\n\nMAE test τ = 28\n10.221859\n0.003779\n0.005210\n0.000027\n0.398179\n8.146092e-02\n4.263438e-01\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol14_28[0][-500:], vol14_28[1], vol14_28[2], vol14_28_se, vol14_28_se_val, vol14_28_se_test)\nfig.show()\n\n\n\n\nmetrics_vol14_d = pd.concat([\n    metrics_vol14_7, metrics_vol14_7_val, metrics_vol14_7_test,\n    metrics_vol14_14, metrics_vol14_14_val, metrics_vol14_14_test,\n    metrics_vol14_21, metrics_vol14_21_val, metrics_vol14_21_test,\n    metrics_vol14_28, metrics_vol14_28_val, metrics_vol14_28_test\n])\n\n\n\n\nVolatilidad \\(\\omega = 21\\)\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(vol21_7[0], smoothing_level = param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\nNaN\n0.011977\n0.037543\n0.001409\n0.629595\n0.1\n\n\n1\nNaN\n0.008228\n0.029254\n0.000856\n0.775093\n0.2\n\n\n2\nNaN\n0.006499\n0.025110\n0.000630\n0.834308\n0.3\n\n\n3\nNaN\n0.005543\n0.022778\n0.000519\n0.863656\n0.4\n\n\n4\nNaN\n0.004951\n0.021346\n0.000456\n0.880255\n0.5\n\n\n5\nNaN\n0.004547\n0.020424\n0.000417\n0.890372\n0.6\n\n\n6\nNaN\n0.004251\n0.019822\n0.000393\n0.896747\n0.7\n\n\n7\nNaN\n0.004034\n0.019434\n0.000378\n0.900745\n0.8\n\n\n8\nNaN\n0.003870\n0.019202\n0.000369\n0.903106\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol21_7[0], smoothing_level = best_param)\nvol21_7_se = model.smooth()\n\nmetrics_vol21_7 = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'train τ = 7', vol21_7[0], vol21_7_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol21_7[1], smoothing_level = best_param)\nvol21_7_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol21_7[2], smoothing_level = best_param)\nvol21_7_se_test = model.smooth()\n\nmetrics_vol21_7_val = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'val τ = 7', vol21_7[1], vol21_7_se_val)\nmetrics_vol21_7_test = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'test τ = 7', vol21_7[2], vol21_7_se_test)\n\npd.concat([metrics_vol21_7, metrics_vol21_7_val, metrics_vol21_7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\nNaN\n0.003870\n0.019202\n3.687029e-04\n0.903106\n3.427794e-46\n0.000000\n\n\nMAE val τ = 7\n24.639399\n0.004875\n0.005659\n3.202053e-05\n-93.863726\n9.767082e-01\n0.063760\n\n\nMAE test τ = 7\n2.275079\n0.000456\n0.000654\n4.281579e-07\n-0.268456\n4.601006e-01\n0.973017\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol21_7[0][-500:], vol21_7[1], vol21_7[2], vol21_7_se, vol21_7_se_val, vol21_7_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol21_14[0], smoothing_level = best_param)\nvol21_14_se = model.smooth()\n\nmetrics_vol21_14 = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'train τ = 14', vol21_14[0], vol21_14_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol21_14[1], smoothing_level = best_param)\nvol21_14_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol21_14[2], smoothing_level = best_param)\nvol21_14_se_test = model.smooth()\n\nmetrics_vol21_14_val = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'val τ = 14', vol21_14[1], vol21_14_se_val)\nmetrics_vol21_14_test = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'test τ = 14', vol21_14[2], vol21_14_se_test)\n\npd.concat([metrics_vol21_14, metrics_vol21_14_val, metrics_vol21_14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\nNaN\n0.003870\n0.019202\n3.687029e-04\n0.903106\n3.427794e-46\n0.000000e+00\n\n\nMAE val τ = 14\n32.155220\n0.005409\n0.006234\n3.885917e-05\n-87.343266\n9.018262e-01\n6.074575e-08\n\n\nMAE test τ = 14\n4.408937\n0.000741\n0.000901\n8.110631e-07\n-0.843888\n3.785980e-01\n3.502899e-01\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol21_14[0][-500:], vol21_14[1], vol21_14[2], vol21_14_se, vol21_14_se_val, vol21_14_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol21_21[0], smoothing_level = best_param)\nvol21_21_se = model.smooth()\n\nmetrics_vol21_21 = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'train τ = 21', vol21_21[0], vol21_21_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol21_21[1], smoothing_level = best_param)\nvol21_21_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol21_21[2], smoothing_level = best_param)\nvol21_21_se_test = model.smooth()\n\nmetrics_vol21_21_val = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'val τ = 21', vol21_21[1], vol21_21_se_val)\nmetrics_vol21_21_test = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'test τ = 21', vol21_21[2], vol21_21_se_test)\n\npd.concat([metrics_vol21_21, metrics_vol21_21_val, metrics_vol21_21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\nNaN\n0.003870\n0.019202\n0.000369\n0.903106\n3.427794e-46\n0.000000e+00\n\n\nMAE val τ = 21\n39.490052\n0.010181\n0.011634\n0.000135\n-1.179671\n4.597669e-01\n5.724525e-07\n\n\nMAE test τ = 21\n7.883541\n0.002106\n0.003458\n0.000012\n0.807484\n1.146190e-01\n5.402006e-04\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol21_21[0][-500:], vol21_21[1], vol21_21[2], vol21_21_se, vol21_21_se_val, vol21_21_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol21_28[0], smoothing_level = best_param)\nvol21_28_se = model.smooth()\n\nmetrics_vol21_28 = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'train τ = 28', vol21_28[0], vol21_28_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol21_28[1], smoothing_level = best_param)\nvol21_28_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol21_28[2], smoothing_level = best_param)\nvol21_28_se_test = model.smooth()\n\nmetrics_vol21_28_val = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'val τ = 28', vol21_28[1], vol21_28_se_val)\nmetrics_vol21_28_test = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'test τ = 28', vol21_28[2], vol21_28_se_test)\n\npd.concat([metrics_vol21_28, metrics_vol21_28_val, metrics_vol21_28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\nNaN\n0.003870\n0.019202\n0.000369\n0.903106\n3.427794e-46\n0.000000e+00\n\n\nMAE val τ = 28\n41.624074\n0.016049\n0.018614\n0.000346\n-5.760398\n5.223368e-01\n5.210346e-17\n\n\nMAE test τ = 28\n6.174221\n0.002060\n0.003240\n0.000010\n0.795153\n4.854350e-01\n7.161414e-05\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol21_28[0][-500:], vol21_28[1], vol21_28[2], vol21_28_se, vol21_28_se_val, vol21_28_se_test)\nfig.show()\n\n\n\n\nmetrics_vol21_d = pd.concat([\n    metrics_vol21_7, metrics_vol21_7_val, metrics_vol21_7_test,\n    metrics_vol21_14, metrics_vol21_14_val, metrics_vol21_14_test,\n    metrics_vol21_21, metrics_vol21_21_val, metrics_vol21_21_test,\n    metrics_vol21_28, metrics_vol21_28_val, metrics_vol21_28_test\n])\n\n\n\n\nVolatilidad \\(\\omega = 28\\)\n\n\\(\\tau = 7\\)\n\nresults = []\nfor param in params:\n    model = ExponentialSmoothing(vol28_7[0], smoothing_level = param)\n    yt = model.smooth()\n    eval_ = model.evaluate(yt)\n    eval_['λ'] = param\n    results.append(eval_)\n\nresults = pd.concat(results, ignore_index = True)\nresults\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nλ\n\n\n\n\n0\nNaN\n0.010221\n0.032930\n0.001084\n0.707122\n0.1\n\n\n1\nNaN\n0.006674\n0.024917\n0.000621\n0.832307\n0.2\n\n\n2\nNaN\n0.005232\n0.021289\n0.000453\n0.877593\n0.3\n\n\n3\nNaN\n0.004451\n0.019303\n0.000373\n0.899359\n0.4\n\n\n4\nNaN\n0.003956\n0.018093\n0.000327\n0.911582\n0.5\n\n\n5\nNaN\n0.003620\n0.017316\n0.000300\n0.919013\n0.6\n\n\n6\nNaN\n0.003376\n0.016809\n0.000283\n0.923684\n0.7\n\n\n7\nNaN\n0.003193\n0.016485\n0.000272\n0.926600\n0.8\n\n\n8\nNaN\n0.003055\n0.016293\n0.000265\n0.928302\n0.9\n\n\n\n\n\n\n\n\nbest_param = params[results['MAE'].idxmin()]\n\nplt.plot(params, results['MAE'], marker = 'o', linestyle = '-')\nplt.title('$MAE$ v.s. $\\lambda$')\nplt.xlabel('$\\lambda$')\nplt.ylabel('$MAE$')\nplt.axvline(x = best_param, color = 'red', linestyle = '--', label = f'Best λ = {best_param:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol28_7[0], smoothing_level = best_param)\nvol28_7_se = model.smooth()\n\nmetrics_vol28_7 = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'train τ = 7', vol28_7[0], vol28_7_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol28_7[1], smoothing_level = best_param)\nvol28_7_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol28_7[2], smoothing_level = best_param)\nvol28_7_se_test = model.smooth()\n\nmetrics_vol28_7_val = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'val τ = 7', vol28_7[1], vol28_7_se_val)\nmetrics_vol28_7_test = model.residuals('Simple Exponential Smoothing - Price: τ = 7', 'MAE', 'test τ = 7', vol28_7[2], vol28_7_se_test)\n\npd.concat([metrics_vol28_7, metrics_vol28_7_val, metrics_vol28_7_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 7\nNaN\n0.003055\n0.016293\n0.000265\n0.928302\n2.080088e-42\n0.000000\n\n\nMAE val τ = 7\n24.090786\n0.005197\n0.005680\n0.000032\n-10.081326\n4.853045e-01\n0.363816\n\n\nMAE test τ = 7\n6.550145\n0.001370\n0.001931\n0.000004\n-0.280476\n9.865148e-01\n0.180878\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol28_7[0][-500:], vol28_7[1], vol28_7[2], vol28_7_se, vol28_7_se_val, vol28_7_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 14\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol28_14[0], smoothing_level = best_param)\nvol28_14_se = model.smooth()\n\nmetrics_vol28_14 = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'train τ = 14', vol28_14[0], vol28_14_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol28_14[1], smoothing_level = best_param)\nvol28_14_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol28_14[2], smoothing_level = best_param)\nvol28_14_se_test = model.smooth()\n\nmetrics_vol28_14_val = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'val τ = 14', vol28_14[1], vol28_14_se_val)\nmetrics_vol28_14_test = model.residuals('Simple Exponential Smoothing - Price: τ = 14', 'MAE', 'test τ = 14', vol28_14[2], vol28_14_se_test)\n\npd.concat([metrics_vol28_14, metrics_vol28_14_val, metrics_vol28_14_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 14\nNaN\n0.003055\n0.016293\n0.000265\n0.928302\n2.080088e-42\n0.000000\n\n\nMAE val τ = 14\n40.176470\n0.007236\n0.007645\n0.000058\n-15.313175\n3.171246e-01\n0.055724\n\n\nMAE test τ = 14\n5.134818\n0.000891\n0.001376\n0.000002\n0.471169\n1.825919e-01\n0.006916\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol28_14[0][-500:], vol28_14[1], vol28_14[2], vol28_14_se, vol28_14_se_val, vol28_14_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 21\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol28_21[0], smoothing_level = best_param)\nvol28_21_se = model.smooth()\n\nmetrics_vol28_21 = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'train τ = 21', vol28_21[0], vol28_21_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol28_21[1], smoothing_level = best_param)\nvol28_21_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol28_21[2], smoothing_level = best_param)\nvol28_21_se_test = model.smooth()\n\nmetrics_vol28_21_val = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'val τ = 21', vol28_21[1], vol28_21_se_val)\nmetrics_vol28_21_test = model.residuals('Simple Exponential Smoothing - Price: τ = 21', 'MAE', 'test τ = 21', vol28_21[2], vol28_21_se_test)\n\npd.concat([metrics_vol28_21, metrics_vol28_21_val, metrics_vol28_21_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 21\nNaN\n0.003055\n0.016293\n0.000265\n0.928302\n2.080088e-42\n0.000000\n\n\nMAE val τ = 21\n37.953130\n0.008205\n0.009337\n0.000087\n-1.189503\n1.525188e-01\n0.000680\n\n\nMAE test τ = 21\n7.330286\n0.001812\n0.002927\n0.000009\n0.784885\n1.883750e-01\n0.000639\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol28_21[0][-500:], vol28_21[1], vol28_21[2], vol28_21_se, vol28_21_se_val, vol28_21_se_test)\nfig.show()\n\n\n\n\n\n\\(\\tau = 28\\)\nA continuación, evaluaremos los residuales del pronóstico.\n\nmodel = ExponentialSmoothing(vol28_28[0], smoothing_level = best_param)\nvol28_28_se = model.smooth()\n\nmetrics_vol28_28 = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'train τ = 28', vol28_28[0], vol28_28_se, show_info = True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe rechaza H0: hay autocorrelación en los residuales.\nJarque-Bera p-value: 0.0\nSe rechaza H0: los residuales no siguen una distribución normal.\n\n\nAhora, veamos el gráfico de nuestras predicciones para la validación y el test\n\nmodel = ExponentialSmoothing(vol28_28[1], smoothing_level = best_param)\nvol28_28_se_val = model.smooth()\n\nmodel = ExponentialSmoothing(vol28_28[2], smoothing_level = best_param)\nvol28_28_se_test = model.smooth()\n\nmetrics_vol28_28_val = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'val τ = 28', vol28_28[1], vol28_28_se_val)\nmetrics_vol28_28_test = model.residuals('Simple Exponential Smoothing - Price: τ = 28', 'MAE', 'test τ = 28', vol28_28[2], vol28_28_se_test)\n\npd.concat([metrics_vol28_28, metrics_vol28_28_val, metrics_vol28_28_test])\n\n\n\n\n\n\n\n\nMAPE\nMAE\nRMSE\nMSE\nR2\nLjung-Box p-value\nJarque-Bera p-value\n\n\n\n\nMAE train τ = 28\nNaN\n0.003055\n0.016293\n0.000265\n0.928302\n2.080088e-42\n0.000000\n\n\nMAE val τ = 28\n33.769083\n0.012103\n0.014920\n0.000223\n-3.125747\n9.645792e-03\n0.000266\n\n\nMAE test τ = 28\n6.719563\n0.002137\n0.003148\n0.000010\n0.816352\n2.064221e-01\n0.003490\n\n\n\n\n\n\n\n\nfig = model.plot_preds(vol28_28[0][-500:], vol28_28[1], vol28_28[2], vol28_28_se, vol28_28_se_val, vol28_28_se_test)\nfig.show()\n\n\n\n\nmetrics_vol28_d = pd.concat([\n    metrics_vol28_7, metrics_vol28_7_val, metrics_vol28_7_test,\n    metrics_vol28_14, metrics_vol28_14_val, metrics_vol28_14_test,\n    metrics_vol28_21, metrics_vol28_21_val, metrics_vol28_21_test,\n    metrics_vol28_28, metrics_vol28_28_val, metrics_vol28_28_test\n])",
    "crumbs": [
      "Modelos estadísticos"
    ]
  }
]